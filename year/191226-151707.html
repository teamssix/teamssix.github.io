<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="【Python Scrapy 爬虫框架】 6、继续爬虫、终止和重启任务"><meta name="keywords" content="Python,学习笔记,Scrapy"><meta name="author" content="Teams Six,undefined"><meta name="copyright" content="Teams Six"><title>【Python Scrapy 爬虫框架】 6、继续爬虫、终止和重启任务【Teams Six】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.json","languages":{"hits_empty":"local_search.hits_empty"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#0x00-前言"><span class="toc-number">1.</span> <span class="toc-text">0x00 前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0x01-修改代码"><span class="toc-number">2.</span> <span class="toc-text">0x01 修改代码</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0x02-运行"><span class="toc-number">3.</span> <span class="toc-text">0x02 运行</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Teams Six</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/teamssix" target="_blank">GitHub<i class="icon-dot bg-color4"></i></a><a class="links-button button-hover" href="https://me.csdn.net/qq_37683287" target="_blank">CSDN<i class="icon-dot bg-color8"></i></a><a class="links-button button-hover" href="https://twitter.com/TeamsSix" target="_blank">Twitter<i class="icon-dot bg-color0"></i></a><a class="links-button button-hover" href="https://instagram.com/TeamsSix" target="_blank">Instagram<i class="icon-dot bg-color4"></i></a><a class="links-button button-hover" href="https://www.youtube.com/channel/UCKeJhJYi_tmXWETzVxKbSvA" target="_blank">YouTube<i class="icon-dot bg-color7"></i></a><a class="links-button button-hover" href="https://space.bilibili.com/148389186" target="_blank">BiliBili<i class="icon-dot bg-color7"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">51</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">76</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">15</span></a></div><div class="friend-link"><a class="friend-link-text" target="_blank">欢迎关注本站微信公众号：TeamsSix</a><a class="friend-link-text" href="https://www.loongten.com/" target="_blank">友情链接：loongten</a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a></nav><div class="right-info"><a class="title-name" href="/">Teams Six</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">【Python Scrapy 爬虫框架】 6、继续爬虫、终止和重启任务</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2019-12-26 | 更新于 2019-12-26</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Python-Scrapy-爬虫框架学习笔记/">Python Scrapy 爬虫框架学习笔记</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Python/">Python</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/学习笔记/">学习笔记</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div></div><div class="main-content"><h1 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h1><p>有时候我们不想只爬一个页面的，比如之前我只爬了主页，但是现在想把其他页面的也爬下来，这就是本文的任务。</p>
<h1 id="0x01-修改代码"><a href="#0x01-修改代码" class="headerlink" title="0x01 修改代码"></a>0x01 修改代码</h1><p>在之前的基础上，修改 teamssix_blog_spider.py 文件，首先添加 start_urls</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">start_urls = [</span><br><span class="line">   <span class="string">'https://www.teamssix.com'</span>,</span><br><span class="line">   <span class="string">'https://www.teamssix.com/page/2/'</span>,</span><br><span class="line">   <span class="string">'https://www.teamssix.com/page/3/'</span>,</span><br><span class="line">   <span class="string">'https://www.teamssix.com/page/4/'</span>,</span><br><span class="line">   <span class="string">'https://www.teamssix.com/page/5/'</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<a id="more"></a>
<p>接下来在 sub_article 函数尾部添加 parse 函数的全部代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(response.text, <span class="string">'html.parser'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'.post-title'</span>):</span><br><span class="line">   url = <span class="string">'https://www.teamssix.com&#123;&#125;'</span>.format(i[<span class="string">'href'</span>])</span><br><span class="line">   <span class="keyword">yield</span> scrapy.Request(url, callback=self.sub_article)</span><br></pre></td></tr></table></figure>

<p>所以 sub_article 函数的完整代码就是这个样子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sub_article</span><span class="params">(self,response)</span>:</span></span><br><span class="line">   soup = BeautifulSoup(response.text,<span class="string">'html.parser'</span>)</span><br><span class="line">   title = self.article_title(soup)</span><br><span class="line">   list = self.article_list(soup)</span><br><span class="line">   print(title)</span><br><span class="line">   item = TeamssixItem(_id = response.url,title = title,list = list)</span><br><span class="line">   <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">   soup = BeautifulSoup(response.text, <span class="string">'html.parser'</span>)</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'.post-title'</span>):</span><br><span class="line">      url = <span class="string">'https://www.teamssix.com&#123;&#125;'</span>.format(i[<span class="string">'href'</span>])</span><br><span class="line">      <span class="keyword">yield</span> scrapy.Request(url, callback=self.sub_article)</span><br></pre></td></tr></table></figure>

<p>从最后一行 callback=self.sub_article 这里不难看出这里其实就是一个循环， sub_article 函数第一遍执行完，又会调用继续执行第二遍，直到 start_urls 被执行完。</p>
<h1 id="0x02-运行"><a href="#0x02-运行" class="headerlink" title="0x02 运行"></a>0x02 运行</h1><p>代码修改的就这些，接下来直接 scrapy crawl blogurl 运行代码，来到 robo 3T 看看爬取到的数据。</p>
<p><img src="https://uploader.shimo.im/f/4C5P0BNBAy8DfwVP.png!thumbnail" alt="图片"></p>
<p>最终在这些 start_urls 中爬取下来了 43 篇文章，Emm，还行。</p>
<p>这次的 Scrapy 学习笔记就更新到这里，这个项目的代码已经放在了我的 GitHub 里，项目链接已经放在了下面。</p>
<blockquote>
<p>更多信息欢迎关注我的个人微信公众号：TeamsSix<br>项目地址：<a href="https://github.com/teamssix/scrapy_study_notes" target="_blank" rel="noopener">https://github.com/teamssix/scrapy_study_notes</a></p>
</blockquote>
<blockquote>
<p>参考链接：<br><a href="https://youtu.be/aDwAmj3VWH4" target="_blank" rel="noopener">https://youtu.be/aDwAmj3VWH4</a><br><a href="http://doc.scrapy.org/en/latest/topics/architecture.html" target="_blank" rel="noopener">http://doc.scrapy.org/en/latest/topics/architecture.html</a></p>
</blockquote>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Teams Six</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://www.teamssix.com/year/191226-151707.html">https://www.teamssix.com/year/191226-151707.html</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.teamssix.com">Teams Six</a>！</span></div></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/year/200105-211642.html"><i class="fas fa-angle-left">&nbsp;</i><span>【经验总结】SQL注入Bypass安全狗360主机卫士</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/year/191226-151702.html"><span>【Python Scrapy 爬虫框架】 5、利用 pipelines 和 settings 将爬取数据存储到 MongoDB</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Teams Six</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--></body></html>