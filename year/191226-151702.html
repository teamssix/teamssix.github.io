<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="【Python Scrapy 爬虫框架】 5、利用 pipelines 和 settings 将爬取数据存储到 MongoDB"><meta name="keywords" content="Python,学习笔记,Scrapy"><meta name="author" content="Teams Six,undefined"><meta name="copyright" content="Teams Six"><title>【Python Scrapy 爬虫框架】 5、利用 pipelines 和 settings 将爬取数据存储到 MongoDB【Teams Six】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.json","languages":{"hits_empty":"local_search.hits_empty"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#0x00-前言"><span class="toc-number">1.</span> <span class="toc-text">0x00 前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0x01-配置-pipelines-py"><span class="toc-number">2.</span> <span class="toc-text">0x01 配置 pipelines.py</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0x02-配置-settings-py"><span class="toc-number">3.</span> <span class="toc-text">0x02 配置 settings.py</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0x02-运行"><span class="toc-number">4.</span> <span class="toc-text">0x02 运行</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Teams Six</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/teamssix" target="_blank">GitHub<i class="icon-dot bg-color1"></i></a><a class="links-button button-hover" href="https://me.csdn.net/qq_37683287" target="_blank">CSDN<i class="icon-dot bg-color7"></i></a><a class="links-button button-hover" href="https://twitter.com/TeamsSix" target="_blank">Twitter<i class="icon-dot bg-color6"></i></a><a class="links-button button-hover" href="https://instagram.com/TeamsSix" target="_blank">Instagram<i class="icon-dot bg-color4"></i></a><a class="links-button button-hover" href="https://www.youtube.com/channel/UCKeJhJYi_tmXWETzVxKbSvA" target="_blank">YouTube<i class="icon-dot bg-color9"></i></a><a class="links-button button-hover" href="https://space.bilibili.com/148389186" target="_blank">BiliBili<i class="icon-dot bg-color3"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">52</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">76</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">15</span></a></div><div class="friend-link"><a class="friend-link-text" target="_blank">欢迎关注本站微信公众号：TeamsSix</a><a class="friend-link-text" href="https://www.loongten.com/" target="_blank">友情链接：loongten</a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a></nav><div class="right-info"><a class="title-name" href="/">Teams Six</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">【Python Scrapy 爬虫框架】 5、利用 pipelines 和 settings 将爬取数据存储到 MongoDB</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2019-12-26 | 更新于 2019-12-26</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Python-Scrapy-爬虫框架学习笔记/">Python Scrapy 爬虫框架学习笔记</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Python/">Python</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/学习笔记/">学习笔记</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div></div><div class="main-content"><h1 id="0x00-前言"><a href="#0x00-前言" class="headerlink" title="0x00 前言"></a>0x00 前言</h1><p>前文中讲到了将爬取的数据导出到文件中，接下来就在前文的代码基础之上，将数据导出到 MongoDB中。</p>
<h1 id="0x01-配置-pipelines-py"><a href="#0x01-配置-pipelines-py" class="headerlink" title="0x01 配置 pipelines.py"></a>0x01 配置 pipelines.py</h1><p>首先来到 pipelines.py 文件下，在这里写入连接操作数据库的一些功能。</p>
<p>将连接操作 mongo 所需要的包导入进来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br></pre></td></tr></table></figure>

<a id="more"></a>
<p>接下来定义一些参数，注意下面的函数都是在 TeamssixPipeline 类下的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">    cls.DB_URL = crawler.settings.get(<span class="string">'MONGO_DB_URI'</span>)</span><br><span class="line">    cls.DB_NAME = crawler.settings.get(<span class="string">'MONGO_DB_NAME'</span>)</span><br><span class="line">    <span class="keyword">return</span> cls()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">    self.client = pymongo.MongoClient(self.DB_URL)</span><br><span class="line">    self.db = self.client[self.DB_NAME]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">    self.client.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">    collection = self.db[spider.name]</span><br><span class="line">    collection.insert_one(dict(item))</span><br><span class="line">    <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<h1 id="0x02-配置-settings-py"><a href="#0x02-配置-settings-py" class="headerlink" title="0x02 配置 settings.py"></a>0x02 配置 settings.py</h1><p>ITEM_PIPELINES 是settings.py 文件自带的，把注释符号删掉就好</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'teamssix.pipelines.TeamssixPipeline'</span>: <span class="number">300</span>,  <span class="comment">#优先级，1-1000，数值越低优先级越高</span></span><br><span class="line">&#125;</span><br><span class="line">MONGO_DB_URI = <span class="string">'mongodb://localhost:27017'</span>  <span class="comment">#mongodb 的连接 url</span></span><br><span class="line">MONGO_DB_NAME = <span class="string">'blog'</span>  <span class="comment">#要连接的库</span></span><br></pre></td></tr></table></figure>

<h1 id="0x02-运行"><a href="#0x02-运行" class="headerlink" title="0x02 运行"></a>0x02 运行</h1><p>直接执行命令，不加参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl blogurl</span><br></pre></td></tr></table></figure>

<p>注意，如果原来 MongoDB 中没有我们要连接的库， MongoDB 会自己创建，就不需要自己创建了，所以还是蛮方便的，使用 Robo 3T 打开后，就能看到刚才存进的数据。</p>
<p><img src="https://teamssix.oss-cn-hangzhou.aliyuncs.com/scrapy12.png" alt></p>
<blockquote>
<p>更多信息欢迎关注我的个人微信公众号：TeamsSix</p>
</blockquote>
<blockquote>
<p>参考链接：<br><a href="https://youtu.be/aDwAmj3VWH4" target="_blank" rel="noopener">https://youtu.be/aDwAmj3VWH4</a><br><a href="http://doc.scrapy.org/en/latest/topics/architecture.html" target="_blank" rel="noopener">http://doc.scrapy.org/en/latest/topics/architecture.html</a><br><a href="https://lemmo.xyz/post/Scrapy-To-MongoDB-By-Pipeline.html" target="_blank" rel="noopener">https://lemmo.xyz/post/Scrapy-To-MongoDB-By-Pipeline.html</a></p>
</blockquote>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Teams Six</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://www.teamssix.com/year/191226-151702.html">https://www.teamssix.com/year/191226-151702.html</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.teamssix.com">Teams Six</a>！</span></div></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/year/191226-151707.html"><i class="fas fa-angle-left">&nbsp;</i><span>【Python Scrapy 爬虫框架】 6、继续爬虫、终止和重启任务</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/year/191226-151659.html"><span>【Python Scrapy 爬虫框架】 4、数据项介绍和导出文件</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Teams Six</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--></body></html>