<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="【Python Scrapy 爬虫框架】 3、利用 Scrapy 爬取博客文章详细信息"><meta name="keywords" content="Python,学习笔记,Scrapy"><meta name="author" content="Teams Six,undefined"><meta name="copyright" content="Teams Six"><title>【Python Scrapy 爬虫框架】 3、利用 Scrapy 爬取博客文章详细信息【Teams Six】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.json","languages":{"hits_empty":"local_search.hits_empty"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#0x00-写在前面"><span class="toc-number">1.</span> <span class="toc-text">0x00 写在前面</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0x01-编写子页面爬取代码"><span class="toc-number">2.</span> <span class="toc-text">0x01 编写子页面爬取代码</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0x02-完整代码"><span class="toc-number">3.</span> <span class="toc-text">0x02 完整代码</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Teams Six</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/teamssix" target="_blank">GitHub<i class="icon-dot bg-color7"></i></a><a class="links-button button-hover" href="https://me.csdn.net/qq_37683287" target="_blank">CSDN<i class="icon-dot bg-color2"></i></a><a class="links-button button-hover" href="https://twitter.com/TeamsSix" target="_blank">Twitter<i class="icon-dot bg-color8"></i></a><a class="links-button button-hover" href="https://instagram.com/TeamsSix" target="_blank">Instagram<i class="icon-dot bg-color9"></i></a><a class="links-button button-hover" href="https://www.youtube.com/channel/UCKeJhJYi_tmXWETzVxKbSvA" target="_blank">YouTube<i class="icon-dot bg-color8"></i></a><a class="links-button button-hover" href="https://space.bilibili.com/148389186" target="_blank">BiliBili<i class="icon-dot bg-color4"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">52</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">76</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">15</span></a></div><div class="friend-link"><a class="friend-link-text" target="_blank">欢迎关注本站微信公众号：TeamsSix</a><a class="friend-link-text" href="https://www.loongten.com/" target="_blank">友情链接：loongten</a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a></nav><div class="right-info"><a class="title-name" href="/">Teams Six</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">【Python Scrapy 爬虫框架】 3、利用 Scrapy 爬取博客文章详细信息</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2019-12-26 | 更新于 2019-12-26</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Python-Scrapy-爬虫框架学习笔记/">Python Scrapy 爬虫框架学习笔记</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Python/">Python</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/学习笔记/">学习笔记</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div></div><div class="main-content"><h1 id="0x00-写在前面"><a href="#0x00-写在前面" class="headerlink" title="0x00 写在前面"></a>0x00 写在前面</h1><p>在之前的文章中，会发现如果直接使用爬取命令，终端会回显很多调试信息，这样输出的内容就会显得很乱，所以就可以使用下面的命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl blogurl  -s LOG_FILE=all.log</span><br></pre></td></tr></table></figure>

<a id="more"></a>
<p>也就是在原来的基础上加上一个 -s 参数，这样调试信息就会保存到参数指定的文件中，不过也可以在 class 下添加下面的代码，这样只会显示调试出现错误的信息，所以这种方式就不用加 -s 了，至于选择哪一个，就需要​视情况而定。​</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;&apos;LOG_LEVEL&apos;:&apos;ERROR&apos;&#125;</span><br></pre></td></tr></table></figure>

<h1 id="0x01-编写子页面爬取代码"><a href="#0x01-编写子页面爬取代码" class="headerlink" title="0x01 编写子页面爬取代码"></a>0x01 编写子页面爬取代码</h1><p>先来看一行关键代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> scrapy.Request(url,callback=self.sub_article)</span><br></pre></td></tr></table></figure>

<p>上面这行代码中，使用 yield 返回利用 scrapy 请求 url 所获得的数据，并将数据通过 callback 传递到 sub_article 函数中。</p>
<p>其实对于 yield 和 return 都可以返回数据，但是利用 yield 返回数据后，还可以继续运行下面的代码，而使用 return 后，接下来的代码就不会再运行了，在 scrapy 中，如果使用 return 返回数据再用 list 存储数据，会造成不少的内存消耗，而使用 yield 则可以减少这些不必要的内存浪费。</p>
<p>所以接下来在 sub_article 函数中写上我们爬取子页面的代码即可，这里就爬取每个文章的标题和目录作为示例了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sub_article</span><span class="params">(self,response)</span>:</span></span><br><span class="line">   soup = BeautifulSoup(response.text,<span class="string">'html.parser'</span>)</span><br><span class="line">   print(<span class="string">'\n'</span>,soup.select(<span class="string">'.title'</span>)[<span class="number">0</span>].text)</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'.toc-text'</span>):</span><br><span class="line">      print(<span class="string">'\t'</span>,i.text)</span><br></pre></td></tr></table></figure>

<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">~# scrapy crawl blogurl  -s LOG_FILE=all.log</span><br><span class="line">【漏洞笔记】Robots.txt站点文件</span><br><span class="line">         0x00 概述</span><br><span class="line">         0x01 漏洞描述</span><br><span class="line">         0x02 漏洞危害</span><br><span class="line">         0x03 修复建议</span><br><span class="line">【经验总结】常见的HTTP方法</span><br><span class="line">         0x00 概述</span><br><span class="line">         0x01 GET</span><br><span class="line">         0x02 HEAD</span><br><span class="line">         0x03 POST</span><br><span class="line">         0x04 PUT</span><br><span class="line">         0x05 DELETE</span><br><span class="line">         0x06 CONNECT</span><br><span class="line">         0x07 OPTIONS</span><br><span class="line">         0x08 TRACE</span><br><span class="line">         0x09 PATCH</span><br><span class="line">【漏洞笔记】Host头攻击</span><br><span class="line">         0x00 概述</span><br><span class="line">         0x01 漏洞描述</span><br><span class="line">         0x02 漏洞危害</span><br><span class="line">         0x03 修复建议</span><br><span class="line">【直播笔记】白帽子的成长之路</span><br><span class="line">【Python 学习笔记】 异步IO (asyncio) 协程</span><br><span class="line">         0x00 前言</span><br><span class="line">         0x01 基本用法</span><br><span class="line">……省略……</span><br></pre></td></tr></table></figure>

<h1 id="0x02-完整代码"><a href="#0x02-完整代码" class="headerlink" title="0x02 完整代码"></a>0x02 完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BlogSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">   name = <span class="string">'blogurl'</span></span><br><span class="line">   start_urls = [<span class="string">'https://www.teamssix.com'</span>]</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self,response)</span>:</span></span><br><span class="line">      soup = BeautifulSoup(response.text,<span class="string">'html.parser'</span>)</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'.post-title'</span>):</span><br><span class="line">         url = <span class="string">'https://www.teamssix.com&#123;&#125;'</span>.format(i[<span class="string">'href'</span>])</span><br><span class="line">         <span class="keyword">yield</span> scrapy.Request(url,callback=self.sub_article)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">sub_article</span><span class="params">(self,response)</span>:</span></span><br><span class="line">      soup = BeautifulSoup(response.text,<span class="string">'html.parser'</span>)</span><br><span class="line">      title = self.article_title(soup)</span><br><span class="line">      list = self.article_list(soup)</span><br><span class="line">      print(title)</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> list:</span><br><span class="line">         print(<span class="string">'\t'</span>,i)</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">article_title</span><span class="params">(self,soup)</span>:</span></span><br><span class="line">      <span class="keyword">return</span> soup.select(<span class="string">'.title'</span>)[<span class="number">0</span>].text</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">article_list</span><span class="params">(self,soup)</span>:</span></span><br><span class="line">      list = []</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'.toc-text'</span>):</span><br><span class="line">         list.append(i.text)</span><br><span class="line">      <span class="keyword">return</span> list</span><br></pre></td></tr></table></figure>

<blockquote>
<p>更多信息欢迎关注我的个人微信公众号：TeamsSix</p>
</blockquote>
<blockquote>
<p>参考链接：<br><a href="https://youtu.be/aDwAmj3VWH4" target="_blank" rel="noopener">https://youtu.be/aDwAmj3VWH4</a><br><a href="https://blog.csdn.net/DEREK_D/article/details/84239813" target="_blank" rel="noopener">https://blog.csdn.net/DEREK_D/article/details/84239813</a><br><a href="http://doc.scrapy.org/en/latest/topics/architecture.html" target="_blank" rel="noopener">http://doc.scrapy.org/en/latest/topics/architecture.html</a></p>
</blockquote>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Teams Six</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://www.teamssix.com/year/191226-151652.html">https://www.teamssix.com/year/191226-151652.html</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.teamssix.com">Teams Six</a>！</span></div></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/year/191226-151659.html"><i class="fas fa-angle-left">&nbsp;</i><span>【Python Scrapy 爬虫框架】 4、数据项介绍和导出文件</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/year/191224-093319.html"><span>【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Teams Six</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--></body></html>