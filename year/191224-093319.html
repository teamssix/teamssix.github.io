<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接"><meta name="keywords" content="Python,学习笔记,Scrapy"><meta name="author" content="Teams Six,undefined"><meta name="copyright" content="Teams Six"><title>【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接【Teams Six】</title><link rel="stylesheet" href="/css/fan.css"><link rel="stylesheet" href="/css/thirdparty/jquery.mCustomScrollbar.min.css"><link rel="icon" href="/favicon.ico"><!-- script(src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML")--><script src="/js/mathjax/mathjax.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
});
</script><script>var isPassword = '' || false;
if (isPassword) {
    if (prompt('请输入文章密码') !== '') {
        alert('密码错误！');
        history.back();
    }
}</script><script>window.GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.json","languages":{"hits_empty":"local_search.hits_empty"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
}</script></head><body><canvas id="universe"></canvas><!--#body--><div id="sidebar"><div class="toggle-sidebar-info button-hover"><span data-toggle="文章目录">站点概览</span></div><div class="sidebar-toc"><div class="sidebar-toc-title">目录</div><div class="sidebar-toc-progress"><span class="progress-notice">您已阅读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc-progress-bar"></div></div><div class="sidebar-toc-content" id="sidebar-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#0x00-新建项目"><span class="toc-number">1.</span> <span class="toc-text">0x00 新建项目</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0x01-创建一个爬虫"><span class="toc-number">2.</span> <span class="toc-text">0x01 创建一个爬虫</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0x02-运行爬虫"><span class="toc-number">3.</span> <span class="toc-text">0x02 运行爬虫</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#0x03-爬取内容解析"><span class="toc-number">4.</span> <span class="toc-text">0x03 爬取内容解析</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info-avatar"><img class="author-info-avatar-img" src="/avatar.png"></div><div class="author-info-name">Teams Six</div><div class="author-info-description"></div><div class="links-buttons"><a class="links-button button-hover" href="https://github.com/teamssix" target="_blank">GitHub<i class="icon-dot bg-color0"></i></a><a class="links-button button-hover" href="https://me.csdn.net/qq_37683287" target="_blank">CSDN<i class="icon-dot bg-color8"></i></a><a class="links-button button-hover" href="https://twitter.com/TeamsSix" target="_blank">Twitter<i class="icon-dot bg-color5"></i></a><a class="links-button button-hover" href="https://instagram.com/TeamsSix" target="_blank">Instagram<i class="icon-dot bg-color8"></i></a><a class="links-button button-hover" href="https://www.youtube.com/channel/UCKeJhJYi_tmXWETzVxKbSvA" target="_blank">YouTube<i class="icon-dot bg-color7"></i></a><a class="links-button button-hover" href="https://space.bilibili.com/148389186" target="_blank">BiliBili<i class="icon-dot bg-color7"></i></a></div><div class="author-info-articles"><a class="author-info-articles-archives article-meta" href="/archives"><span class="pull-top">日志</span><span class="pull-bottom">48</span></a><a class="author-info-articles-tags article-meta" href="/tags"><span class="pull-top">标签</span><span class="pull-bottom">69</span></a><a class="author-info-articles-categories article-meta" href="/categories"><span class="pull-top">分类</span><span class="pull-bottom">13</span></a></div><div class="friend-link"><a class="friend-link-text" target="_blank">欢迎关注本站微信公众号：TeamsSix</a><a class="friend-link-text" href="https://www.loongten.com/" target="_blank">友情链接：loongten</a></div></div></div><div id="main-container"><header><div id="menu-outer"><i class="menu-list-icon fas fa-bars"></i><nav id="menu-inner"><a class="menu-item" href="/">首页</a><a class="menu-item" href="/tags">标签</a><a class="menu-item" href="/categories">分类</a><a class="menu-item" href="/archives">归档</a></nav><div class="right-info"><a class="title-name" href="/">Teams Six</a><span id="now-time"></span></div></div></header><div id="content-outer"><div id="content-inner"><article id="post"><div class="post-header"><div class="title">【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接</div><div class="container"><time class="button-hover post-date"><i class="fas fa-calendar-alt article-icon" aria-hidden="true"></i> 发表于 2019-12-24 | 更新于 2019-12-24</time><!--time.button-hover.post-date #[i.fas.fa-calendar-alt.article-icon(aria-hidden="true")] #[=__('post.modified')] #[=date(page['updated'], config.date_format)]--><div class="button-hover categories"><i class="fa fa-inbox article-icon" aria-hidden="true"></i><a class="link-a" href="/categories/Python-Scrapy-爬虫框架学习笔记/">Python Scrapy 爬虫框架学习笔记</a></div><div class="button-hover tags"><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Python/">Python</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/学习笔记/">学习笔记</a><span>&nbsp;|&nbsp;</span><i class="fa fa-tag article-icon" aria-hidden="true"></i><a class="link-a" href="/tags/Scrapy/">Scrapy</a></div></div></div><div class="main-content"><h1 id="0x00-新建项目"><a href="#0x00-新建项目" class="headerlink" title="0x00 新建项目"></a>0x00 新建项目</h1><p>在终端中即可直接新建项目，这里我创建一个名称为 teamssix 的项目，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject teamssix</span><br></pre></td></tr></table></figure>

<p>命令运行后，会自动在当前目录下生成许多文件，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">teamssix</span><br><span class="line">    │  scrapy.cfg  #scrapy的配置文件</span><br><span class="line">    └─teamssix  #项目的Python模块，在这里写自己的代码</span><br><span class="line">        │  items.py  #项目定义文件</span><br><span class="line">        │  middlewares.py  #项目中间件文件</span><br><span class="line">        │  pipelines.py  #项目管道文件，用来处理数据的写入存储等操作</span><br><span class="line">        │  settings.py  #项目设置文件</span><br><span class="line">        │  __init__.py</span><br><span class="line">        ├─spiders  #在这里写爬虫代码</span><br><span class="line">        └─ __init__.py</span><br></pre></td></tr></table></figure>

<a id="more"></a>
<p>接下来使用 Pycharm 打开我们刚才新建的项目。</p>
<h1 id="0x01-创建一个爬虫"><a href="#0x01-创建一个爬虫" class="headerlink" title="0x01 创建一个爬虫"></a>0x01 创建一个爬虫</h1><p>首先，在 spiders 文件下 new 一个 python file，这里我新建了一个名为 teamssix_blog_spider 的 py 文件。</p>
<p>在新建的文件中写入自己的代码，这里我写的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BlogSpider</span><span class="params">(scrapy.Spider)</span>:</span>  <span class="comment">#创建 Spider 类</span></span><br><span class="line">   name = <span class="string">'blogurl'</span>，  <span class="comment">#爬虫名称，必填</span></span><br><span class="line">   start_urls = [<span class="string">'https://www.teamssix.com'</span>]  <span class="comment">#待爬取的 url ，必填</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self,response)</span>:</span>  <span class="comment">#定义 parse 函数，以解析爬到的东西</span></span><br><span class="line">      print(response.url)</span><br><span class="line">      print(response.text)</span><br></pre></td></tr></table></figure>

<h1 id="0x02-运行爬虫"><a href="#0x02-运行爬虫" class="headerlink" title="0x02 运行爬虫"></a>0x02 运行爬虫</h1><p>之后运行我们刚新建的 blogurl 项目，运行命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl blogurl</span><br></pre></td></tr></table></figure>

<p>之后输出结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2019-12-23 18:33:45 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)</span><br><span class="line">2019-12-23 18:33:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7e&apos;</span><br><span class="line">……省略……</span><br><span class="line">https://www.teamssix.com</span><br><span class="line">&lt;!DOCTYPE html&gt;&lt;html lang=&quot;zh-CN&quot;&gt;&lt;head&gt;&lt;meta name=&quot;generator&quot; content=&quot;Hexo 3.8.0&quot;&gt;&lt;meta charset=&quot;UTF-8&quot;&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt;&lt;meta name=&quot;vi                                      </span><br><span class="line">tent&gt;&lt;meta name=&quot;keywords&quot; content&gt;&lt;meta name=&quot;author&quot; content=&quot;Teams Six,undefined&quot;&gt;&lt;meta name=&quot;copyright&quot; content=&quot;Teams Six&quot;&gt;&lt;title&gt;【Teams Six】&lt;/title&gt;&lt;link rel=&quot;styles                                      </span><br><span class="line">css&quot;&gt;&lt;link rel=&quot;icon&quot; href=&quot;/favicon.ico&quot;&gt;&lt;!-- script(src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;)--&gt;&lt;script src=&quot;/js/mathjax/mathjax</span><br><span class="line">    tex2jax: &#123;inlineMath: [[&apos;$&apos;, &apos;$&apos;], [&apos;\\(&apos;, &apos;\\)&apos;]]&#125;</span><br><span class="line">&#125;);</span><br><span class="line">……省略……</span><br></pre></td></tr></table></figure>

<p>不难看出，我们想要的内容已经被打印出来了，但这还远远不够，我们还需要对其进行简单的解析，这里就用到了 BeautifulSoup ，有过爬虫经验的对这个库应该是不陌生了。</p>
<h1 id="0x03-爬取内容解析"><a href="#0x03-爬取内容解析" class="headerlink" title="0x03 爬取内容解析"></a>0x03 爬取内容解析</h1><p>接下来，想要获取到每个文章的链接，只需要对 parse 的内容进行修改，修改也很简单，基本之前写的多线程里的代码一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self,response)</span>:</span></span><br><span class="line">   soup = BeautifulSoup(response.text,<span class="string">'html.parser'</span>)</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> soup.select(<span class="string">'.post-title'</span>):</span><br><span class="line">      print(<span class="string">'https://www.teamssix.com&#123;&#125;'</span>.format(i[<span class="string">'href'</span>]))</span><br></pre></td></tr></table></figure>

<p>很简单的一个小爬虫，然后将爬虫运行一下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">~# scrapy crawl blogurl  #运行命令</span><br><span class="line">2019-12-23 19:02:01 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)</span><br><span class="line">……省略……</span><br><span class="line">https://www.teamssix.com/year/191222-192227.html</span><br><span class="line">https://www.teamssix.com/year/191220-161745.html</span><br><span class="line">……省略……</span><br><span class="line">2019-12-23 19:02:04 [scrapy.core.engine] INFO: Spider closed (finished)</span><br></pre></td></tr></table></figure>

<p>此时就能够将我们想要的东西爬下来了，但这实现的功能还是比较简单，接下来将介绍如何使用 Scrapy 爬取每个子页面中的详细信息。</p>
<blockquote>
<p>更多信息欢迎关注我的个人微信公众号：TeamsSix<br>原文链接：<a href="https://www.temassix.com/year/191224-093319.html" target="_blank" rel="noopener">https://www.temassix.com/year/191224-093319.html</a></p>
</blockquote>
<blockquote>
<p>参考链接：<br><a href="https://youtu.be/aDwAmj3VWH4" target="_blank" rel="noopener">https://youtu.be/aDwAmj3VWH4</a><br><a href="http://doc.scrapy.org/en/latest/intro/tutorial.html" target="_blank" rel="noopener">http://doc.scrapy.org/en/latest/intro/tutorial.html</a></p>
</blockquote>
</div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-meta">本文作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Teams Six</a></span></div><div class="post-copyright-type"><span class="post-copyright-meta">本文链接: </span><span class="post-copyright-info"><a href="https://www.teamssix.com/year/191224-093319.html">https://www.teamssix.com/year/191224-093319.html</a></span></div><div class="post-copyright-notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.teamssix.com">Teams Six</a>！</span></div></div></article><div id="pagination"><div class="prev-post pull-left"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/year/191226-151652.html"><i class="fas fa-angle-left">&nbsp;</i><span>【Python Scrapy 爬虫框架】 3、利用 Scrapy 爬取博客文章详细信息</span></a></div><div class="next-post pull-right"><span class="line line-top"></span><span class="line line-right"></span><span class="line line-bottom"></span><span class="line line-left"></span><a href="/year/191224-092208.html"><span>【Python Scrapy 爬虫框架】 1、简介与安装</span><span>&nbsp;</span><i class="fas fa-angle-right"></i></a></div></div><!--div!= paginator()--></div></div><div class="button-hover" id="return-top"><i class="fas fa-arrow-up" aria-hidden="true"></i></div><footer><div id="footer"><div class="button-hover" id="side-button"><i class="fas fa-arrow-right"></i></div><div class="right-content"><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file-o"></i></span><span id="busuanzi_value_page_pv"></span><span></span></div><div class="copyright">&copy;2017 ～ 2020 By Teams Six</div></div></div></footer></div><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery-3.3.1.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/velocity.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/thirdparty/jquery.mCustomScrollbar.concat.min.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/fan.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/canvas_bg.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/utils.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/scroll.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/sidebar.js"></script><!--script(src=url)--><!--js(src=url_for(url) + '?version=' + version())--><script src="/js/copy.js"></script><!--script(src=url)--></body></html>